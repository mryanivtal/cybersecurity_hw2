{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0cd8b4e",
      "metadata": {},
      "source": [
        "## Requirements\n",
        "\n",
        "* Detect predetorial patterns in other side chat messages, alert parents / block chat\n",
        "    - Per message\n",
        "    - Sequence\n",
        "    - Media\n",
        "* Detect and warn / block personal information giveaway by own side of chat (Child)\n",
        "    - text\n",
        "    - media\n",
        "* Support 2 party / multiple party chats\n",
        "* Block known predators from past chats\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4754d185",
      "metadata": {},
      "source": [
        "## Flow control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9313de52",
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE_FULL_PJ_DATAFRAME = 'Load'\n",
        "\n",
        "CREATE_FULL_PAN12_DATAFRAME = 'Load'\n",
        "PREPROCESS_FULL_PAN12_DATAFRAME = 'Load'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f086e0e1",
      "metadata": {
        "id": "f086e0e1",
        "tags": []
      },
      "source": [
        "## General - imports paths etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "X7ncp3Hf4hJX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7ncp3Hf4hJX",
        "outputId": "3286ae7f-0849-41d9-d87f-2bb9342b8ba9"
      },
      "outputs": [],
      "source": [
        "# %pip install pyspellchecker\n",
        "# %python -m spacy download en_core_web_sm\n",
        "# %pip install pyLDAvis\n",
        "# %pip install altair\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98514e4d",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "683f43e6-ff1c-4c28-af4c-3452553fc476",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683f43e6-ff1c-4c28-af4c-3452553fc476",
        "outputId": "e026db55-de59-40bd-bc1e-fe33a2fd15ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mryan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "d:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  from imp import reload\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "tqdm.pandas()\n",
        "from ipywidgets import IntProgress\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import altair\n",
        "\n",
        "# from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.corpora as corpora\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "import xml.etree.ElementTree as ET \n",
        "from xml.etree.ElementTree import ParseError\n",
        "\n",
        "import csv\n",
        "\n",
        "from typing import Dict, Callable, List, Dict, Set, Any\n",
        "import logging\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m3FuuqiQ2Sk3",
      "metadata": {
        "id": "m3FuuqiQ2Sk3"
      },
      "source": [
        "### Env control and folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "LAmVCG5d2Y-M",
      "metadata": {
        "id": "LAmVCG5d2Y-M"
      },
      "outputs": [],
      "source": [
        "# ENV = 'Colab'\n",
        "ENV = 'Local'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bf9de114-ddde-43d7-9d6a-69d1b0a913d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9de114-ddde-43d7-9d6a-69d1b0a913d0",
        "outputId": "8384cef9-9e3c-47a2-ce08-e86f3a761b2c"
      },
      "outputs": [],
      "source": [
        "# Folders\n",
        "if ENV=='Local':\n",
        "  PROJECT_ROOT = Path('./')\n",
        "\n",
        "elif ENV=='Colab':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = Path('/content/drive/MyDrive/colab_data/cyber2/')\n",
        "  \n",
        "\n",
        "PJ_DATA_FOLDER = PROJECT_ROOT / Path('customer_data')\n",
        "PAN12_TEST_DATA_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-test-corpus-2012-05-21/pan12-sexual-predator-identification-test-corpus-2012-05-17.xml')\n",
        "PAN12_LINE_LABELS_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-test-corpus-2012-05-21/pan12-sexual-predator-identification-groundtruth-problem2.txt')\n",
        "PAN12_USER_LABELS_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-test-corpus-2012-05-21/pan12-sexual-predator-identification-groundtruth-problem1.txt')\n",
        "\n",
        "PAN12_TRAIN_DATA_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-training-corpus-2012-05-01/pan12-sexual-predator-identification-training-corpus-2012-05-01.xml')\n",
        "PAN12_TRAIN_USER_LABELS_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-training-corpus-2012-05-01/pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt')\n",
        "\n",
        "\n",
        "OUTPUT_FOLDER = PROJECT_ROOT / Path('output')\n",
        "\n",
        "if not PAN12_TEST_DATA_FILE.exists():\n",
        "    raise FileNotFoundError('File not found!')\n",
        "\n",
        "if not PAN12_LINE_LABELS_FILE.exists():\n",
        "    raise FileNotFoundError('File not found!')  \n",
        "\n",
        "if not PAN12_USER_LABELS_FILE.exists():\n",
        "    raise FileNotFoundError('File not found!') \n",
        "\n",
        "if not PJ_DATA_FOLDER.is_dir():\n",
        "    raise FileNotFoundError('Directry not found!') \n",
        "\n",
        "if not OUTPUT_FOLDER.is_dir():\n",
        "    print(f'creating output folder: {OUTPUT_FOLDER}')\n",
        "    OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037ad22d",
      "metadata": {
        "id": "037ad22d"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eaf51658",
      "metadata": {
        "id": "eaf51658"
      },
      "outputs": [],
      "source": [
        "# Define datasets with texts and labels\n",
        "\n",
        "def list_files_in_dir(folder: Path, extension='*') -> List:\n",
        "    \n",
        "    file_list = [f for f in folder.glob(f'**/*.{extension}') if f.is_file()]\n",
        "    return file_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8f4bc9ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "def unify_csv_dataframes_to_one_sorted(csv_parts_folder: Path, target_csv_path=None)-> pd.DataFrame:\n",
        "    '''\n",
        "    Gets:\n",
        "        csv_parts_folder - Path of folder with partial df CSV files\n",
        "        target_csv_path - (optional) - path to save unified CSV\n",
        "\n",
        "    Returns:\n",
        "        Unified PD dataframe with all csvs concatenated on axis 0\n",
        "    '''\n",
        "\n",
        "    file_list = list_files_in_dir(csv_parts_folder, extension='csv')\n",
        "    ordered_filenames = sorted([str(filename) for filename in file_list])\n",
        "    ordered_file_list = [Path(filename) for filename in ordered_filenames]\n",
        "    print(f'Found {len(ordered_file_list)} files to unify')\n",
        "\n",
        "    unified_df = None\n",
        "    for file in ordered_file_list:\n",
        "        part_df = pd.read_csv(file, header=0, index_col=0)\n",
        "\n",
        "        if unified_df is not None:\n",
        "            unified_df = pd.concat([unified_df, part_df], axis=0)\n",
        "        else:\n",
        "            unified_df = part_df\n",
        "\n",
        "    if target_csv_path is not None:\n",
        "        unified_df.to_csv(target_csv_path)\n",
        "    \n",
        "    return unified_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065d0df3",
      "metadata": {
        "id": "065d0df3"
      },
      "source": [
        "### Load word lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "70954f6b",
      "metadata": {
        "id": "70954f6b"
      },
      "outputs": [],
      "source": [
        "# Load word lists\n",
        "SEX_WL_PATH = PROJECT_ROOT / Path(r'sex_words.txt')\n",
        "with open(SEX_WL_PATH, 'rt') as handle:\n",
        "    sex_word_list = handle.read().split('\\n')\n",
        "\n",
        "MEETING_WL_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'meeting_words.txt')\n",
        "with open(MEETING_WL_PATH, 'rt') as handle:\n",
        "    meeting_word_list = handle.read().split('\\n')\n",
        "\n",
        "FAMILY_WL_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'family_words.txt')\n",
        "with open(FAMILY_WL_PATH, 'rt') as handle:\n",
        "    family_word_list = handle.read().split('\\n')\n",
        "\n",
        "CHAT_SLANG_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'chat_slang.txt')\n",
        "with open(CHAT_SLANG_PATH, mode='rt') as handle:\n",
        "    csv_reader = csv.reader(handle, delimiter='\\t')\n",
        "    chat_slang = {rows[0]:rows[1] for rows in csv_reader}\n",
        "\n",
        "EMOTICONS_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'emoticons.txt')\n",
        "with open(EMOTICONS_PATH, mode='rt', encoding=\"utf8\") as handle:\n",
        "    csv_reader = csv.reader(handle, delimiter='\\t')\n",
        "    emoticons = {rows[0]:rows[1] for rows in csv_reader}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0844b40",
      "metadata": {
        "id": "b0844b40",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e182ed0",
      "metadata": {
        "id": "3e182ed0",
        "tags": []
      },
      "source": [
        "### Chat text preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "34d2873c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "34d2873c",
        "outputId": "b23b6c28-f30f-4d69-9d8f-3e7a0e752c33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:75: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:75: DeprecationWarning: invalid escape sequence \\w\n",
            "C:\\Users\\mryan\\AppData\\Local\\Temp\\ipykernel_1240\\398274303.py:75: DeprecationWarning: invalid escape sequence \\w\n",
            "  text_words = re.sub(\"[^\\w]\", \" \",  text).split()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def remove_stopwords(text: str, words_to_remove: List[str])-> str:\n",
        "    '''\n",
        "    Gets string, returns it without stopwords\n",
        "    '''\n",
        "    return \" \".join([word for word in str(text).split() if word not in words_to_remove])\n",
        "\n",
        "\n",
        "def stem_text(text: str, stemmer: Any)-> str:\n",
        "    '''\n",
        "    stem text string\n",
        "    '''\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def remove_emoji(text: str) -> str:\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_emoticons(text: str, emoticons: Dict) -> str:\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in emoticons) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def replace_pornsites_with_string(text:str, replacement_string:str='porn')->str:\n",
        "    pornsite_pattern = re.compile(r'\\S+xnxx\\.co\\S+' + r'|\\S+pornhub\\.co\\S+' + r'|\\S+nude\\.co\\S+' + r'|\\S+sex\\.co\\S+')\n",
        "    return pornsite_pattern.sub(replacement_string, text)\n",
        "\n",
        "def remove_urls(text:str)-> str:\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_special_characters(text:str)-> str:\n",
        "    special_chars_pattern = re.compile(r'[^A-Za-z0-9 ]+')\n",
        "    return special_chars_pattern.sub(r' ', text)\n",
        "\n",
        "\n",
        "def replace_chat_slang(text: str, chat_slang: Dict[str, str])-> str:\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_slang.keys():\n",
        "            new_text.append(chat_slang[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "\n",
        "def correct_spellings(text: str, speller: Callable) -> str:\n",
        "    corrected_text = []\n",
        "    misspelled_words = speller.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(speller.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "\n",
        "def lemmation(text:str, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    nlp_lem = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "    result = nlp_lem(text)\n",
        "    result = [token.lemma_ for token in result if token.pos_ in allowed_postags]\n",
        "    return  \" \".join(result)\n",
        "\n",
        "\n",
        "def contains_words_from_list(text: str, word_list: List[str])-> bool:\n",
        "    text_words = re.sub(\"[^\\w]\", \" \",  text).split()\n",
        "    if any(word in word_list for word in text_words):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def preprocess_string_for_bow(text: str, stemmer: Callable=None, speller: Callable=None, words_to_remove:List[str]=None, emoticons: Dict[str, str]=None, chat_slang: Dict[str, str]=None)-> str:\n",
        "    try:\n",
        "        # text = remove_emoji(text)\n",
        "        # text = remove_emoticons(text, emoticons)\n",
        "        text = text.lower()\n",
        "        text = replace_chat_slang(text, chat_slang)\n",
        "        text = replace_pornsites_with_string(text)\n",
        "        # text = remove_urls(text)\n",
        "        text = remove_special_characters(text)\n",
        "        text = correct_spellings(text, speller)\n",
        "        text = lemmation(text)\n",
        "        # text = remove_stopwords(text, words_to_remove)\n",
        "        # text = stem_text(text, stemmer)\n",
        "    except(TypeError):\n",
        "        print(f'Problematic string: {text}')\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_df_for_bow(df: pd.DataFrame, text_col: str, output_col_name='preprocessed_bow', stemmer=None, speller=None, words_to_remove=None, emoticons=None, chat_slang=None)-> pd.DataFrame:\n",
        "    '''\n",
        "    Gets a PD dataframe and a text column name\n",
        "    returns the same dataframe with additional column called 'posts_preprocessed_bow'\n",
        "    '''\n",
        "    df[output_col_name] = df[text_col].progress_apply(lambda text: preprocess_string_for_bow(text, stemmer=stemmer, speller=speller, words_to_remove=words_to_remove, emoticons=emoticons, chat_slang=chat_slang))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fb38ab5c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go com porn walk laugh amp come flight now right right back'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "preprocess_args = {'stemmer': PorterStemmer(),\n",
        "                    'speller': SpellChecker(),\n",
        "                    'words_to_remove': set(stopwords.words('english')),\n",
        "                    'emoticons': emoticons,\n",
        "                    'chat_slang': chat_slang,\n",
        "                    }\n",
        "\n",
        "text = 'r u going to www.google.com http://xnxx.com im walking LOL ths is not &amp;right im caming flight now u r right brb and fu :-)'\n",
        "# text = 'yeah--well I just want to see you before I go in the apt--cause one of my friends :) lol :X) got arrested for doing the same thing with a 16 year old--it was a set-up type thing'\n",
        "\n",
        "preprocess_string_for_bow(text, **preprocess_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bd5e29",
      "metadata": {},
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a594d2",
      "metadata": {},
      "source": [
        "### Word-list based features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8de75393",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
            "C:\\Users\\mryan\\AppData\\Local\\Temp\\ipykernel_1240\\3953379730.py:2: DeprecationWarning: invalid escape sequence \\w\n",
            "  text_words = re.sub(\"[^\\w]\", \" \",  text).split()\n"
          ]
        }
      ],
      "source": [
        "def contains_words_from_list(text: str, word_list: List[str])-> bool:\n",
        "    text_words = re.sub(\"[^\\w]\", \" \",  text).split()\n",
        "    if any(word in word_list for word in text_words):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def add_wordlist_features(df: pd.DataFrame, text_column: str, sex_word_list, family_word_list, meeting_word_list):\n",
        "    df['contains_sex_words'] = df[text_column].apply(lambda text: contains_words_from_list(text, sex_word_list))\n",
        "    df['contains_family_words'] = df[text_column].apply(lambda text: contains_words_from_list(text, family_word_list))\n",
        "    df['contains_meeting_words'] = df[text_column].apply(lambda text: contains_words_from_list(text, meeting_word_list))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d31dd2",
      "metadata": {
        "id": "f4d31dd2",
        "tags": []
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8db35ba",
      "metadata": {},
      "source": [
        "### PJ dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8fc852-a907-4706-a742-14207e2eccf2",
      "metadata": {
        "id": "0f8fc852-a907-4706-a742-14207e2eccf2"
      },
      "source": [
        "#### PJ Convesation level dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "00ea3ef6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "00ea3ef6",
        "outputId": "d17865e4-16d2-4647-eec6-8d3348a5ac1c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>USERNAME</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>BODY</th>\n",
              "      <th>COMMENT</th>\n",
              "      <th>CODING</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:02:01 pm)</td>\n",
              "      <td>im dennis us army soldier from cincinnati</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:02:30 pm)</td>\n",
              "      <td>hi im becky from ky</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:02:35 pm)</td>\n",
              "      <td>how old ru</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:02:42 pm)</td>\n",
              "      <td>13 how old ru</td>\n",
              "      <td>(age stated and he didn't bat an eye)</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:02:44 pm)</td>\n",
              "      <td>u single</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:02:51 pm)</td>\n",
              "      <td>yeah</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:03:03 pm)</td>\n",
              "      <td>i had a bf but we broke up when i moved here</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:03:11 pm)</td>\n",
              "      <td>ok u have sex at 13</td>\n",
              "      <td>(he obviously knows my age)</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:03:28 pm)</td>\n",
              "      <td>u mean did i ever</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:03:32 pm)</td>\n",
              "      <td>yeah</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       USERNAME      DATETIME                                          BODY  \\\n",
              "0   armysgt1961  (7:02:01 pm)     im dennis us army soldier from cincinnati   \n",
              "1  peekaboo1293  (7:02:30 pm)                           hi im becky from ky   \n",
              "2   armysgt1961  (7:02:35 pm)                                    how old ru   \n",
              "3  peekaboo1293  (7:02:42 pm)                                 13 how old ru   \n",
              "4   armysgt1961  (7:02:44 pm)                                      u single   \n",
              "5  peekaboo1293  (7:02:51 pm)                                          yeah   \n",
              "6  peekaboo1293  (7:03:03 pm)  i had a bf but we broke up when i moved here   \n",
              "7   armysgt1961  (7:03:11 pm)                           ok u have sex at 13   \n",
              "8  peekaboo1293  (7:03:28 pm)                             u mean did i ever   \n",
              "9   armysgt1961  (7:03:32 pm)                                          yeah   \n",
              "\n",
              "                                 COMMENT CODING  \n",
              "0                                   <NA>      \n",
              "  \n",
              "1                                   <NA>   <NA>  \n",
              "2                                   <NA>      \n",
              "  \n",
              "3  (age stated and he didn't bat an eye)   <NA>  \n",
              "4                                   <NA>      \n",
              "  \n",
              "5                                   <NA>   <NA>  \n",
              "6                                   <NA>   <NA>  \n",
              "7            (he obviously knows my age)      \n",
              "  \n",
              "8                                   <NA>   <NA>  \n",
              "9                                   <NA>   <NA>  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_one_chat_as_df_pj(file_path: Path) -> Dict[str, pd.DataFrame]:\n",
        "    '''\n",
        "    Gets an path to a PJ XML file\n",
        "    returns a dict with three dataframes:\n",
        "        - victim data\n",
        "        - predator data\n",
        "        - conversation posts\n",
        "    '''\n",
        "    parser = ET.XMLParser(encoding=\"utf-8\")\n",
        "    try:\n",
        "        doc_tree = ET.parse(file_path, parser=parser)\n",
        "    except(ParseError):\n",
        "        print(f'failed to parse {str(file_path)}')\n",
        "        return None\n",
        "        \n",
        "    doc_root = doc_tree.getroot()\n",
        "    \n",
        "    posts_df = pd.DataFrame(columns = ['USERNAME', 'DATETIME', 'BODY', 'COMMENT', 'CODING'], dtype=str)\n",
        "    predator_df = pd.DataFrame(columns = ['FIRSTNAME', 'LASTNAME', 'STATEDNAME', 'STATEDAGE', 'GENDER', 'RACE', 'CITY', 'STATE', 'REPEATOFFENDER', 'ADMITGUILT', 'TRUTHFULNAME', 'SCREENNAME'], dtype=str)\n",
        "    victim_df = pd.DataFrame(columns = ['FIRSTNAME', 'LASTNAME', 'STATEDNAME', 'STATEDAGE', 'GENDER', 'RACE', 'CITY', 'STATE', 'PREVIOUSVICTIMIZATION', 'ADMITGUILT', 'SCREENNAME'], dtype=str)\n",
        "\n",
        "    for post in doc_root.findall('POST'):\n",
        "        post_dict = {}\n",
        "        for field in post:\n",
        "            post_dict[field.tag] = field.text\n",
        "\n",
        "        posts_df = posts_df.append(post_dict, ignore_index=True)\n",
        "    posts_df = posts_df.astype('string')\n",
        "\n",
        "\n",
        "    for predator in doc_root.findall('PREDATOR'):\n",
        "        predator_dict = {}\n",
        "        for field in predator:\n",
        "            predator_dict[field.tag] = field.text\n",
        "\n",
        "        predator_df = predator_df.append(predator_dict, ignore_index=True)   \n",
        "    predator_df = predator_df.astype('string')\n",
        "\n",
        "    for victim in doc_root.findall('VICTIM'):\n",
        "        victim_dict = {}\n",
        "        for field in victim:\n",
        "            victim_dict[field.tag] = field.text\n",
        "\n",
        "        victim_df = victim_df.append(victim_dict, ignore_index=True)  \n",
        "    victim_df = victim_df.astype('string')\n",
        "\n",
        "    return {'predator': predator_df, 'victim': victim_df, 'conversation': posts_df, 'conversation_id': str(file_path.parts[-1])}\n",
        "\n",
        "\n",
        "#----------------------------------------------------------\n",
        "# Test XML parse functions:\n",
        "file_path = PJ_DATA_FOLDER / Path('ArmySgt1961.xml')\n",
        "chat_dict = load_one_chat_as_df_pj(file_path)\n",
        "chat_dict['victim'].head()\n",
        "chat_dict['predator'].head()\n",
        "chat_dict['conversation'].head(10)\n",
        "# chat_dict['conversation_id']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3818aedd-70b1-4802-b66b-6f8116cb4dbf",
      "metadata": {
        "id": "3818aedd-70b1-4802-b66b-6f8116cb4dbf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PjSentencesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wrapper around Torch Dataset.\n",
        "    Prepares an indexed list of PJ conversation in a folder, returns conversations per index (like an array)\n",
        "    Load is lazy - loads conversation from disk on request.\n",
        "    Uses load_one_chat_as_df_pj() for conversation loading\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder: Path, df_preprocess_fn=None, df_preprocess_args:Dict=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          data_folder - folder with PJ XML files\n",
        "          df_preprocess_fn - function that gets a dataframe and adds preprocesed text column based on given text column\n",
        "\n",
        "        \"\"\"\n",
        "       \n",
        "        self.file_list = list_files_in_dir(data_folder, extension='xml')\n",
        "        self.df_preprocess_fn = df_preprocess_fn\n",
        "        self.df_preprocess_args = df_preprocess_args\n",
        "        self.TEXT_COLUMN_NAME = 'BODY'\n",
        "\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Gets element of the dataset\n",
        "\n",
        "        Args:\n",
        "            index (int): index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"        \n",
        "        sample = load_one_chat_as_df_pj(self.file_list[idx])\n",
        "        if (self.df_preprocess_fn is not None) and (sample is not None):\n",
        "            sample['conversation'] = self.df_preprocess_fn(sample['conversation'], self.TEXT_COLUMN_NAME, **self.df_preprocess_args)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7562daf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "7562daf7",
        "outputId": "29bd848e-bb88-464c-ad87-1e7ffd617ae3"
      },
      "outputs": [],
      "source": [
        "# # Test the dataset\n",
        "# preprocess_args = {'stemmer': PorterStemmer(),\n",
        "#                     'speller': SpellChecker(),\n",
        "#                     'words_to_remove': set(stopwords.words('english')),\n",
        "#                     'emoticons': emoticons,\n",
        "#                     'chat_slang': chat_slang,\n",
        "#                     }\n",
        "                    \n",
        "# pj_ds = PjSentencesDataset(PJ_DATA_FOLDER, df_preprocess_fn=preprocess_df_for_bow, df_preprocess_args=preprocess_args)\n",
        "# print(len(pj_ds))\n",
        "# print(pj_ds[1]['conversation_id'])\n",
        "# pj_ds[1]['conversation'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886275cc",
      "metadata": {},
      "source": [
        "#### Load entire PJ dataset as single dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c8a5905e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create full dataframe, no preprocessing yet\n",
        "\n",
        "def load_pj_dataset(data_folder:Path):\n",
        "    pj_df = None                    \n",
        "    pj_ds = PjSentencesDataset(data_folder)\n",
        "\n",
        "    for i in tqdm(range(len(pj_ds))):\n",
        "        conversation_dict = pj_ds[i]\n",
        "        if not conversation_dict is None:\n",
        "            conversation = conversation_dict['conversation']\n",
        "            conversation['conversation_id'] = conversation_dict['conversation_id']\n",
        "\n",
        "            if not pj_df is None:\n",
        "                pj_df = pj_df.append(conversation)\n",
        "            else:\n",
        "                pj_df = conversation.copy()\n",
        "    \n",
        "    return pj_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bd051e12",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>USERNAME</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>BODY</th>\n",
              "      <th>COMMENT</th>\n",
              "      <th>CODING</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>preprocessed_bow</th>\n",
              "      <th>contains_sex_words</th>\n",
              "      <th>contains_family_words</th>\n",
              "      <th>contains_meeting_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tunnels12000</td>\n",
              "      <td>07/19/06  7:48:24 PM</td>\n",
              "      <td>hi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>tracy_in_xcess</td>\n",
              "      <td>(07/19/06  7:49:06 PM)</td>\n",
              "      <td>hi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>tunnels12000</td>\n",
              "      <td>07/19/06  7:49:09 PM</td>\n",
              "      <td>very pretty pic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>['very', 'pretty', 'pic']</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>tunnels12000</td>\n",
              "      <td>07/19/06  7:49:19 PM</td>\n",
              "      <td>im david hope i didnt bother u</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\n</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>['bother']</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>tracy_in_xcess</td>\n",
              "      <td>07/19/06  7:49:48 PM</td>\n",
              "      <td>no thats ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>['s']</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        USERNAME                DATETIME  \\\n",
              "0           0    tunnels12000    07/19/06  7:48:24 PM   \n",
              "1           1  tracy_in_xcess  (07/19/06  7:49:06 PM)   \n",
              "2           2    tunnels12000    07/19/06  7:49:09 PM   \n",
              "3           3    tunnels12000    07/19/06  7:49:19 PM   \n",
              "4           4  tracy_in_xcess    07/19/06  7:49:48 PM   \n",
              "\n",
              "                             BODY COMMENT CODING   conversation_id  \\\n",
              "0                              hi     NaN    NaN  tunnels12000.xml   \n",
              "1                              hi     NaN    NaN  tunnels12000.xml   \n",
              "2                 very pretty pic     NaN    NaN  tunnels12000.xml   \n",
              "3  im david hope i didnt bother u     NaN     \\n  tunnels12000.xml   \n",
              "4                     no thats ok     NaN    NaN  tunnels12000.xml   \n",
              "\n",
              "            preprocessed_bow  contains_sex_words  contains_family_words  \\\n",
              "0                         []               False                  False   \n",
              "1                         []               False                  False   \n",
              "2  ['very', 'pretty', 'pic']               False                  False   \n",
              "3                 ['bother']               False                  False   \n",
              "4                      ['s']               False                  False   \n",
              "\n",
              "   contains_meeting_words  \n",
              "0                   False  \n",
              "1                   False  \n",
              "2                   False  \n",
              "3                   False  \n",
              "4                   False  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PJ_PREPROCESSED_CSV_PATH = OUTPUT_FOLDER / Path('pj_preprocessed_dataframe.csv')\n",
        "PJ_FULL_RAW_CSV = OUTPUT_FOLDER / Path('pan12_raw_full.csv')\n",
        "\n",
        "if CREATE_FULL_PJ_DATAFRAME == 'Process':\n",
        "    # load original dataset\n",
        "    pj_df = load_pj_dataset(PJ_DATA_FOLDER)\n",
        "    pj_df.to_csv(PJ_FULL_RAW_CSV)\n",
        "\n",
        "    # preprocess and add features\n",
        "    preprocess_args = {'stemmer': PorterStemmer(),\n",
        "                        'speller': SpellChecker(),\n",
        "                        'words_to_remove': set(stopwords.words('english')),\n",
        "                        'emoticons': emoticons,\n",
        "                        'chat_slang': chat_slang,\n",
        "                        }\n",
        "\n",
        "    pj_df = preprocess_df_for_bow(pj_df, 'BODY', **preprocess_args)\n",
        "    pj_df = add_wordlist_features(pj_df, 'preprocessed_bow', sex_word_list, family_word_list, meeting_word_list)\n",
        "    pj_df.to_csv(PJ_PREPROCESSED_CSV_PATH)\n",
        "\n",
        "elif CREATE_FULL_PJ_DATAFRAME == 'Load':\n",
        "    pj_df = pd.read_csv(PJ_PREPROCESSED_CSV_PATH)\n",
        "\n",
        "pj_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0fbbf9a3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>contains_sex_words</th>\n",
              "      <th>contains_family_words</th>\n",
              "      <th>contains_meeting_words</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conversation_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ArmySgt1961.xml</th>\n",
              "      <td>4560</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>arthinice.xml</th>\n",
              "      <td>574056</td>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aticloose.xml</th>\n",
              "      <td>13861</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corazon23456partio23456.xml</th>\n",
              "      <td>97020</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crazytrini85.xml</th>\n",
              "      <td>22366</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flxnonya.xml</th>\n",
              "      <td>6903</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fotophix.xml</th>\n",
              "      <td>5253</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ghost27_73.xml</th>\n",
              "      <td>3692403</td>\n",
              "      <td>60</td>\n",
              "      <td>36</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hiexcitement.xml</th>\n",
              "      <td>580503</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i_8u_raw.xml</th>\n",
              "      <td>1119756</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>icepirate53.xml</th>\n",
              "      <td>61776</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>italianlover37.xml</th>\n",
              "      <td>3160</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jleno9.xml</th>\n",
              "      <td>20100</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jon_raven2000.xml</th>\n",
              "      <td>40755</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lee_greer74.xml</th>\n",
              "      <td>1209790</td>\n",
              "      <td>27</td>\n",
              "      <td>22</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>manofdarkneedsl951.xml</th>\n",
              "      <td>15051</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>marc_00_48089.xml</th>\n",
              "      <td>114960</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>needinit1983.xml</th>\n",
              "      <td>77421</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sebastian_calif.xml</th>\n",
              "      <td>216153</td>\n",
              "      <td>69</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sjklanke.xml</th>\n",
              "      <td>221445</td>\n",
              "      <td>45</td>\n",
              "      <td>18</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sphinx_56_02.xml</th>\n",
              "      <td>1268028</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spongebob_giantdick.xml</th>\n",
              "      <td>7260</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stylelisticgrooves.xml</th>\n",
              "      <td>235641</td>\n",
              "      <td>29</td>\n",
              "      <td>21</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugardavis.xml</th>\n",
              "      <td>387640</td>\n",
              "      <td>35</td>\n",
              "      <td>30</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sweet_jason002.xml</th>\n",
              "      <td>229503</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texassailor04.xml</th>\n",
              "      <td>38503</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the_third_storm.xml</th>\n",
              "      <td>714610</td>\n",
              "      <td>36</td>\n",
              "      <td>10</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thedude420xxx.xml</th>\n",
              "      <td>142845</td>\n",
              "      <td>36</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tunnels12000.xml</th>\n",
              "      <td>2170486</td>\n",
              "      <td>100</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user194547.xml</th>\n",
              "      <td>11325</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Unnamed: 0  contains_sex_words  \\\n",
              "conversation_id                                               \n",
              "ArmySgt1961.xml                    4560                   5   \n",
              "arthinice.xml                    574056                  43   \n",
              "aticloose.xml                     13861                  16   \n",
              "corazon23456partio23456.xml       97020                   6   \n",
              "crazytrini85.xml                  22366                  11   \n",
              "flxnonya.xml                       6903                  13   \n",
              "fotophix.xml                       5253                   1   \n",
              "ghost27_73.xml                  3692403                  60   \n",
              "hiexcitement.xml                 580503                  12   \n",
              "i_8u_raw.xml                    1119756                  15   \n",
              "icepirate53.xml                   61776                   5   \n",
              "italianlover37.xml                 3160                   4   \n",
              "jleno9.xml                        20100                  17   \n",
              "jon_raven2000.xml                 40755                   1   \n",
              "lee_greer74.xml                 1209790                  27   \n",
              "manofdarkneedsl951.xml            15051                   9   \n",
              "marc_00_48089.xml                114960                  12   \n",
              "needinit1983.xml                  77421                   4   \n",
              "sebastian_calif.xml              216153                  69   \n",
              "sjklanke.xml                     221445                  45   \n",
              "sphinx_56_02.xml                1268028                  16   \n",
              "spongebob_giantdick.xml            7260                   0   \n",
              "stylelisticgrooves.xml           235641                  29   \n",
              "sugardavis.xml                   387640                  35   \n",
              "sweet_jason002.xml               229503                   3   \n",
              "texassailor04.xml                 38503                   5   \n",
              "the_third_storm.xml              714610                  36   \n",
              "thedude420xxx.xml                142845                  36   \n",
              "tunnels12000.xml                2170486                 100   \n",
              "user194547.xml                    11325                  14   \n",
              "\n",
              "                             contains_family_words  contains_meeting_words  \n",
              "conversation_id                                                             \n",
              "ArmySgt1961.xml                                  6                       4  \n",
              "arthinice.xml                                    7                       8  \n",
              "aticloose.xml                                    3                       6  \n",
              "corazon23456partio23456.xml                     13                       8  \n",
              "crazytrini85.xml                                 4                       3  \n",
              "flxnonya.xml                                     4                       1  \n",
              "fotophix.xml                                     2                       4  \n",
              "ghost27_73.xml                                  36                      44  \n",
              "hiexcitement.xml                                 3                      18  \n",
              "i_8u_raw.xml                                    19                      51  \n",
              "icepirate53.xml                                  8                      11  \n",
              "italianlover37.xml                               1                       3  \n",
              "jleno9.xml                                       2                       2  \n",
              "jon_raven2000.xml                                3                      11  \n",
              "lee_greer74.xml                                 22                      19  \n",
              "manofdarkneedsl951.xml                           1                       1  \n",
              "marc_00_48089.xml                                4                      34  \n",
              "needinit1983.xml                                 2                       7  \n",
              "sebastian_calif.xml                              4                      22  \n",
              "sjklanke.xml                                    18                      34  \n",
              "sphinx_56_02.xml                                 3                      30  \n",
              "spongebob_giantdick.xml                          0                       3  \n",
              "stylelisticgrooves.xml                          21                      16  \n",
              "sugardavis.xml                                  30                      33  \n",
              "sweet_jason002.xml                              25                      48  \n",
              "texassailor04.xml                               12                       8  \n",
              "the_third_storm.xml                             10                      39  \n",
              "thedude420xxx.xml                                5                       0  \n",
              "tunnels12000.xml                                23                      23  \n",
              "user194547.xml                                   1                       3  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pj_df.groupby(['conversation_id']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09720ea7",
      "metadata": {},
      "source": [
        "### Pan12 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aa278e",
      "metadata": {
        "id": "d3aa278e"
      },
      "source": [
        "#### Convert Pan12 to labeled datafreame for use later as Train data - Takes 3-4 days for raw load, ~20 days for preprocess (2.5M lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2d0e52c1",
      "metadata": {
        "id": "2d0e52c1"
      },
      "outputs": [],
      "source": [
        "# class Pan12converterToDF():\n",
        "\n",
        "#     # Pan12 converter for TEST dataset - with line labels!\n",
        "    \n",
        "#     \"\"\"\n",
        "#     Wrapper around Torch Dataset to perform text classification\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, chat_data_file: Path, user_labels_file: Path=None, line_labels_file: Path=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             chat_data_file: path to chat xml file\n",
        "#             conversation_labels:\n",
        "#             line_labels:\n",
        "#             mode:   full - all data \n",
        "#                     positive_lines - Only lines labeled as problematic\n",
        "#         \"\"\"\n",
        "       \n",
        "#         self.chat_data_file = chat_data_file\n",
        "#         self.conversations = self._get_conversation_roots(chat_data_file)\n",
        "\n",
        "#         self.user_labels_file = user_labels_file\n",
        "#         self.line_labels_file = line_labels_file\n",
        "#         self.TEXT_COLUMN_NAME = 'text'\n",
        "\n",
        "#         self.length = self._get_ds_length()\n",
        "#         self.num_conversations = len(self.conversations)\n",
        "\n",
        "#         # Initiate queue\n",
        "#         self.message_list = None\n",
        "#         self.current_conversation_id = None\n",
        "#         self.next_conversation_idx = 0\n",
        "#         self.next_message_idx = 0\n",
        "\n",
        "#         # Create sets of problematic lines and authors for labels\n",
        "#         user_labels = pd.read_csv(user_labels_file, delimiter='\\t', header=None)\n",
        "#         self.perverted_authors = set(user_labels[0])\n",
        "\n",
        "#         line_labels = pd.read_csv(line_labels_file, delimiter='\\t', header=None)\n",
        "#         line_labels['concat'] = line_labels[0] + '_' + line_labels[1].astype(str)\n",
        "#         self.perverted_conversations = set(line_labels[0].unique())\n",
        "#         self.pervert_lines = set(line_labels['concat'])\n",
        "\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         return self\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#             int: length of the dataset\n",
        "#         \"\"\"\n",
        "#         return self.length\n",
        "\n",
        "#     def convert(self, filename:Path, save_every=2000, mode: str='full') -> pd.DataFrame:\n",
        "#         \"\"\"Gets element of the dataset\n",
        "\n",
        "#         Args:\n",
        "#             index (int): index of the element in the dataset\n",
        "#         Returns:\n",
        "#             Single element by index\n",
        "#         \"\"\"        \n",
        "#         pan12_df = pd.DataFrame(columns=['conversation_id', 'line', 'author', 'time', 'text', 'line_label', 'author_label'])\n",
        "\n",
        "#         self._load_next_conversation_to_list(mode) \n",
        "\n",
        "#         if(mode == 'full'):\n",
        "#             iter_len = self.length\n",
        "#         elif(mode == 'positive_lines'):\n",
        "#             iter_len = len(self.pervert_lines)\n",
        "        \n",
        "#         for i in tqdm(range(iter_len)):\n",
        "#             message_dict = {}\n",
        "#             try:\n",
        "#                 message = self.message_list[self.next_message_idx]\n",
        "#             except(IndexError):\n",
        "#                 self._load_next_conversation_to_list(mode)\n",
        "#                 message = self.message_list[self.next_message_idx]\n",
        "            \n",
        "#             message_dict['conversation_id'] = self.current_conversation_id\n",
        "#             self.next_message_idx += 1\n",
        "            \n",
        "#             message_dict['line'] = message.attrib['line']  \n",
        "#             for field in message:\n",
        "#                 message_dict[field.tag] = field.text\n",
        "            \n",
        "#             message_dict['author_label'] = 1 if message_dict['author'] in self.perverted_authors else 0\n",
        "#             message_dict['line_label'] = 1 if message_dict['conversation_id'] + '_' + message_dict['line'] in self.pervert_lines else 0\n",
        "            \n",
        "#             pan12_df = pan12_df.append(message_dict, ignore_index=True)\n",
        "#             if i % save_every == 0:\n",
        "#                 pan12_df.to_csv(filename)\n",
        "#                 print('.', end='')\n",
        "\n",
        "#             # #######\n",
        "#             # if i == 1001:\n",
        "#             #     print(pan12_df.head(2001))\n",
        "#             #     break\n",
        "#             # ######\n",
        "#         pan12_df.to_csv(filename)\n",
        "#         return pan12_df\n",
        "    \n",
        "#     def _get_conversation_roots(self, file_path):\n",
        "#         doc_tree = ET.parse(file_path)\n",
        "#         conversation_roots = doc_tree.getroot().findall('conversation')\n",
        "#         return conversation_roots\n",
        "\n",
        "#     def _get_ds_length(self):\n",
        "#         number_messages = 0\n",
        "#         for conversation in self.conversations:\n",
        "#             number_messages += len(conversation.findall('message'))\n",
        "        \n",
        "#         return number_messages\n",
        "\n",
        "#     def _load_next_conversation_to_list(self, mode):\n",
        "#         try:\n",
        "#             conversation = self.conversations[self.next_conversation_idx] \n",
        "#             self.next_conversation_idx += 1\n",
        "#             self.current_conversation_id = conversation.attrib['id']  \n",
        "\n",
        "#             if mode == 'positive_lines':\n",
        "#                 while self.current_conversation_id not in self.perverted_conversations:\n",
        "#                     conversation = self.conversations[self.next_conversation_idx] \n",
        "#                     self.next_conversation_idx += 1\n",
        "#                     self.current_conversation_id = conversation.attrib['id']  \n",
        "     \n",
        "#         except(IndexError):\n",
        "#             raise StopIteration()\n",
        "\n",
        "#         if mode == 'positive_lines':\n",
        "#             self.message_list = [m for m in conversation.findall('message') if (self.current_conversation_id + '_' + m.attrib['line'] in self.pervert_lines)]\n",
        "#         else:\n",
        "#             self.message_list = [m for m in conversation.findall('message')]\n",
        "#         self.next_message_idx = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "92dd4938",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505,
          "referenced_widgets": [
            "5a30eca9cd274aedba3c3cfee08e986a",
            "e80d4d0d52a54f9ab8ac1f41f281366e",
            "7f1f823c15724c948715c043b9ea89b9",
            "14c4346449c64fa4a695d62f33d10cc1",
            "e032f1f7e816478a96c3c1f9246034c6",
            "bc6c2d7cdac74c079d40c0e1c69aaeec",
            "6aebf996aa9c4960bf0a46ebca04d783",
            "45501c874a53430ea0979ac01e4dcd87",
            "295caedfec664f0bacdab9553c5e674d",
            "3304f532f9c34d59bf10cf3a4b143a8f",
            "cbd617ce94ed4af587dd6dfa9789ac2d"
          ]
        },
        "id": "92dd4938",
        "outputId": "077a296b-5be6-4f27-9be2-c63377c83a1a"
      },
      "outputs": [],
      "source": [
        "# # PAN12_PERVERTED_LINES_CSV = OUTPUT_FOLDER / Path('pan12_perverted_lines_preprocessed.csv')\n",
        "# PAN12_PERVERTED_LINES_CSV = OUTPUT_FOLDER / Path('pan12_full_lines_preprocessed.csv')\n",
        "\n",
        "# PAN12_FULL_RAW_CSV = OUTPUT_FOLDER / Path('pan12_raw_full.csv')\n",
        "\n",
        "# if CREATE_FULL_PAN12_DATAFRAME == 'Process':\n",
        "#     # Create a dataframe of all pan12 test perverted lines\n",
        "#     pan12_converter = Pan12converterToDF(PAN12_TEST_DATA_FILE, user_labels_file=PAN12_USER_LABELS_FILE, line_labels_file=PAN12_LINE_LABELS_FILE)\n",
        "#     print(len(pan12_converter))\n",
        "#     # pan12_df = pan12_converter.convert(PAN12_FULL_RAW_CSV, mode='positive_lines')\n",
        "#     pan12_df = pan12_converter.convert(PAN12_FULL_RAW_CSV, mode='full')\n",
        "#     print(f'lines in pan12_df: {len(pan12_df)}')\n",
        "\n",
        "#     # Preprocess pan12 perverted lines only and save to csv\n",
        "#     preprocess_args = {'stemmer': PorterStemmer(),\n",
        "#                         'speller': SpellChecker(),\n",
        "#                         'words_to_remove': set(stopwords.words('english')),\n",
        "#                         'emoticons': emoticons,\n",
        "#                         'chat_slang': chat_slang\n",
        "#                         }\n",
        "\n",
        "#     pan12_df = preprocess_df_for_bow(pan12_df, 'text', **preprocess_args)\n",
        "#     pan12_df.to_csv(PAN12_PERVERTED_LINES_CSV)\n",
        "\n",
        "#     # add features to pan12 df\n",
        "#     pan12_df = add_wordlist_features(pan12_df, 'preprocessed_bow', sex_word_list, family_word_list, meeting_word_list)\n",
        "#     pan12_df.to_csv(PAN12_PERVERTED_LINES_CSV)\n",
        "\n",
        "# elif CREATE_FULL_PAN12_DATAFRAME == 'Load':\n",
        "#     pan12_df = pd.read_csv(PAN12_PERVERTED_LINES_CSV)\n",
        "\n",
        "# pan12_df = pan12_df.dropna()\n",
        "# pan12_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cde0de",
      "metadata": {},
      "source": [
        "#### Pan12 Conversation  level dataset for train data (No line labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4792225b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pan12Dataset(Dataset):\n",
        "    '''\n",
        "    Wrapper around Torch Dataset.\n",
        "    Prepares an indexed list of Pan12 conversation in a folder, returns conversations per index (like an array)\n",
        "    Load is lazy - loads conversation from disk on request.\n",
        "    Uses load_one_chat_as_df_pj() for conversation loading\n",
        "    '''\n",
        "\n",
        "    def __init__(self, chat_data_file: Path, user_labels_file: Path=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            chat_data_file: path to chat xml file\n",
        "            conversation_labels:\n",
        "            line_labels:  \n",
        "        \"\"\"\n",
        "        self.chat_data_file = chat_data_file\n",
        "        self.conversations = self._get_conversation_roots(chat_data_file)\n",
        "        self.preprocess_args = preprocess_args\n",
        "        self.user_labels_file = user_labels_file\n",
        "        self.TEXT_COLUMN_NAME = 'text'\n",
        "\n",
        "        # Create sets of problematic lines and authors for labels\n",
        "        user_labels = pd.read_csv(user_labels_file, header=None)\n",
        "        self.perverted_authors = set(user_labels[0])\n",
        "\n",
        "                \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Gets element of the dataset\n",
        "\n",
        "        Args:\n",
        "            index (int): index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"        \n",
        "\n",
        "        conversation = self.conversations[idx]\n",
        "        conversation_id = conversation.attrib['id']\n",
        "        conversation_list = []\n",
        "\n",
        "        for message in conversation.findall('message'):\n",
        "            message_list = [message.attrib['line']]\n",
        "            for field in message:\n",
        "                message_list.append(field.text)\n",
        "            \n",
        "            conversation_list.append(message_list)\n",
        "\n",
        "        conversation_df = pd.DataFrame(conversation_list, columns = ['line', 'author', 'time', 'text'])\n",
        "        conversation_df = conversation_df.dropna()\n",
        "        \n",
        "        if self.user_labels_file is not None:\n",
        "            chat_predetors = [author for author in conversation_df.author.unique() if author in self.perverted_authors]\n",
        "            result = {'conversation_id': conversation_id, 'conversation': conversation_df, 'predators': chat_predetors}\n",
        "        else:\n",
        "            result = {'conversation_id': conversation_id, 'conversation': conversation_df}\n",
        "\n",
        "        return result\n",
        "    \n",
        "    def _get_conversation_roots(self, file_path):\n",
        "        doc_tree = ET.parse(file_path)\n",
        "        conversation_roots = doc_tree.getroot().findall('conversation')\n",
        "        return conversation_roots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0d4bf5",
      "metadata": {},
      "source": [
        "#### Pan12 TRAIN converter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b636179",
      "metadata": {},
      "source": [
        "This section creates a dataset of single sided chat strings and tags them as predator or innocent.\n",
        "I.e. - each 2-party conversation will create two samples - one of each party, and each party will be tagged separately.\n",
        "The samples of predators are not necessary predatory in every sample, sinnce the label is on the person and not on the specific chat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3fc3456f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13eb600c8cce495aa79d4b8f2b7dc511",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/99502 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of predator texts: 753\n",
            "Number of innocent texts: 38250\n"
          ]
        }
      ],
      "source": [
        "PAN12_TRAIN_RAW_CSV = OUTPUT_FOLDER / Path('pan12_raw_full.csv')\n",
        "\n",
        "if CREATE_FULL_PAN12_DATAFRAME == 'Process':\n",
        "    print('Running process...')\n",
        "\n",
        "\n",
        "    pan12_train_ds = Pan12Dataset(PAN12_TRAIN_DATA_FILE, PAN12_TRAIN_USER_LABELS_FILE)\n",
        "    predator_texts = []\n",
        "    innocent_texts = []\n",
        "\n",
        "    for conversation_dict in tqdm(pan12_train_ds):\n",
        "        conversation_df = conversation_dict['conversation']\n",
        "        authors = conversation_df['author'].unique()\n",
        "        for author in authors:\n",
        "            author_side_text = ' '.join(conversation_df[conversation_df.author == author]['text'])\n",
        "            if author in conversation_dict['predators']:\n",
        "                predator_texts.append(author_side_text)\n",
        "            else:\n",
        "                innocent_texts.append(author_side_text)\n",
        "\n",
        "    predator_texts_df = pd.DataFrame(predator_texts, columns=['text'])\n",
        "    predator_texts_df['predator'] = np.ones(len(predator_texts_df))\n",
        "    innocent_texts_df = pd.DataFrame(innocent_texts, columns=['text'])\n",
        "    innocent_texts_df['predator'] = np.zeros(len(innocent_texts_df))\n",
        "\n",
        "    pan12_train_df = pd.concat([predator_texts_df, innocent_texts_df]).reset_index(drop=True)\n",
        "    pan12_train_df.to_csv(PAN12_TRAIN_RAW_CSV, header=True)\n",
        "\n",
        "elif CREATE_FULL_PAN12_DATAFRAME == 'Load':\n",
        "    print('Loading dataset...')\n",
        "    pan12_train_df = pd.read_csv(PAN12_TRAIN_RAW_CSV)\n",
        "\n",
        "\n",
        "# Remove conversations with leq than 10 words\n",
        "pan12_train_df['text_len'] = pan12_train_df['text'].progress_apply(lambda text: len(text.split()))\n",
        "pan12_train_df = pan12_train_df[pan12_train_df.text_len > 15]\n",
        "\n",
        "print(f'Number of predator texts: {len(pan12_train_df[pan12_train_df.predator == True])}')\n",
        "print(f'Number of innocent texts: {len(pan12_train_df[pan12_train_df.predator == False])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0b6b863f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "PAN12_TRAIN_PREPROCESSED_CSV = OUTPUT_FOLDER / Path('pan12_preprocessed_full.csv')\n",
        "\n",
        "if PREPROCESS_FULL_PAN12_DATAFRAME == 'Process':\n",
        "    print('Running preprocess...')\n",
        "\n",
        "    preprocess_args = {'stemmer': PorterStemmer(),\n",
        "                        'speller': SpellChecker(),\n",
        "                        'words_to_remove': set(stopwords.words('english')),\n",
        "                        'emoticons': emoticons,\n",
        "                        'chat_slang': chat_slang,\n",
        "                        }\n",
        "\n",
        "    for i in range(0, 40):\n",
        "      print(f'current batch: {i * 1000} - {(i+1) * 1000}')\n",
        "      df = preprocess_df_for_bow(pan12_train_df.iloc[i * 1000:((i+1) * 1000), :], 'text', output_col_name='preprocessed_bow', **preprocess_args)\n",
        "      PAN12_TRAIN_PREPROCESSED_CSV = OUTPUT_FOLDER / Path(f'pan12_preprocessed_full_{i}_{i+1}.csv')\n",
        "      df.to_csv(PAN12_TRAIN_PREPROCESSED_CSV, header=True)\n",
        "    \n",
        "    # join and update Pan12 preprocessed csv parts - Required because the preprocessing was ran on several computers in parts\n",
        "    PAN12_TRAIN_PREPROCESSED_CSV = OUTPUT_FOLDER / Path('pan12_preprocessed_full.csv')\n",
        "    PAN12_TRAIN_PREPROCESSED_CSV_PARTS_FOLDER = OUTPUT_FOLDER / Path('pan12_preprocessed_parts')\n",
        "\n",
        "    unified_df = unify_csv_dataframes_to_one_sorted(PAN12_TRAIN_PREPROCESSED_CSV_PARTS_FOLDER)\n",
        "    # unified_df.preprocessed_bow = unified_df.preprocessed_bow.apply(lambda text: ''.join(ch for ch in text if ch not in [\"'\", \"[\", \"]\", \",\"]))\n",
        "\n",
        "    unified_df.to_csv(PAN12_TRAIN_PREPROCESSED_CSV)        \n",
        "\n",
        "elif PREPROCESS_FULL_PAN12_DATAFRAME == 'Load':\n",
        "    print('Loading preprocessed data...')\n",
        "    pan12_train_df = pd.read_csv(PAN12_TRAIN_PREPROCESSED_CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "78433167",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>predator</th>\n",
              "      <th>text_len</th>\n",
              "      <th>preprocessed_bow</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hey!! a little better what are u doing? yea i ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>320</td>\n",
              "      <td>little better do yea think just wake check see...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>hello hey where r u from in nj?? same here cnj...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>468</td>\n",
              "      <td>where same here co so bring room work old here...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>hi liz nothing much...how have u been doin any...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>84</td>\n",
              "      <td>much how do good ok wrong finish homework good...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Hello Whatcha doin? Oh yeah hows brad and darl...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45</td>\n",
              "      <td>do how s loud laugh loud ink school kinda bori...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>i came to tonapah and you were't on left at 6 ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24</td>\n",
              "      <td>come on leave back talk later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35998</th>\n",
              "      <td>24904</td>\n",
              "      <td>24904</td>\n",
              "      <td>65325d50b2e25aca54bc871b89758c9c: For the poll...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49</td>\n",
              "      <td>poll vote member vote represent understanding ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35999</th>\n",
              "      <td>24905</td>\n",
              "      <td>24905</td>\n",
              "      <td>Ooh, impressive, 3 out of 3 of the times liste...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29</td>\n",
              "      <td>impressive time list new reminder email correc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36000</th>\n",
              "      <td>24906</td>\n",
              "      <td>24906</td>\n",
              "      <td>hello asl 21m uk u wasuup omg me too lol rape ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>221</td>\n",
              "      <td>uk omg too laugh loud rape just start jude lau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36001</th>\n",
              "      <td>24907</td>\n",
              "      <td>24907</td>\n",
              "      <td>heyy asl? 17 f australia&amp;apos; mehh not much b...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>177</td>\n",
              "      <td>much blast nirvana bedroom laugh haha love buz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36002</th>\n",
              "      <td>24908</td>\n",
              "      <td>24908</td>\n",
              "      <td>hey what u doing 16 wby m wbu what bout you we...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24</td>\n",
              "      <td>do why web cam site porn yea come com guy girl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36003 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0.1  Unnamed: 0  \\\n",
              "0                 0           0   \n",
              "1                 2           2   \n",
              "2                 3           3   \n",
              "3                 4           4   \n",
              "4                 7           7   \n",
              "...             ...         ...   \n",
              "35998         24904       24904   \n",
              "35999         24905       24905   \n",
              "36000         24906       24906   \n",
              "36001         24907       24907   \n",
              "36002         24908       24908   \n",
              "\n",
              "                                                    text  predator  text_len  \\\n",
              "0      hey!! a little better what are u doing? yea i ...       1.0       320   \n",
              "1      hello hey where r u from in nj?? same here cnj...       1.0       468   \n",
              "2      hi liz nothing much...how have u been doin any...       1.0        84   \n",
              "3      Hello Whatcha doin? Oh yeah hows brad and darl...       1.0        45   \n",
              "4      i came to tonapah and you were't on left at 6 ...       1.0        24   \n",
              "...                                                  ...       ...       ...   \n",
              "35998  65325d50b2e25aca54bc871b89758c9c: For the poll...       0.0        49   \n",
              "35999  Ooh, impressive, 3 out of 3 of the times liste...       0.0        29   \n",
              "36000  hello asl 21m uk u wasuup omg me too lol rape ...       0.0       221   \n",
              "36001  heyy asl? 17 f australia&apos; mehh not much b...       0.0       177   \n",
              "36002  hey what u doing 16 wby m wbu what bout you we...       0.0        24   \n",
              "\n",
              "                                        preprocessed_bow  \n",
              "0      little better do yea think just wake check see...  \n",
              "1      where same here co so bring room work old here...  \n",
              "2      much how do good ok wrong finish homework good...  \n",
              "3      do how s loud laugh loud ink school kinda bori...  \n",
              "4                          come on leave back talk later  \n",
              "...                                                  ...  \n",
              "35998  poll vote member vote represent understanding ...  \n",
              "35999  impressive time list new reminder email correc...  \n",
              "36000  uk omg too laugh loud rape just start jude lau...  \n",
              "36001  much blast nirvana bedroom laugh haha love buz...  \n",
              "36002     do why web cam site porn yea come com guy girl  \n",
              "\n",
              "[36003 rows x 6 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pan12_train_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600af7b9",
      "metadata": {},
      "source": [
        "### LDA topic model - Bad performance due to sexual term sparsity and diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a734e28c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bag f words list from chat sentences\n",
        "def create_bow_from_text(text):\n",
        "    bow_words = []\n",
        "    text_word_list = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "    bow_words.append(text_word_list)\n",
        "    return bow_words[0]\n",
        "\n",
        "\n",
        "def create_bow_from_text_list(text_list):\n",
        "    bow_words = []\n",
        "    for text in text_list:\n",
        "        # text_word_list = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "        # bow_words.append(text_word_list)\n",
        "        bow_words.append(create_bow_from_text(text))\n",
        "    \n",
        "    return bow_words\n",
        "\n",
        "\n",
        "bow_words = create_bow_from_text_list(pan12_df['preprocessed_bow'])\n",
        "bow_words\n",
        "\n",
        "# bow_words.append(sex_word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a1f56f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create word indexes and frequencies from chat sentences bow\n",
        "id2word = corpora.Dictionary(bow_words)\n",
        "corpus = []\n",
        "for word in bow_words:\n",
        "    corpus.append(id2word.doc2bow(word))\n",
        "\n",
        "\n",
        "print(pan12_df['preprocessed_bow'][7])\n",
        "print(corpus[7])\n",
        "print(id2word[31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964c65d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create LDA topic model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "id2word=id2word,\n",
        "num_topics=10, \n",
        "update_every=1,\n",
        "chunksize=100,\n",
        "passes=10,\n",
        "alpha='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6679c62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a3c8a5",
      "metadata": {},
      "source": [
        "### TF/IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f21fe2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train TF/IDF model\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.95)\n",
        "tfidf_vectorizer.fit(pan12_df['preprocessed_bow'])\n",
        "\n",
        "# Transform chat messages to vocabulary vectors\n",
        "vectorized_data = tfidf_vectorizer.transform(pan12_df['preprocessed_bow'])\n",
        "print(f'Vectorized data shape: {vectorized_data.shape}')\n",
        "\n",
        "print(pan12_df['preprocessed_bow'][6])\n",
        "print(vectorized_data[6])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc2379a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataframe of vectors\n",
        "tfidf_df = pd.DataFrame(vectorized_data.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
        "# tfidf_df[['sex', 'babe', 'young', 'age', 'dick']].sort_values('dick', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62539499",
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html\n",
        "#Visualize TF/IDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bac1a26",
      "metadata": {},
      "source": [
        "## Backup - not useful currently"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae40be1",
      "metadata": {},
      "source": [
        "### Pan12 dataloader and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab1a7b2",
      "metadata": {
        "id": "bab1a7b2"
      },
      "source": [
        "#### pan12 line level dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64c6027",
      "metadata": {
        "id": "e64c6027"
      },
      "outputs": [],
      "source": [
        "class Pan12LineLevelDataloader():  \n",
        "    \"\"\"\n",
        "    Wrapper around Torch Dataset to perform text classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chat_data_file: Path, user_labels_file: Path=None, line_labels_file: Path=None, preprocess_fn=None, preprocess_args:Dict=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            chat_data_file: path to chat xml file\n",
        "            conversation_labels:\n",
        "            line_labels:  \n",
        "        \"\"\"\n",
        "       \n",
        "        self.chat_data_file = chat_data_file\n",
        "        self.conversations = self._get_conversation_roots(chat_data_file)\n",
        "        self.preprocess_fn = preprocess_fn\n",
        "        self.preprocess_args = preprocess_args\n",
        "\n",
        "        self.user_labels_file = user_labels_file\n",
        "        self.line_labels_file = line_labels_file\n",
        "        self.TEXT_COLUMN_NAME = 'text'\n",
        "\n",
        "        self.length = self._get_ds_length()\n",
        "        self.num_conversations = len(self.conversations)\n",
        "\n",
        "        # Initiate queue\n",
        "        self.message_list = None\n",
        "        self.current_conversation_id = None\n",
        "        self.next_conversation_idx = 0\n",
        "        self.next_message_idx = 0\n",
        "\n",
        "        # Create sets of problematic lines and authors for labels\n",
        "        user_labels = pd.read_csv(user_labels_file, delimiter='\\t', header=None)\n",
        "        self.perverted_authors = set(user_labels[0])\n",
        "\n",
        "        line_labels = pd.read_csv(line_labels_file, delimiter='\\t', header=None)\n",
        "        line_labels['concat'] = line_labels[0] + '_' + line_labels[1].astype(str)\n",
        "        self.pervert_lines = set(line_labels['concat'])\n",
        "\n",
        "        self.load_next_conversation_to_list()\n",
        "                       \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def __next__(self) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Gets element of the dataset\n",
        "\n",
        "        Args:\n",
        "            index (int): index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"        \n",
        "        message_dict = {}\n",
        "        try:\n",
        "            message = self.message_list[self.next_message_idx]\n",
        "        except(IndexError):\n",
        "            self.load_next_conversation_to_list()\n",
        "            message = self.message_list[self.next_message_idx]\n",
        "\n",
        "        message_dict['conversation_id'] = self.current_conversation_id\n",
        "        self.next_message_idx += 1\n",
        "        \n",
        "        message_dict['line'] = message.attrib['line']  \n",
        "\n",
        "        for field in message:\n",
        "            message_dict[field.tag] = field.text\n",
        "        \n",
        "        if self.preprocess_fn is not None:\n",
        "            message_dict['text'] = self.preprocess_fn(message_dict['text'], **self.preprocess_args)\n",
        "        \n",
        "        message_dict['author_label'] = 1 if message_dict['author'] in self.perverted_authors else 0\n",
        "        message_dict['line_label'] = 1 if message_dict['conversation_id'] + '_' + message_dict['line'] in self.pervert_lines else 0\n",
        "\n",
        "        return message_dict\n",
        "    \n",
        "    def _get_conversation_roots(self, file_path):\n",
        "        doc_tree = ET.parse(file_path)\n",
        "        conversation_roots = doc_tree.getroot().findall('conversation')\n",
        "        return conversation_roots\n",
        "\n",
        "    def _get_ds_length(self):\n",
        "        number_messages = 0\n",
        "        for conversation in self.conversations:\n",
        "            number_messages += len(conversation.findall('message'))\n",
        "        \n",
        "        return number_messages\n",
        "\n",
        "    def load_next_conversation_to_list(self):\n",
        "        try:\n",
        "            conversation = self.conversations[self.next_conversation_idx] \n",
        "            self.current_conversation_id = conversation.attrib['id']  \n",
        "        except(IndexError):\n",
        "            raise StopIteration()\n",
        "\n",
        "        self.next_conversation_idx += 1\n",
        "        self.message_list = [m for m in conversation.findall('message')]\n",
        "        self.next_message_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfc77a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dfc77a4",
        "outputId": "1029d1bd-7ff7-4823-c0d8-89f3481c5706"
      },
      "outputs": [],
      "source": [
        "# # Test dataset\n",
        "# preprocess_args = {'stemmer': PorterStemmer(),\n",
        "#                     'speller': SpellChecker(),\n",
        "#                     'words_to_remove': set(stopwords.words('english')),\n",
        "#                     'emoticons': emoticons,\n",
        "#                     'chat_slang': chat_slang\n",
        "#                     }\n",
        "\n",
        "# pan12_ds = Pan12LineLevelDataloader(PAN12_TEST_DATA_FILE, user_labels_file=PAN12_USER_LABELS_FILE, line_labels_file=PAN12_LINE_LABELS_FILE, preprocess_fn=preprocess_string_for_bow, preprocess_args=preprocess_args)\n",
        "# print(len(pan12_ds))\n",
        "\n",
        "# for i, m in enumerate(pan12_ds):\n",
        "#     print(i, m) \n",
        "#     if i==50:\n",
        "#         break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d684c5",
      "metadata": {
        "id": "72d684c5"
      },
      "source": [
        "## some thoughts\n",
        "Bag of words - sexual words, fear, trust, family, approach (Location, transport) , other categories - DrouinBoydHancockJames2017\n",
        "Good article: file:///D:/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/ref%20docs/Early%20Text%20Classification%20using%20Multi-Resolution%20Concept%20Representations.pdf\n",
        "Ensamble and preprocessing: file:///D:/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/ref%20docs/PredatoryConversationDetection.pdf\n",
        "file:///D:/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/ref%20docs/Analyzing_Chat_Conversations_of_Pedophil.pdf\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "cyber_hw2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1a8e22f9a50968eda7c2b2da6b1cd647c6294c71990fbcb0be47dbd614eb6ed8"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14c4346449c64fa4a695d62f33d10cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3304f532f9c34d59bf10cf3a4b143a8f",
            "placeholder": "",
            "style": "IPY_MODEL_cbd617ce94ed4af587dd6dfa9789ac2d",
            "value": " 355158/2058781 [3:52:56&lt;35:12:01, 13.44it/s]"
          }
        },
        "295caedfec664f0bacdab9553c5e674d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3304f532f9c34d59bf10cf3a4b143a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45501c874a53430ea0979ac01e4dcd87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a30eca9cd274aedba3c3cfee08e986a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e80d4d0d52a54f9ab8ac1f41f281366e",
              "IPY_MODEL_7f1f823c15724c948715c043b9ea89b9",
              "IPY_MODEL_14c4346449c64fa4a695d62f33d10cc1"
            ],
            "layout": "IPY_MODEL_e032f1f7e816478a96c3c1f9246034c6"
          }
        },
        "6aebf996aa9c4960bf0a46ebca04d783": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f1f823c15724c948715c043b9ea89b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45501c874a53430ea0979ac01e4dcd87",
            "max": 2058781,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_295caedfec664f0bacdab9553c5e674d",
            "value": 355158
          }
        },
        "bc6c2d7cdac74c079d40c0e1c69aaeec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd617ce94ed4af587dd6dfa9789ac2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e032f1f7e816478a96c3c1f9246034c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80d4d0d52a54f9ab8ac1f41f281366e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc6c2d7cdac74c079d40c0e1c69aaeec",
            "placeholder": "",
            "style": "IPY_MODEL_6aebf996aa9c4960bf0a46ebca04d783",
            "value": " 17%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
