{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0cd8b4e",
      "metadata": {},
      "source": [
        "## Requirements\n",
        "\n",
        "* Detect predetorial patterns in other side chat messages, alert parents / block chat\n",
        "    - Per message\n",
        "    - Sequence\n",
        "    - Media\n",
        "* Detect and warn / block personal information giveaway by own side of chat (Child)\n",
        "    - text\n",
        "    - media\n",
        "* Support 2 party / multiple party chats\n",
        "* Block known predators from past chats\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f086e0e1",
      "metadata": {
        "id": "f086e0e1",
        "tags": []
      },
      "source": [
        "## General - imports paths etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4754d185",
      "metadata": {},
      "source": [
        "## Flow control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9313de52",
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE_FULL_PAN12_DATAFRAME = 'Load'\n",
        "CREATE_FULL_PJ_DATAFRAME = 'Load'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "X7ncp3Hf4hJX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7ncp3Hf4hJX",
        "outputId": "3286ae7f-0849-41d9-d87f-2bb9342b8ba9"
      },
      "outputs": [],
      "source": [
        "# %pip install pyspellchecker\n",
        "# %python -m spacy download en_core_web_sm\n",
        "# %pip install pyLDAvis\n",
        "# %pip install altair\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98514e4d",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "683f43e6-ff1c-4c28-af4c-3452553fc476",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683f43e6-ff1c-4c28-af4c-3452553fc476",
        "outputId": "e026db55-de59-40bd-bc1e-fe33a2fd15ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mryan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "d:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  from imp import reload\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "tqdm.pandas()\n",
        "from ipywidgets import IntProgress\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import altair\n",
        "\n",
        "# from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.corpora as corpora\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "import xml.etree.ElementTree as ET \n",
        "from xml.etree.ElementTree import ParseError\n",
        "\n",
        "import csv\n",
        "\n",
        "from typing import Dict, Callable, List, Dict, Set, Any\n",
        "import logging\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m3FuuqiQ2Sk3",
      "metadata": {
        "id": "m3FuuqiQ2Sk3"
      },
      "source": [
        "### Env control and folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "LAmVCG5d2Y-M",
      "metadata": {
        "id": "LAmVCG5d2Y-M"
      },
      "outputs": [],
      "source": [
        "# ENV = 'Colab'\n",
        "ENV = 'Local'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bf9de114-ddde-43d7-9d6a-69d1b0a913d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9de114-ddde-43d7-9d6a-69d1b0a913d0",
        "outputId": "8384cef9-9e3c-47a2-ce08-e86f3a761b2c"
      },
      "outputs": [],
      "source": [
        "# Folders\n",
        "if ENV=='Local':\n",
        "  PROJECT_ROOT = Path('./')\n",
        "\n",
        "elif ENV=='Colab':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  PROJECT_ROOT = Path('/content/drive/MyDrive/colab_data/cyber2/')\n",
        "  \n",
        "\n",
        "PJ_DATA_FOLDER = PROJECT_ROOT / Path('customer_data')\n",
        "PAN12_DATA_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-test-corpus-2012-05-21/pan12-sexual-predator-identification-test-corpus-2012-05-17.xml')\n",
        "PAN12_LINE_LABELS_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-test-corpus-2012-05-21/pan12-sexual-predator-identification-groundtruth-problem2.txt')\n",
        "PAN12_USER_LABELS_FILE = PROJECT_ROOT / Path('ref_data/pan12_corpus/pan12-sexual-predator-identification-test-corpus-2012-05-21/pan12-sexual-predator-identification-groundtruth-problem1.txt')\n",
        "OUTPUT_FOLDER = PROJECT_ROOT / Path('output')\n",
        "\n",
        "if not PAN12_DATA_FILE.exists():\n",
        "    raise FileNotFoundError('File not found!')\n",
        "\n",
        "if not PAN12_LINE_LABELS_FILE.exists():\n",
        "    raise FileNotFoundError('File not found!')  \n",
        "\n",
        "if not PAN12_USER_LABELS_FILE.exists():\n",
        "    raise FileNotFoundError('File not found!') \n",
        "\n",
        "if not PJ_DATA_FOLDER.is_dir():\n",
        "    raise FileNotFoundError('Directry not found!') \n",
        "\n",
        "if not OUTPUT_FOLDER.is_dir():\n",
        "    print(f'creating output folder: {OUTPUT_FOLDER}')\n",
        "    OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037ad22d",
      "metadata": {
        "id": "037ad22d"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eaf51658",
      "metadata": {
        "id": "eaf51658"
      },
      "outputs": [],
      "source": [
        "# Define datasets with texts and labels\n",
        "\n",
        "def list_files_in_dir(folder: Path, extension='*') -> List:\n",
        "    \n",
        "    file_list = [f for f in folder.glob(f'**/*.{extension}') if f.is_file()]\n",
        "    return file_list\n",
        "\n",
        "## Test funcion\n",
        "# list_files_in_dir(DATA_FOLDER, 'dtd')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065d0df3",
      "metadata": {
        "id": "065d0df3"
      },
      "source": [
        "### Load word lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "70954f6b",
      "metadata": {
        "id": "70954f6b"
      },
      "outputs": [],
      "source": [
        "# Load word lists\n",
        "SEX_WL_PATH = PROJECT_ROOT / Path(r'sex_words.txt')\n",
        "with open(SEX_WL_PATH, 'rt') as handle:\n",
        "    sex_word_list = handle.read().split('\\n')\n",
        "\n",
        "MEETING_WL_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'meeting_words.txt')\n",
        "with open(MEETING_WL_PATH, 'rt') as handle:\n",
        "    meeting_word_list = handle.read().split('\\n')\n",
        "\n",
        "FAMILY_WL_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'family_words.txt')\n",
        "with open(FAMILY_WL_PATH, 'rt') as handle:\n",
        "    family_word_list = handle.read().split('\\n')\n",
        "\n",
        "CHAT_SLANG_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'chat_slang.txt')\n",
        "with open(CHAT_SLANG_PATH, mode='rt') as handle:\n",
        "    csv_reader = csv.reader(handle, delimiter='\\t')\n",
        "    chat_slang = {rows[0]:rows[1] for rows in csv_reader}\n",
        "\n",
        "EMOTICONS_PATH = SEX_WL_PATH = PROJECT_ROOT / Path(r'emoticons.txt')\n",
        "with open(EMOTICONS_PATH, mode='rt', encoding=\"utf8\") as handle:\n",
        "    csv_reader = csv.reader(handle, delimiter='\\t')\n",
        "    emoticons = {rows[0]:rows[1] for rows in csv_reader}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0844b40",
      "metadata": {
        "id": "b0844b40",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e182ed0",
      "metadata": {
        "id": "3e182ed0",
        "tags": []
      },
      "source": [
        "### Chat text preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "34d2873c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "34d2873c",
        "outputId": "b23b6c28-f30f-4d69-9d8f-3e7a0e752c33"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_stopwords(text: str, words_to_remove: List[str])-> str:\n",
        "    '''\n",
        "    Gets string, returns it without stopwords\n",
        "    '''\n",
        "    return \" \".join([word for word in str(text).split() if word not in words_to_remove])\n",
        "\n",
        "\n",
        "def stem_text(text: str, stemmer: Any)-> str:\n",
        "    '''\n",
        "    stem text string\n",
        "    '''\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def remove_emoji(text: str) -> str:\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_emoticons(text: str, emoticons: Dict) -> str:\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in emoticons) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def replace_pornsites_with_string(text:str, replacement_string:str='porn')->str:\n",
        "    pornsite_pattern = re.compile(r'\\S+xnxx\\.co\\S+' + r'|\\S+pornhub\\.co\\S+' + r'|\\S+nude\\.co\\S+' + r'|\\S+sex\\.co\\S+')\n",
        "    return pornsite_pattern.sub(replacement_string, text)\n",
        "\n",
        "def remove_urls(text:str)-> str:\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_special_characters(text:str)-> str:\n",
        "    special_chars_pattern = re.compile(r'[^A-Za-z0-9 ]+')\n",
        "    return special_chars_pattern.sub(r' ', text)\n",
        "\n",
        "\n",
        "def replace_chat_slang(text: str, chat_slang: Dict[str, str])-> str:\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_slang.keys():\n",
        "            new_text.append(chat_slang[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "\n",
        "def correct_spellings(text: str, speller: Callable) -> str:\n",
        "    corrected_text = []\n",
        "    misspelled_words = speller.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(speller.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "\n",
        "def lemmation(text:str, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    nlp_lem = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "    result = nlp_lem(text)\n",
        "    result = [token.lemma_ for token in result if token.pos_ in allowed_postags]\n",
        "    result = str(result)\n",
        "    return result\n",
        "\n",
        "\n",
        "def contains_words_from_list(text: str, word_list: List[str])-> bool:\n",
        "    text_words = re.sub(\"[^\\w]\", \" \",  text).split()\n",
        "    if any(word in word_list for word in text_words):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def preprocess_string_for_bow(text: str, stemmer: Callable=None, speller: Callable=None, words_to_remove:List[str]=None, emoticons: Dict[str, str]=None, chat_slang: Dict[str, str]=None)-> str:\n",
        "    try:\n",
        "        text = remove_emoji(text)\n",
        "        text = remove_emoticons(text, emoticons)\n",
        "        text = replace_chat_slang(text, chat_slang)\n",
        "        text = text.lower()\n",
        "        text = replace_pornsites_with_string(text)\n",
        "        text = remove_urls(text)\n",
        "        text = remove_special_characters(text)\n",
        "        text = correct_spellings(text, speller)\n",
        "        # text = remove_stopwords(text, words_to_remove)\n",
        "        text = lemmation(text)\n",
        "        # text = stem_text(text, stemmer)\n",
        "    except(TypeError):\n",
        "        print(f'Problematic string: {text}')\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_df_for_bow(df: pd.DataFrame, text_col: str, output_col_name='preprocessed_bow', stemmer=None, speller=None, words_to_remove=None, emoticons=None, chat_slang=None)-> pd.DataFrame:\n",
        "    '''\n",
        "    Gets a PD dataframe and a text column name\n",
        "    returns the same dataframe with additional column called 'posts_preprocessed_bow'\n",
        "    '''\n",
        "    df[output_col_name] = df[text_col].progress_apply(lambda text: preprocess_string_for_bow(text, stemmer=stemmer, speller=speller, words_to_remove=words_to_remove, emoticons=emoticons, chat_slang=chat_slang))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fb38ab5c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"['just', 'want', 'see', 'go', 'friend', 'get', 'arrest', 'do', 'same', 'thing', 'year', 'old', 'set', 'type', 'thing']\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "preprocess_args = {'stemmer': PorterStemmer(),\n",
        "                    'speller': SpellChecker(),\n",
        "                    'words_to_remove': set(stopwords.words('english')),\n",
        "                    'emoticons': emoticons,\n",
        "                    'chat_slang': chat_slang,\n",
        "                    }\n",
        "\n",
        "text = 'r u going to www.google.com http://xnxx.com im walking LOL ths is not &amp;right im caming flight now u r right brb and fu :-)'\n",
        "text = 'yeah--well I just want to see you before I go in the apt--cause one of my friends got arrested for doing the same thing with a 16 year old--it was a set-up type thing'\n",
        "\n",
        "preprocess_string_for_bow(text, **preprocess_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bd5e29",
      "metadata": {},
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a594d2",
      "metadata": {},
      "source": [
        "### Word-list based features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8de75393",
      "metadata": {},
      "outputs": [],
      "source": [
        "def contains_words_from_list(text: str, word_list: List[str])-> bool:\n",
        "    text_words = re.sub(\"[^\\w]\", \" \",  text).split()\n",
        "    if any(word in word_list for word in text_words):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def add_wordlist_features(df: pd.DataFrame, text_column: str, sex_word_list, family_word_list, meeting_word_list):\n",
        "    df['contains_sex_words'] = df[text_column].apply(lambda text: contains_words_from_list(text, sex_word_list))\n",
        "    df['contains_family_words'] = df[text_column].apply(lambda text: contains_words_from_list(text, family_word_list))\n",
        "    df['contains_meeting_words'] = df[text_column].apply(lambda text: contains_words_from_list(text, meeting_word_list))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d31dd2",
      "metadata": {
        "id": "f4d31dd2",
        "tags": []
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8db35ba",
      "metadata": {},
      "source": [
        "### PJ dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8fc852-a907-4706-a742-14207e2eccf2",
      "metadata": {
        "id": "0f8fc852-a907-4706-a742-14207e2eccf2"
      },
      "source": [
        "#### PJ Convesation level dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "00ea3ef6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "00ea3ef6",
        "outputId": "d17865e4-16d2-4647-eec6-8d3348a5ac1c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>USERNAME</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>BODY</th>\n",
              "      <th>COMMENT</th>\n",
              "      <th>CODING</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:02:01 pm)</td>\n",
              "      <td>im dennis us army soldier from cincinnati</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:02:30 pm)</td>\n",
              "      <td>hi im becky from ky</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:02:35 pm)</td>\n",
              "      <td>how old ru</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:02:42 pm)</td>\n",
              "      <td>13 how old ru</td>\n",
              "      <td>(age stated and he didn't bat an eye)</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:02:44 pm)</td>\n",
              "      <td>u single</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:02:51 pm)</td>\n",
              "      <td>yeah</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:03:03 pm)</td>\n",
              "      <td>i had a bf but we broke up when i moved here</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:03:11 pm)</td>\n",
              "      <td>ok u have sex at 13</td>\n",
              "      <td>(he obviously knows my age)</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>peekaboo1293</td>\n",
              "      <td>(7:03:28 pm)</td>\n",
              "      <td>u mean did i ever</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>armysgt1961</td>\n",
              "      <td>(7:03:32 pm)</td>\n",
              "      <td>yeah</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       USERNAME      DATETIME                                          BODY  \\\n",
              "0   armysgt1961  (7:02:01 pm)     im dennis us army soldier from cincinnati   \n",
              "1  peekaboo1293  (7:02:30 pm)                           hi im becky from ky   \n",
              "2   armysgt1961  (7:02:35 pm)                                    how old ru   \n",
              "3  peekaboo1293  (7:02:42 pm)                                 13 how old ru   \n",
              "4   armysgt1961  (7:02:44 pm)                                      u single   \n",
              "5  peekaboo1293  (7:02:51 pm)                                          yeah   \n",
              "6  peekaboo1293  (7:03:03 pm)  i had a bf but we broke up when i moved here   \n",
              "7   armysgt1961  (7:03:11 pm)                           ok u have sex at 13   \n",
              "8  peekaboo1293  (7:03:28 pm)                             u mean did i ever   \n",
              "9   armysgt1961  (7:03:32 pm)                                          yeah   \n",
              "\n",
              "                                 COMMENT CODING  \n",
              "0                                   <NA>      \n",
              "  \n",
              "1                                   <NA>   <NA>  \n",
              "2                                   <NA>      \n",
              "  \n",
              "3  (age stated and he didn't bat an eye)   <NA>  \n",
              "4                                   <NA>      \n",
              "  \n",
              "5                                   <NA>   <NA>  \n",
              "6                                   <NA>   <NA>  \n",
              "7            (he obviously knows my age)      \n",
              "  \n",
              "8                                   <NA>   <NA>  \n",
              "9                                   <NA>   <NA>  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_one_chat_as_df_pj(file_path: Path) -> Dict[str, pd.DataFrame]:\n",
        "    '''\n",
        "    Gets an path to a PJ XML file\n",
        "    returns a dict with three dataframes:\n",
        "        - victim data\n",
        "        - predator data\n",
        "        - conversation posts\n",
        "    '''\n",
        "    parser = ET.XMLParser(encoding=\"utf-8\")\n",
        "    try:\n",
        "        doc_tree = ET.parse(file_path, parser=parser)\n",
        "    except(ParseError):\n",
        "        print(f'failed to parse {str(file_path)}')\n",
        "        return None\n",
        "        \n",
        "    doc_root = doc_tree.getroot()\n",
        "    \n",
        "    posts_df = pd.DataFrame(columns = ['USERNAME', 'DATETIME', 'BODY', 'COMMENT', 'CODING'], dtype=str)\n",
        "    predator_df = pd.DataFrame(columns = ['FIRSTNAME', 'LASTNAME', 'STATEDNAME', 'STATEDAGE', 'GENDER', 'RACE', 'CITY', 'STATE', 'REPEATOFFENDER', 'ADMITGUILT', 'TRUTHFULNAME', 'SCREENNAME'], dtype=str)\n",
        "    victim_df = pd.DataFrame(columns = ['FIRSTNAME', 'LASTNAME', 'STATEDNAME', 'STATEDAGE', 'GENDER', 'RACE', 'CITY', 'STATE', 'PREVIOUSVICTIMIZATION', 'ADMITGUILT', 'SCREENNAME'], dtype=str)\n",
        "\n",
        "    for post in doc_root.findall('POST'):\n",
        "        post_dict = {}\n",
        "        for field in post:\n",
        "            post_dict[field.tag] = field.text\n",
        "\n",
        "        posts_df = posts_df.append(post_dict, ignore_index=True)\n",
        "    posts_df = posts_df.astype('string')\n",
        "\n",
        "\n",
        "    for predator in doc_root.findall('PREDATOR'):\n",
        "        predator_dict = {}\n",
        "        for field in predator:\n",
        "            predator_dict[field.tag] = field.text\n",
        "\n",
        "        predator_df = predator_df.append(predator_dict, ignore_index=True)   \n",
        "    predator_df = predator_df.astype('string')\n",
        "\n",
        "    for victim in doc_root.findall('VICTIM'):\n",
        "        victim_dict = {}\n",
        "        for field in victim:\n",
        "            victim_dict[field.tag] = field.text\n",
        "\n",
        "        victim_df = victim_df.append(victim_dict, ignore_index=True)  \n",
        "    victim_df = victim_df.astype('string')\n",
        "\n",
        "    return {'predator': predator_df, 'victim': victim_df, 'conversation': posts_df, 'conversation_id': str(file_path.parts[-1])}\n",
        "\n",
        "\n",
        "#----------------------------------------------------------\n",
        "# Test XML parse functions:\n",
        "file_path = PJ_DATA_FOLDER / Path('ArmySgt1961.xml')\n",
        "chat_dict = load_one_chat_as_df_pj(file_path)\n",
        "chat_dict['victim'].head()\n",
        "chat_dict['predator'].head()\n",
        "chat_dict['conversation'].head(10)\n",
        "# chat_dict['conversation_id']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3818aedd-70b1-4802-b66b-6f8116cb4dbf",
      "metadata": {
        "id": "3818aedd-70b1-4802-b66b-6f8116cb4dbf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PjSentencesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wrapper around Torch Dataset.\n",
        "    Prepares an indexed list of PJ conversation in a folder, returns conversations per index (like an array)\n",
        "    Load is lazy - loads conversation from disk on request.\n",
        "    Uses load_one_chat_as_df_pj() for conversation loading\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder: Path, df_preprocess_fn=None, df_preprocess_args:Dict=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          data_folder - folder with PJ XML files\n",
        "          df_preprocess_fn - function that gets a dataframe and adds preprocesed text column based on given text column\n",
        "\n",
        "        \"\"\"\n",
        "       \n",
        "        self.file_list = list_files_in_dir(data_folder, extension='xml')\n",
        "        self.df_preprocess_fn = df_preprocess_fn\n",
        "        self.df_preprocess_args = df_preprocess_args\n",
        "        self.TEXT_COLUMN_NAME = 'BODY'\n",
        "\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Gets element of the dataset\n",
        "\n",
        "        Args:\n",
        "            index (int): index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"        \n",
        "        sample = load_one_chat_as_df_pj(self.file_list[idx])\n",
        "        if (self.df_preprocess_fn is not None) and (sample is not None):\n",
        "            sample['conversation'] = self.df_preprocess_fn(sample['conversation'], self.TEXT_COLUMN_NAME, **self.df_preprocess_args)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7562daf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "7562daf7",
        "outputId": "29bd848e-bb88-464c-ad87-1e7ffd617ae3"
      },
      "outputs": [],
      "source": [
        "# # Test the dataset\n",
        "# preprocess_args = {'stemmer': PorterStemmer(),\n",
        "#                     'speller': SpellChecker(),\n",
        "#                     'words_to_remove': set(stopwords.words('english')),\n",
        "#                     'emoticons': emoticons,\n",
        "#                     'chat_slang': chat_slang,\n",
        "#                     }\n",
        "                    \n",
        "# pj_ds = PjSentencesDataset(PJ_DATA_FOLDER, df_preprocess_fn=preprocess_df_for_bow, df_preprocess_args=preprocess_args)\n",
        "# print(len(pj_ds))\n",
        "# print(pj_ds[1]['conversation_id'])\n",
        "# pj_ds[1]['conversation'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886275cc",
      "metadata": {},
      "source": [
        "### Load entire PJ dataset as single dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c8a5905e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create full dataframe, no preprocessing yet\n",
        "\n",
        "def load_pj_dataset(data_folder:Path):\n",
        "    pj_df = None                    \n",
        "    pj_ds = PjSentencesDataset(data_folder)\n",
        "\n",
        "    for i in tqdm(range(len(pj_ds))):\n",
        "        conversation_dict = pj_ds[i]\n",
        "        if not conversation_dict is None:\n",
        "            conversation = conversation_dict['conversation']\n",
        "            conversation['conversation_id'] = conversation_dict['conversation_id']\n",
        "\n",
        "            if not pj_df is None:\n",
        "                pj_df = pj_df.append(conversation)\n",
        "            else:\n",
        "                pj_df = conversation.copy()\n",
        "    \n",
        "    return pj_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bd051e12",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>USERNAME</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>BODY</th>\n",
              "      <th>COMMENT</th>\n",
              "      <th>CODING</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>preprocessed_bow</th>\n",
              "      <th>contains_sex_words</th>\n",
              "      <th>contains_family_words</th>\n",
              "      <th>contains_meeting_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tunnels12000</td>\n",
              "      <td>07/19/06  7:48:24 PM</td>\n",
              "      <td>hi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>tracy_in_xcess</td>\n",
              "      <td>(07/19/06  7:49:06 PM)</td>\n",
              "      <td>hi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>[]</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>tunnels12000</td>\n",
              "      <td>07/19/06  7:49:09 PM</td>\n",
              "      <td>very pretty pic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>['very', 'pretty', 'pic']</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>tunnels12000</td>\n",
              "      <td>07/19/06  7:49:19 PM</td>\n",
              "      <td>im david hope i didnt bother u</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\n</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>['bother']</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>tracy_in_xcess</td>\n",
              "      <td>07/19/06  7:49:48 PM</td>\n",
              "      <td>no thats ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tunnels12000.xml</td>\n",
              "      <td>['s']</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        USERNAME                DATETIME  \\\n",
              "0           0    tunnels12000    07/19/06  7:48:24 PM   \n",
              "1           1  tracy_in_xcess  (07/19/06  7:49:06 PM)   \n",
              "2           2    tunnels12000    07/19/06  7:49:09 PM   \n",
              "3           3    tunnels12000    07/19/06  7:49:19 PM   \n",
              "4           4  tracy_in_xcess    07/19/06  7:49:48 PM   \n",
              "\n",
              "                             BODY COMMENT CODING   conversation_id  \\\n",
              "0                              hi     NaN    NaN  tunnels12000.xml   \n",
              "1                              hi     NaN    NaN  tunnels12000.xml   \n",
              "2                 very pretty pic     NaN    NaN  tunnels12000.xml   \n",
              "3  im david hope i didnt bother u     NaN     \\n  tunnels12000.xml   \n",
              "4                     no thats ok     NaN    NaN  tunnels12000.xml   \n",
              "\n",
              "            preprocessed_bow  contains_sex_words  contains_family_words  \\\n",
              "0                         []               False                  False   \n",
              "1                         []               False                  False   \n",
              "2  ['very', 'pretty', 'pic']               False                  False   \n",
              "3                 ['bother']               False                  False   \n",
              "4                      ['s']               False                  False   \n",
              "\n",
              "   contains_meeting_words  \n",
              "0                   False  \n",
              "1                   False  \n",
              "2                   False  \n",
              "3                   False  \n",
              "4                   False  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PJ_PREPROCESSED_CSV_PATH = OUTPUT_FOLDER / Path('pj_preprocessed_dataframe.csv')\n",
        "PJ_FULL_RAW_CSV = OUTPUT_FOLDER / Path('pan12_raw_full.csv')\n",
        "\n",
        "if CREATE_FULL_PJ_DATAFRAME == 'Process':\n",
        "    # load original dataset\n",
        "    pj_df = load_pj_dataset(PJ_DATA_FOLDER)\n",
        "    pj_df.to_csv(PJ_FULL_RAW_CSV)\n",
        "\n",
        "    # preprocess and add features\n",
        "    preprocess_args = {'stemmer': PorterStemmer(),\n",
        "                        'speller': SpellChecker(),\n",
        "                        'words_to_remove': set(stopwords.words('english')),\n",
        "                        'emoticons': emoticons,\n",
        "                        'chat_slang': chat_slang,\n",
        "                        }\n",
        "\n",
        "    pj_df = preprocess_df_for_bow(pj_df, 'BODY', **preprocess_args)\n",
        "    pj_df = add_wordlist_features(pj_df, 'preprocessed_bow', sex_word_list, family_word_list, meeting_word_list)\n",
        "    pj_df.to_csv(PJ_PREPROCESSED_CSV_PATH)\n",
        "\n",
        "elif CREATE_FULL_PJ_DATAFRAME == 'Load':\n",
        "    pj_df = pd.read_csv(PJ_PREPROCESSED_CSV_PATH)\n",
        "\n",
        "pj_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0fbbf9a3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>contains_sex_words</th>\n",
              "      <th>contains_family_words</th>\n",
              "      <th>contains_meeting_words</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conversation_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ArmySgt1961.xml</th>\n",
              "      <td>4560</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>arthinice.xml</th>\n",
              "      <td>574056</td>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aticloose.xml</th>\n",
              "      <td>13861</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corazon23456partio23456.xml</th>\n",
              "      <td>97020</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crazytrini85.xml</th>\n",
              "      <td>22366</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flxnonya.xml</th>\n",
              "      <td>6903</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fotophix.xml</th>\n",
              "      <td>5253</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ghost27_73.xml</th>\n",
              "      <td>3692403</td>\n",
              "      <td>60</td>\n",
              "      <td>36</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hiexcitement.xml</th>\n",
              "      <td>580503</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i_8u_raw.xml</th>\n",
              "      <td>1119756</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>icepirate53.xml</th>\n",
              "      <td>61776</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>italianlover37.xml</th>\n",
              "      <td>3160</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jleno9.xml</th>\n",
              "      <td>20100</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jon_raven2000.xml</th>\n",
              "      <td>40755</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lee_greer74.xml</th>\n",
              "      <td>1209790</td>\n",
              "      <td>27</td>\n",
              "      <td>22</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>manofdarkneedsl951.xml</th>\n",
              "      <td>15051</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>marc_00_48089.xml</th>\n",
              "      <td>114960</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>needinit1983.xml</th>\n",
              "      <td>77421</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sebastian_calif.xml</th>\n",
              "      <td>216153</td>\n",
              "      <td>69</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sjklanke.xml</th>\n",
              "      <td>221445</td>\n",
              "      <td>45</td>\n",
              "      <td>18</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sphinx_56_02.xml</th>\n",
              "      <td>1268028</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spongebob_giantdick.xml</th>\n",
              "      <td>7260</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stylelisticgrooves.xml</th>\n",
              "      <td>235641</td>\n",
              "      <td>29</td>\n",
              "      <td>21</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugardavis.xml</th>\n",
              "      <td>387640</td>\n",
              "      <td>35</td>\n",
              "      <td>30</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sweet_jason002.xml</th>\n",
              "      <td>229503</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texassailor04.xml</th>\n",
              "      <td>38503</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the_third_storm.xml</th>\n",
              "      <td>714610</td>\n",
              "      <td>36</td>\n",
              "      <td>10</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thedude420xxx.xml</th>\n",
              "      <td>142845</td>\n",
              "      <td>36</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tunnels12000.xml</th>\n",
              "      <td>2170486</td>\n",
              "      <td>100</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user194547.xml</th>\n",
              "      <td>11325</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Unnamed: 0  contains_sex_words  \\\n",
              "conversation_id                                               \n",
              "ArmySgt1961.xml                    4560                   5   \n",
              "arthinice.xml                    574056                  43   \n",
              "aticloose.xml                     13861                  16   \n",
              "corazon23456partio23456.xml       97020                   6   \n",
              "crazytrini85.xml                  22366                  11   \n",
              "flxnonya.xml                       6903                  13   \n",
              "fotophix.xml                       5253                   1   \n",
              "ghost27_73.xml                  3692403                  60   \n",
              "hiexcitement.xml                 580503                  12   \n",
              "i_8u_raw.xml                    1119756                  15   \n",
              "icepirate53.xml                   61776                   5   \n",
              "italianlover37.xml                 3160                   4   \n",
              "jleno9.xml                        20100                  17   \n",
              "jon_raven2000.xml                 40755                   1   \n",
              "lee_greer74.xml                 1209790                  27   \n",
              "manofdarkneedsl951.xml            15051                   9   \n",
              "marc_00_48089.xml                114960                  12   \n",
              "needinit1983.xml                  77421                   4   \n",
              "sebastian_calif.xml              216153                  69   \n",
              "sjklanke.xml                     221445                  45   \n",
              "sphinx_56_02.xml                1268028                  16   \n",
              "spongebob_giantdick.xml            7260                   0   \n",
              "stylelisticgrooves.xml           235641                  29   \n",
              "sugardavis.xml                   387640                  35   \n",
              "sweet_jason002.xml               229503                   3   \n",
              "texassailor04.xml                 38503                   5   \n",
              "the_third_storm.xml              714610                  36   \n",
              "thedude420xxx.xml                142845                  36   \n",
              "tunnels12000.xml                2170486                 100   \n",
              "user194547.xml                    11325                  14   \n",
              "\n",
              "                             contains_family_words  contains_meeting_words  \n",
              "conversation_id                                                             \n",
              "ArmySgt1961.xml                                  6                       4  \n",
              "arthinice.xml                                    7                       8  \n",
              "aticloose.xml                                    3                       6  \n",
              "corazon23456partio23456.xml                     13                       8  \n",
              "crazytrini85.xml                                 4                       3  \n",
              "flxnonya.xml                                     4                       1  \n",
              "fotophix.xml                                     2                       4  \n",
              "ghost27_73.xml                                  36                      44  \n",
              "hiexcitement.xml                                 3                      18  \n",
              "i_8u_raw.xml                                    19                      51  \n",
              "icepirate53.xml                                  8                      11  \n",
              "italianlover37.xml                               1                       3  \n",
              "jleno9.xml                                       2                       2  \n",
              "jon_raven2000.xml                                3                      11  \n",
              "lee_greer74.xml                                 22                      19  \n",
              "manofdarkneedsl951.xml                           1                       1  \n",
              "marc_00_48089.xml                                4                      34  \n",
              "needinit1983.xml                                 2                       7  \n",
              "sebastian_calif.xml                              4                      22  \n",
              "sjklanke.xml                                    18                      34  \n",
              "sphinx_56_02.xml                                 3                      30  \n",
              "spongebob_giantdick.xml                          0                       3  \n",
              "stylelisticgrooves.xml                          21                      16  \n",
              "sugardavis.xml                                  30                      33  \n",
              "sweet_jason002.xml                              25                      48  \n",
              "texassailor04.xml                               12                       8  \n",
              "the_third_storm.xml                             10                      39  \n",
              "thedude420xxx.xml                                5                       0  \n",
              "tunnels12000.xml                                23                      23  \n",
              "user194547.xml                                   1                       3  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pj_df.groupby(['conversation_id']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aa278e",
      "metadata": {
        "id": "d3aa278e"
      },
      "source": [
        "### Convert Pan12 to labeled datafreame for use later as Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2d0e52c1",
      "metadata": {
        "id": "2d0e52c1"
      },
      "outputs": [],
      "source": [
        "class Pan12converterToDF():\n",
        "\n",
        "    # Pan12 converter for TEST dataset - with line labels!\n",
        "    \n",
        "    \"\"\"\n",
        "    Wrapper around Torch Dataset to perform text classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chat_data_file: Path, user_labels_file: Path=None, line_labels_file: Path=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            chat_data_file: path to chat xml file\n",
        "            conversation_labels:\n",
        "            line_labels:\n",
        "            mode:   full - all data \n",
        "                    positive_lines - Only lines labeled as problematic\n",
        "        \"\"\"\n",
        "       \n",
        "        self.chat_data_file = chat_data_file\n",
        "        self.conversations = self._get_conversation_roots(chat_data_file)\n",
        "\n",
        "        self.user_labels_file = user_labels_file\n",
        "        self.line_labels_file = line_labels_file\n",
        "        self.TEXT_COLUMN_NAME = 'text'\n",
        "\n",
        "        self.length = self._get_ds_length()\n",
        "        self.num_conversations = len(self.conversations)\n",
        "\n",
        "        # Initiate queue\n",
        "        self.message_list = None\n",
        "        self.current_conversation_id = None\n",
        "        self.next_conversation_idx = 0\n",
        "        self.next_message_idx = 0\n",
        "\n",
        "        # Create sets of problematic lines and authors for labels\n",
        "        user_labels = pd.read_csv(user_labels_file, delimiter='\\t', header=None)\n",
        "        self.perverted_authors = set(user_labels[0])\n",
        "\n",
        "        line_labels = pd.read_csv(line_labels_file, delimiter='\\t', header=None)\n",
        "        line_labels['concat'] = line_labels[0] + '_' + line_labels[1].astype(str)\n",
        "        self.perverted_conversations = set(line_labels[0].unique())\n",
        "        self.pervert_lines = set(line_labels['concat'])\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def convert(self, filename:Path, save_every=2000, mode: str='full') -> pd.DataFrame:\n",
        "        \"\"\"Gets element of the dataset\n",
        "\n",
        "        Args:\n",
        "            index (int): index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"        \n",
        "        pan12_df = pd.DataFrame(columns=['conversation_id', 'line', 'author', 'time', 'text', 'line_label', 'author_label'])\n",
        "\n",
        "        self._load_next_conversation_to_list(mode) \n",
        "\n",
        "        if(mode == 'full'):\n",
        "            iter_len = self.length\n",
        "        elif(mode == 'positive_lines'):\n",
        "            iter_len = len(self.pervert_lines)\n",
        "        \n",
        "        for i in tqdm(range(iter_len)):\n",
        "            message_dict = {}\n",
        "            try:\n",
        "                message = self.message_list[self.next_message_idx]\n",
        "            except(IndexError):\n",
        "                self._load_next_conversation_to_list(mode)\n",
        "                message = self.message_list[self.next_message_idx]\n",
        "            \n",
        "            message_dict['conversation_id'] = self.current_conversation_id\n",
        "            self.next_message_idx += 1\n",
        "            \n",
        "            message_dict['line'] = message.attrib['line']  \n",
        "            for field in message:\n",
        "                message_dict[field.tag] = field.text\n",
        "            \n",
        "            message_dict['author_label'] = 1 if message_dict['author'] in self.perverted_authors else 0\n",
        "            message_dict['line_label'] = 1 if message_dict['conversation_id'] + '_' + message_dict['line'] in self.pervert_lines else 0\n",
        "            \n",
        "            pan12_df = pan12_df.append(message_dict, ignore_index=True)\n",
        "            if i % save_every == 0:\n",
        "                pan12_df.to_csv(filename)\n",
        "                print('.', end='')\n",
        "\n",
        "            # #######\n",
        "            # if i == 1001:\n",
        "            #     print(pan12_df.head(2001))\n",
        "            #     break\n",
        "            # ######\n",
        "        pan12_df.to_csv(filename)\n",
        "        return pan12_df\n",
        "    \n",
        "    def _get_conversation_roots(self, file_path):\n",
        "        doc_tree = ET.parse(file_path)\n",
        "        conversation_roots = doc_tree.getroot().findall('conversation')\n",
        "        return conversation_roots\n",
        "\n",
        "    def _get_ds_length(self):\n",
        "        number_messages = 0\n",
        "        for conversation in self.conversations:\n",
        "            number_messages += len(conversation.findall('message'))\n",
        "        \n",
        "        return number_messages\n",
        "\n",
        "    def _load_next_conversation_to_list(self, mode):\n",
        "        try:\n",
        "            conversation = self.conversations[self.next_conversation_idx] \n",
        "            self.next_conversation_idx += 1\n",
        "            self.current_conversation_id = conversation.attrib['id']  \n",
        "\n",
        "            if mode == 'positive_lines':\n",
        "                while self.current_conversation_id not in self.perverted_conversations:\n",
        "                    conversation = self.conversations[self.next_conversation_idx] \n",
        "                    self.next_conversation_idx += 1\n",
        "                    self.current_conversation_id = conversation.attrib['id']  \n",
        "     \n",
        "        except(IndexError):\n",
        "            raise StopIteration()\n",
        "\n",
        "        if mode == 'positive_lines':\n",
        "            self.message_list = [m for m in conversation.findall('message') if (self.current_conversation_id + '_' + m.attrib['line'] in self.pervert_lines)]\n",
        "        else:\n",
        "            self.message_list = [m for m in conversation.findall('message')]\n",
        "        self.next_message_idx = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92dd4938",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505,
          "referenced_widgets": [
            "5a30eca9cd274aedba3c3cfee08e986a",
            "e80d4d0d52a54f9ab8ac1f41f281366e",
            "7f1f823c15724c948715c043b9ea89b9",
            "14c4346449c64fa4a695d62f33d10cc1",
            "e032f1f7e816478a96c3c1f9246034c6",
            "bc6c2d7cdac74c079d40c0e1c69aaeec",
            "6aebf996aa9c4960bf0a46ebca04d783",
            "45501c874a53430ea0979ac01e4dcd87",
            "295caedfec664f0bacdab9553c5e674d",
            "3304f532f9c34d59bf10cf3a4b143a8f",
            "cbd617ce94ed4af587dd6dfa9789ac2d"
          ]
        },
        "id": "92dd4938",
        "outputId": "077a296b-5be6-4f27-9be2-c63377c83a1a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'output\\\\pan12_full_lines_preprocessed.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\Tasks\\Task2\\task2_code\\cyber_hw2.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/task2_code/cyber_hw2.ipynb#ch0000033?line=26'>27</a>\u001b[0m     pan12_df\u001b[39m.\u001b[39mto_csv(PAN12_PERVERTED_LINES_CSV)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/task2_code/cyber_hw2.ipynb#ch0000033?line=28'>29</a>\u001b[0m \u001b[39melif\u001b[39;00m CREATE_FULL_PAN12_DATAFRAME \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLoad\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/task2_code/cyber_hw2.ipynb#ch0000033?line=29'>30</a>\u001b[0m     pan12_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(PAN12_PERVERTED_LINES_CSV)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/task2_code/cyber_hw2.ipynb#ch0000033?line=31'>32</a>\u001b[0m pan12_df \u001b[39m=\u001b[39m pan12_df\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/task2_code/cyber_hw2.ipynb#ch0000033?line=32'>33</a>\u001b[0m pan12_df\n",
            "File \u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=573'>574</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=574'>575</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=576'>577</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=929'>930</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=932'>933</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1212'>1213</a>\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1213'>1214</a>\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1214'>1215</a>\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1215'>1216</a>\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1216'>1217</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1217'>1218</a>\u001b[0m     f,\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1218'>1219</a>\u001b[0m     mode,\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1219'>1220</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1220'>1221</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1221'>1222</a>\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1222'>1223</a>\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1223'>1224</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1224'>1225</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1225'>1226</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1226'>1227</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1227'>1228</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[1;32md:\\docs\\DSML_IDC\\Semester 4\\Cyber\\venv\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=783'>784</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=784'>785</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=785'>786</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=786'>787</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=787'>788</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=788'>789</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=789'>790</a>\u001b[0m             handle,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=790'>791</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=791'>792</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=792'>793</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=793'>794</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=794'>795</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=795'>796</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=796'>797</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/docs/DSML_IDC/Semester%204/Cyber/venv/lib/site-packages/pandas/io/common.py?line=797'>798</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output\\\\pan12_full_lines_preprocessed.csv'"
          ]
        }
      ],
      "source": [
        "# PAN12_PERVERTED_LINES_CSV = OUTPUT_FOLDER / Path('pan12_perverted_lines_preprocessed.csv')\n",
        "PAN12_PERVERTED_LINES_CSV = OUTPUT_FOLDER / Path('pan12_full_lines_preprocessed.csv')\n",
        "\n",
        "PAN12_FULL_RAW_CSV = OUTPUT_FOLDER / Path('pan12_raw_full.csv')\n",
        "\n",
        "if CREATE_FULL_PAN12_DATAFRAME == 'Process':\n",
        "    # Create a dataframe of all pan12 test perverted lines\n",
        "    pan12_converter = Pan12converterToDF(PAN12_DATA_FILE, user_labels_file=PAN12_USER_LABELS_FILE, line_labels_file=PAN12_LINE_LABELS_FILE)\n",
        "    print(len(pan12_converter))\n",
        "    # pan12_df = pan12_converter.convert(PAN12_FULL_RAW_CSV, mode='positive_lines')\n",
        "    pan12_df = pan12_converter.convert(PAN12_FULL_RAW_CSV, mode='full')\n",
        "    print(f'lines in pan12_df: {len(pan12_df)}')\n",
        "\n",
        "    # Preprocess pan12 perverted lines only and save to csv\n",
        "    preprocess_args = {'stemmer': PorterStemmer(),\n",
        "                        'speller': SpellChecker(),\n",
        "                        'words_to_remove': set(stopwords.words('english')),\n",
        "                        'emoticons': emoticons,\n",
        "                        'chat_slang': chat_slang\n",
        "                        }\n",
        "\n",
        "    pan12_df = preprocess_df_for_bow(pan12_df, 'text', **preprocess_args)\n",
        "    pan12_df.to_csv(PAN12_PERVERTED_LINES_CSV)\n",
        "\n",
        "    # add features to pan12 df\n",
        "    pan12_df = add_wordlist_features(pan12_df, 'preprocessed_bow', sex_word_list, family_word_list, meeting_word_list)\n",
        "    pan12_df.to_csv(PAN12_PERVERTED_LINES_CSV)\n",
        "\n",
        "elif CREATE_FULL_PAN12_DATAFRAME == 'Load':\n",
        "    pan12_df = pd.read_csv(PAN12_PERVERTED_LINES_CSV)\n",
        "\n",
        "pan12_df = pan12_df.dropna()\n",
        "pan12_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1043fe71",
      "metadata": {},
      "source": [
        "## Temp section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6658e8a1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "conversation_id    object\n",
              "line                int64\n",
              "author             object\n",
              "time               object\n",
              "text               string\n",
              "line_label          int64\n",
              "author_label        int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pan12_df = pd.read_csv(PAN12_FULL_RAW_CSV, index_col=0)\n",
        "pan12_df = pan12_df.dropna()\n",
        "pan12_df.text = pan12_df.text.astype('string')\n",
        "pan12_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8cf76db4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['bugmail',\n",
              "  'bug',\n",
              "  'new',\n",
              "  'mark',\n",
              "  'eof',\n",
              "  'terminated',\n",
              "  'script',\n",
              "  'elements',\n",
              "  'as',\n",
              "  'malformed',\n",
              "  'lt',\n",
              "  'http',\n",
              "  'lists',\n",
              "  'org',\n",
              "  'archives',\n",
              "  'public',\n",
              "  'public',\n",
              "  'html',\n",
              "  'bugzilla',\n",
              "  'may',\n",
              "  'html',\n",
              "  'gt'],\n",
              " ['henri', 'can', 'ask', 'you', 'firefox', 'build', 'question', 'windows'],\n",
              " ['cfda', 'sure', 'but', 'probably', 'don', 'know', 'the', 'answer'],\n",
              " ['it',\n",
              "  'appears',\n",
              "  'the',\n",
              "  'build',\n",
              "  'runs',\n",
              "  'through',\n",
              "  'it',\n",
              "  'creates',\n",
              "  'firefox',\n",
              "  'exe',\n",
              "  'in',\n",
              "  'dist',\n",
              "  'bin'],\n",
              " ['when',\n",
              "  'start',\n",
              "  'it',\n",
              "  'get',\n",
              "  'my',\n",
              "  'standard',\n",
              "  'install',\n",
              "  'of',\n",
              "  'ff',\n",
              "  'instead'],\n",
              " ['same',\n",
              "  'if',\n",
              "  'make',\n",
              "  'package',\n",
              "  'unzip',\n",
              "  'it',\n",
              "  'and',\n",
              "  'start',\n",
              "  'from',\n",
              "  'there'],\n",
              " ['cfda', 'do', 'you', 'already', 'have', 'the', 'usual', 'firefox', 'open'],\n",
              " ['likely'],\n",
              " ['so', 'do', 'need', 'to', 'close', 'all', 'instances'],\n",
              " ['other'],\n",
              " ['cfda',\n",
              "  'at',\n",
              "  'least',\n",
              "  'with',\n",
              "  'the',\n",
              "  'linux',\n",
              "  'version',\n",
              "  'you',\n",
              "  'need',\n",
              "  'to'],\n",
              " ['unless',\n",
              "  'you',\n",
              "  'run',\n",
              "  'different',\n",
              "  'profile',\n",
              "  'which',\n",
              "  'may',\n",
              "  'be',\n",
              "  'advisable',\n",
              "  'anyway'],\n",
              " ['good', 'point'],\n",
              " ['will', 'be', 'gone', 'for', 'moment', 'due', 'to', 'chatzilla'],\n",
              " ['henri', 'that', 'was', 'the', 'problem'],\n",
              " ['ok',\n",
              "  'good',\n",
              "  'that',\n",
              "  'can',\n",
              "  'guess',\n",
              "  'some',\n",
              "  'windows',\n",
              "  'things',\n",
              "  'from',\n",
              "  'linux',\n",
              "  'behavior'],\n",
              " ['ah', 'yeah', 'firefox', 'is', 'annoying', 'with', 'that'],\n",
              " ['cfda',\n",
              "  'was',\n",
              "  'merely',\n",
              "  'citing',\n",
              "  'it',\n",
              "  'because',\n",
              "  'your',\n",
              "  'message',\n",
              "  'made',\n",
              "  'it',\n",
              "  'seem',\n",
              "  'as',\n",
              "  'if',\n",
              "  'you',\n",
              "  'thought',\n",
              "  'there',\n",
              "  'was',\n",
              "  'wg',\n",
              "  'agreement'],\n",
              " ['and',\n",
              "  'thought',\n",
              "  'there',\n",
              "  'was',\n",
              "  'some',\n",
              "  'treshold',\n",
              "  'criteria',\n",
              "  'for',\n",
              "  'the',\n",
              "  'tracker',\n",
              "  'but',\n",
              "  'maybe',\n",
              "  'things',\n",
              "  'changed'],\n",
              " ['we',\n",
              "  'don',\n",
              "  'need',\n",
              "  'wg',\n",
              "  'agreement',\n",
              "  'to',\n",
              "  'add',\n",
              "  'something',\n",
              "  'to',\n",
              "  'the',\n",
              "  'tracker'],\n",
              " ['that', 'not', 'what', 'saying'],\n",
              " ['in',\n",
              "  'particular',\n",
              "  'if',\n",
              "  'the',\n",
              "  'author',\n",
              "  'has',\n",
              "  'an',\n",
              "  'outstanding',\n",
              "  'action',\n",
              "  'action',\n",
              "  'to',\n",
              "  'reply',\n",
              "  'due',\n",
              "  'days',\n",
              "  'ago'],\n",
              " ['ian'],\n",
              " ['yes'],\n",
              " ['since',\n",
              "  'when',\n",
              "  'can',\n",
              "  'non',\n",
              "  'telcon',\n",
              "  'participants',\n",
              "  'be',\n",
              "  'assigned',\n",
              "  'action',\n",
              "  'items'],\n",
              " ['what', 'does', 'this', 'have', 'to', 'do', 'with', 'the', 'telco'],\n",
              " ['http', 'www', 'org', 'html', 'wg', 'tracker', 'actions'],\n",
              " ['title', 'action', 'html', 'weekly', 'tracker', 'at', 'www', 'org'],\n",
              " ['let',\n",
              "  'me',\n",
              "  'state',\n",
              "  'it',\n",
              "  'differently',\n",
              "  'did',\n",
              "  'he',\n",
              "  'agree',\n",
              "  'to',\n",
              "  'taking',\n",
              "  'that',\n",
              "  'action',\n",
              "  'item'],\n",
              " ['it',\n",
              "  'against',\n",
              "  'protocol',\n",
              "  'to',\n",
              "  'assign',\n",
              "  'action',\n",
              "  'items',\n",
              "  'to',\n",
              "  'people',\n",
              "  'who',\n",
              "  'did',\n",
              "  'not',\n",
              "  'agree',\n",
              "  'to',\n",
              "  'taking',\n",
              "  'them'],\n",
              " ['don', 'know', 'nor', 'do', 'think', 'he', 'needs', 'to'],\n",
              " ['based',\n",
              "  'on',\n",
              "  'experience',\n",
              "  'with',\n",
              "  'the',\n",
              "  'say',\n",
              "  'he',\n",
              "  'has',\n",
              "  'to',\n",
              "  'agree'],\n",
              " ['but',\n",
              "  'it',\n",
              "  'is',\n",
              "  'ok',\n",
              "  'to',\n",
              "  'add',\n",
              "  'non',\n",
              "  'chartered',\n",
              "  'stuff',\n",
              "  'to',\n",
              "  'spec',\n",
              "  'and',\n",
              "  'then',\n",
              "  'not',\n",
              "  'to',\n",
              "  'reply',\n",
              "  'when',\n",
              "  'asked',\n",
              "  'for',\n",
              "  'th',\n",
              "  'reasons'],\n",
              " ['my',\n",
              "  'agreement',\n",
              "  'to',\n",
              "  'the',\n",
              "  'tracker',\n",
              "  'item',\n",
              "  'assigned',\n",
              "  'to',\n",
              "  'me',\n",
              "  'was',\n",
              "  'that',\n",
              "  'thought',\n",
              "  'it',\n",
              "  'was',\n",
              "  'against',\n",
              "  'protocol',\n",
              "  'to',\n",
              "  'refuse',\n",
              "  'when',\n",
              "  'zakim',\n",
              "  'picked',\n",
              "  'me',\n",
              "  'as',\n",
              "  'quot',\n",
              "  'victim',\n",
              "  'quot'],\n",
              " ['ignoring',\n",
              "  'process',\n",
              "  'for',\n",
              "  'moment',\n",
              "  'it',\n",
              "  'wasn',\n",
              "  'me',\n",
              "  'who',\n",
              "  'added',\n",
              "  'the',\n",
              "  'action',\n",
              "  'but',\n",
              "  'if',\n",
              "  'the',\n",
              "  'wg',\n",
              "  'asks',\n",
              "  'the',\n",
              "  'editor',\n",
              "  'why',\n",
              "  'he',\n",
              "  'did',\n",
              "  'something',\n",
              "  'think',\n",
              "  'it',\n",
              "  'deserves',\n",
              "  'an',\n",
              "  'answer'],\n",
              " ['you',\n",
              "  'raised',\n",
              "  'the',\n",
              "  'issue',\n",
              "  'two',\n",
              "  'weeks',\n",
              "  'ago',\n",
              "  'things',\n",
              "  'take',\n",
              "  'bit',\n",
              "  'of',\n",
              "  'time'],\n",
              " ['if',\n",
              "  'ian',\n",
              "  'has',\n",
              "  'the',\n",
              "  'time',\n",
              "  'to',\n",
              "  'add',\n",
              "  'these',\n",
              "  'four',\n",
              "  'chapters',\n",
              "  'vcard',\n",
              "  'icalendar',\n",
              "  'bibtext',\n",
              "  'atom',\n",
              "  'he',\n",
              "  'probably',\n",
              "  'also',\n",
              "  'has',\n",
              "  'time',\n",
              "  'to',\n",
              "  'explain',\n",
              "  'why'],\n",
              " ['if',\n",
              "  'you',\n",
              "  'catch',\n",
              "  'him',\n",
              "  'on',\n",
              "  'irc',\n",
              "  'you',\n",
              "  'can',\n",
              "  'probably',\n",
              "  'ask',\n",
              "  'if',\n",
              "  'he',\n",
              "  'can',\n",
              "  'prioritize',\n",
              "  'it',\n",
              "  'somewhat',\n",
              "  'if',\n",
              "  'timely',\n",
              "  'response',\n",
              "  'is',\n",
              "  'important',\n",
              "  'to',\n",
              "  'you'],\n",
              " ['think',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'that',\n",
              "  'there',\n",
              "  'an',\n",
              "  'overdue',\n",
              "  'action',\n",
              "  'on',\n",
              "  'him',\n",
              "  'should',\n",
              "  'be',\n",
              "  'sufficient',\n",
              "  'information',\n",
              "  'that',\n",
              "  'the',\n",
              "  'wg',\n",
              "  'wants',\n",
              "  'feedback'],\n",
              " ['seems',\n",
              "  'to',\n",
              "  'me',\n",
              "  'that',\n",
              "  'it',\n",
              "  'followed',\n",
              "  'from',\n",
              "  'the',\n",
              "  'microdata',\n",
              "  'use',\n",
              "  'cases',\n",
              "  'requirements',\n",
              "  'discussion'],\n",
              " ['cfda',\n",
              "  'doubt',\n",
              "  'he',\n",
              "  'even',\n",
              "  'knows',\n",
              "  'an',\n",
              "  'action',\n",
              "  'item',\n",
              "  'is',\n",
              "  'assigned',\n",
              "  'to',\n",
              "  'him'],\n",
              " ['so',\n",
              "  'he',\n",
              "  'both',\n",
              "  'not',\n",
              "  'attending',\n",
              "  'telcos',\n",
              "  'and',\n",
              "  'not',\n",
              "  'reading',\n",
              "  'the',\n",
              "  'minutes'],\n",
              " ['cfda',\n",
              "  'fwiw',\n",
              "  'think',\n",
              "  'the',\n",
              "  'answer',\n",
              "  'to',\n",
              "  'the',\n",
              "  'question',\n",
              "  'is',\n",
              "  'quot',\n",
              "  'because',\n",
              "  'it',\n",
              "  'allows',\n",
              "  'the',\n",
              "  'use',\n",
              "  'cases',\n",
              "  'of',\n",
              "  'rich',\n",
              "  'drag',\n",
              "  'and',\n",
              "  'drop',\n",
              "  'clipboard',\n",
              "  'items',\n",
              "  'quot',\n",
              "  'to',\n",
              "  'be',\n",
              "  'fulfilled'],\n",
              " ['although', 'obviously', 'that', 'is', 'not', 'an', 'offical', 'answer'],\n",
              " ['cfda',\n",
              "  'that',\n",
              "  'seems',\n",
              "  'like',\n",
              "  'quite',\n",
              "  'leap',\n",
              "  'from',\n",
              "  'not',\n",
              "  'knowing',\n",
              "  'he',\n",
              "  'has',\n",
              "  'an',\n",
              "  'action',\n",
              "  'item'],\n",
              " ['in',\n",
              "  'my',\n",
              "  'experience',\n",
              "  'ian',\n",
              "  'always',\n",
              "  'does',\n",
              "  'action',\n",
              "  'items',\n",
              "  'on',\n",
              "  'time'],\n",
              " ['cfda',\n",
              "  'and',\n",
              "  'whilst',\n",
              "  'think',\n",
              "  'it',\n",
              "  'makes',\n",
              "  'sense',\n",
              "  'to',\n",
              "  'consider',\n",
              "  'taking',\n",
              "  'the',\n",
              "  'specific',\n",
              "  'mocrodata',\n",
              "  'bits',\n",
              "  'out',\n",
              "  'of',\n",
              "  'html',\n",
              "  'keeping',\n",
              "  'the',\n",
              "  'amp',\n",
              "  'stuff',\n",
              "  'would',\n",
              "  'make',\n",
              "  'the',\n",
              "  'specs',\n",
              "  'codependant',\n",
              "  'on',\n",
              "  'each',\n",
              "  'other',\n",
              "  'so',\n",
              "  'not',\n",
              "  'sure',\n",
              "  'you',\n",
              "  'gain',\n",
              "  'much'],\n",
              " ['microdata'],\n",
              " ['amongst', 'other', 'errors'],\n",
              " ['gives', 'up', 'on', 'finding', 'rules', 'on', 'action', 'items'],\n",
              " ['back'],\n",
              " ['ok', 'babe'],\n",
              " ['what', 'happend', 'babe'],\n",
              " ['dog',\n",
              "  'wanted',\n",
              "  'out',\n",
              "  'then',\n",
              "  'she',\n",
              "  'took',\n",
              "  'off',\n",
              "  'runnin',\n",
              "  'down',\n",
              "  'the',\n",
              "  'street'],\n",
              " ['oh', 'man', 'hate', 'that', 'what', 'kinda', 'of', 'god'],\n",
              " ['dashund'],\n",
              " ['ic', 'have', 'pitbull', 'hes', 'sweet', 'heart'],\n",
              " ['cool'],\n",
              " ['he', 'thinks', 'his', 'small', 'dog'],\n",
              " ['lol'],\n",
              " ['does', 'he', 'like', 'sit', 'ur', 'lap'],\n",
              " ['if',\n",
              "  'he',\n",
              "  'had',\n",
              "  'his',\n",
              "  'way',\n",
              "  'he',\n",
              "  'would',\n",
              "  'but',\n",
              "  'he',\n",
              "  'will',\n",
              "  'streach',\n",
              "  'out',\n",
              "  'and',\n",
              "  'take',\n",
              "  'over',\n",
              "  'the',\n",
              "  'couch'],\n",
              " ['lol'],\n",
              " ['want', 'to', 'see', 'his', 'baby', 'pics'],\n",
              " ['awwwww'],\n",
              " ['hes', 'sooooo', 'cute'],\n",
              " ['one', 'opened'],\n",
              " ['the', 'other', 'didnt'],\n",
              " ['ok'],\n",
              " ['hes', 'so', 'tiny'],\n",
              " ['yeah', 'but', 'not', 'any', 'more'],\n",
              " ['what', 'ya', 'doin'],\n",
              " ['woundering', 'where', 'where', 'and', 'doing', 'baby'],\n",
              " ['im', 'here'],\n",
              " ['ok'],\n",
              " [],\n",
              " [],\n",
              " ['you', 'know', 'what', 'baby', 'your', 'sssoooo', 'allsome'],\n",
              " ['thanks'],\n",
              " ['yw'],\n",
              " ['did', 'eat', 'supper', 'yet'],\n",
              " ['yeah', 'ordered', 'papjohns', 'pizza', 'and', 'baby'],\n",
              " ['gonna', 'go', 'eat', 'now'],\n",
              " ['will', 'on', 'in', 'later'],\n",
              " ['ok', 'babby', 'what', 'eatting'],\n",
              " ['chicken'],\n",
              " ['mmm'],\n",
              " ['bbl'],\n",
              " [],\n",
              " ['po'],\n",
              " ['ok'],\n",
              " [],\n",
              " ['hello'],\n",
              " ['boys', 'are', 'shit'],\n",
              " ['that', 'they', 'are'],\n",
              " ['hi'],\n",
              " ['sorry'],\n",
              " ['was', 'just', 'angry'],\n",
              " ['what'],\n",
              " [],\n",
              " ['its', 'quite', 'alright', 'get', 'that', 'way', 'myself', 'on', 'occasion'],\n",
              " ['are', 'girl'],\n",
              " [],\n",
              " ['met', 'guy', 'moment', 'ago'],\n",
              " ['yes'],\n",
              " ['and', 'she', 'dumped', 'me'],\n",
              " ['after', 'do', 'cybersex', 'with', 'me'],\n",
              " ['am', 'afraid'],\n",
              " ['wait', 'is', 'it', 'guy', 'or', 'she'],\n",
              " ['he', 'will', 'do', 'something', 'bad', 'to', 'me'],\n",
              " ['mean'],\n",
              " ['after', 'do', 'cybersex', 'with', 'him'],\n",
              " ['you', 'apos', 'll', 'be', 'fine'],\n",
              " ['of', 'course', 'guy'],\n",
              " ['am', 'girl'],\n",
              " ['don', 'apos', 'worry'],\n",
              " ['he', 'was', 'just', 'horny', 'lowlife', 'looking', 'for', 'good', 'time'],\n",
              " ['he', 'apos', 'not', 'going', 'to', 'do', 'anything'],\n",
              " ['in', 'the', 'video'],\n",
              " ['scared', 'if', 'he', 'will', 'record', 'me'],\n",
              " ['while', 'am', 'doing', 'cybersex'],\n",
              " ['and', 'put', 'it', 'in', 'internet'],\n",
              " ['am', 'afraid'],\n",
              " ['my', 'friends', 'and', 'families', 'will', 'know', 'am', 'doing', 'that'],\n",
              " ['don', 'apos', 'think', 'he', 'will'],\n",
              " ['coz', 'this', 'is', 'the', 'first', 'time'],\n",
              " ['do', 'that'],\n",
              " ['why', 'do', 'think', 'so'],\n",
              " ['trust', 'me', 'relax', 'you', 'apos', 'll', 'be', 'fine'],\n",
              " ['because',\n",
              "  'he',\n",
              "  'was',\n",
              "  'just',\n",
              "  'horny',\n",
              "  'and',\n",
              "  'stupid',\n",
              "  'and',\n",
              "  'you',\n",
              "  'did',\n",
              "  'what',\n",
              "  'he',\n",
              "  'wanted'],\n",
              " ['that', 'apos', 'that'],\n",
              " ['it',\n",
              "  'wasn',\n",
              "  'apos',\n",
              "  'smart',\n",
              "  'decision',\n",
              "  'on',\n",
              "  'your',\n",
              "  'part',\n",
              "  'but',\n",
              "  'doubt',\n",
              "  'it',\n",
              "  'will',\n",
              "  'come',\n",
              "  'back',\n",
              "  'to',\n",
              "  'haunt',\n",
              "  'you'],\n",
              " ['what', 'do', 'mean', 'by', 'coming', 'back', 'to', 'haunt', 'me'],\n",
              " ['like', 'make', 'your', 'life', 'bady'],\n",
              " ['bad'],\n",
              " ['yeah'],\n",
              " ['my', 'life', 'is', 'bad'],\n",
              " ['regret'],\n",
              " ['am', 'scared'],\n",
              " ['really', 'scared'],\n",
              " ['don', 'apos', 'its', 'ok'],\n",
              " ['how', 'old', 'are'],\n",
              " ['just', 'don', 'apos', 'do', 'it', 'again'],\n",
              " ['yeah'],\n",
              " ['you', 'apos', 'll', 'be', 'fine'],\n",
              " ['wont', 'do', 'it', 'again'],\n",
              " ['good'],\n",
              " ['how', 'old', 'are', 'you'],\n",
              " ['am'],\n",
              " [],\n",
              " ['thats', 'why', 'am', 'so', 'scared'],\n",
              " ['same'],\n",
              " ['coz', 'am', 'so', 'young'],\n",
              " ['where', 'are', 'you', 'from'],\n",
              " ['where', 'do', 'you', 'come', 'from'],\n",
              " ['where', 'do', 'come', 'from'],\n",
              " ['am', 'from', 'canada'],\n",
              " ['alright'],\n",
              " ['gotta', 'go'],\n",
              " ['bye'],\n",
              " ['thanks'],\n",
              " ['for', 'make', 'me', 'calmer'],\n",
              " [],\n",
              " ['hi'],\n",
              " ['hii'],\n",
              " ['hous', 'it', 'going'],\n",
              " ['from'],\n",
              " ['hello', 'there'],\n",
              " ['how', 'are', 'ya'],\n",
              " ['hey'],\n",
              " ['so', 'where', 'are', 'you', 'from', 'stranger'],\n",
              " ['heeeeeeeey'],\n",
              " ['merhaba', 'trenc', 'karde'],\n",
              " ['umm', 'no', 'comment'],\n",
              " [],\n",
              " ['hi'],\n",
              " ['usa', 'horny'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['yay'],\n",
              " ['now'],\n",
              " [],\n",
              " ['got', 'this', 'witout', 'calculator', 'too'],\n",
              " ['hold', 'on'],\n",
              " [],\n",
              " [],\n",
              " ['nope'],\n",
              " ['gah', 'was', 'close'],\n",
              " [],\n",
              " [],\n",
              " ['roght'],\n",
              " [],\n",
              " ['mod'],\n",
              " ['comp', 'programming'],\n",
              " [],\n",
              " ['little', 'known'],\n",
              " ['but'],\n",
              " ['can', 'believe'],\n",
              " ['your', 'not'],\n",
              " ['pervert'],\n",
              " ['so', 'hi'],\n",
              " ['lol'],\n",
              " ['like', 'math'],\n",
              " [],\n",
              " [],\n",
              " ['soooo'],\n",
              " [],\n",
              " [],\n",
              " ['that'],\n",
              " ['skip'],\n",
              " ['lol'],\n",
              " ['wats', 'next'],\n",
              " ['uhh'],\n",
              " [],\n",
              " ['im', 'kind'],\n",
              " ['of', 'at', 'easy', 'part'],\n",
              " ['soz'],\n",
              " ['theres', 'this', 'girl'],\n",
              " ['who', 'joined', 'club', 'apos', 'in'],\n",
              " ['and', 'she', 'signs', 'up', 'for', 'volunteer', 'work'],\n",
              " ['and', 'begs', 'me', 'to', 'go'],\n",
              " ['there', 'we', 'sit', 'next', 'to', 'eachother'],\n",
              " ['and', 'she', 'like', 'stays', 'with', 'me', 'the', 'whole', 'time'],\n",
              " ['ask',\n",
              "  'if',\n",
              "  'she',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'volunteer',\n",
              "  'agian',\n",
              "  'and',\n",
              "  'she',\n",
              "  'avoids',\n",
              "  'it'],\n",
              " ['but', 'she', 'still', 'talks', 'to', 'me'],\n",
              " ['like', 'more', 'than', 'before', 'the', 'work'],\n",
              " [],\n",
              " ['want',\n",
              "  'to',\n",
              "  'watch',\n",
              "  'me',\n",
              "  'masturbate',\n",
              "  'and',\n",
              "  'cum',\n",
              "  'on',\n",
              "  'webcam',\n",
              "  'male'],\n",
              " ['hi'],\n",
              " ['asl'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['anyone', 'there'],\n",
              " ['hello'],\n",
              " ['hi'],\n",
              " ['whats',\n",
              "  'up',\n",
              "  'on',\n",
              "  'the',\n",
              "  'lookout',\n",
              "  'for',\n",
              "  'girl',\n",
              "  'so',\n",
              "  'am',\n",
              "  'twenty',\n",
              "  'three',\n",
              "  'have',\n",
              "  'sexxy',\n",
              "  'footage',\n",
              "  'are',\n",
              "  'going',\n",
              "  'to',\n",
              "  'go',\n",
              "  'to',\n",
              "  'come',\n",
              "  'visit',\n",
              "  'my',\n",
              "  'facebook',\n",
              "  'taylor',\n",
              "  'rae',\n",
              "  'jaded'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['whats', 'your', 'favourite', 'word'],\n",
              " ['hmmm'],\n",
              " ['don', 'apos', 'have', 'favourite'],\n",
              " ['have', 'most', 'used'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['wuu'],\n",
              " ['asl'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['or'],\n",
              " ['hru'],\n",
              " [],\n",
              " ['de', 'start', 'telcon'],\n",
              " ['is', 'starting', 'teleconference'],\n",
              " ['logging', 'to', 'http', 'www', 'org', 'webapps', 'irc'],\n",
              " ['bbbffbcaf', 'eaf', 'make', 'logs', 'public'],\n",
              " ['have', 'made', 'the', 'request', 'de'],\n",
              " ['dff', 'cbd', 'fbf', 'this', 'will', 'be', 'wapp'],\n",
              " ['do',\n",
              "  'not',\n",
              "  'see',\n",
              "  'conference',\n",
              "  'matching',\n",
              "  'that',\n",
              "  'name',\n",
              "  'scheduled',\n",
              "  'within',\n",
              "  'the',\n",
              "  'next',\n",
              "  'hour',\n",
              "  'de'],\n",
              " ['meeting', 'web', 'applications', 'working', 'group', 'teleconference'],\n",
              " ['date', 'october'],\n",
              " ['dff', 'cbd', 'fbf', 'this', 'will', 'be', 'dom'],\n",
              " ['ok',\n",
              "  'db',\n",
              "  'fb',\n",
              "  'see',\n",
              "  'ia_webapps',\n",
              "  'dom',\n",
              "  'pm',\n",
              "  'scheduled',\n",
              "  'to',\n",
              "  'start',\n",
              "  'minutes',\n",
              "  'ago'],\n",
              " ['zakim', 'call', 'db', 'fb'],\n",
              " ['ok', 'db', 'fb', 'the', 'call', 'is', 'being', 'made'],\n",
              " ['ia_webapps', 'dom', 'pm', 'has', 'now', 'started'],\n",
              " ['shepazu'],\n",
              " ['uh', 'only', 'in', 'my', 'skype', 'account'],\n",
              " ['hope', 'that', 'is', 'enough'],\n",
              " ['it', 'should', 'be'],\n",
              " [],\n",
              " ['dff', 'cbd', 'fbf', 'is', 'me'],\n",
              " ['bb', 'adad', 'dfb', 'ec', 'got', 'it'],\n",
              " ['http', 'mozilla', 'pettay', 'fi', 'moztests', 'timestamp', 'html'],\n",
              " ['hi', 'deb'],\n",
              " ['sorry', 'late'],\n",
              " ['microsoft'],\n",
              " ['scribenick', 'deb'],\n",
              " ['dff', 'cbd', 'fbf', 'microsoft', 'is', 'deb'],\n",
              " ['deb', 'got', 'it'],\n",
              " ['topic', 'timestamps'],\n",
              " ['scribe', 'deb'],\n",
              " ['http',\n",
              "  'www',\n",
              "  'org',\n",
              "  'tr',\n",
              "  'rec',\n",
              "  'dom',\n",
              "  'level',\n",
              "  'events',\n",
              "  'ecma',\n",
              "  'script',\n",
              "  'binding',\n",
              "  'html'],\n",
              " ['all',\n",
              "  'browsers',\n",
              "  'but',\n",
              "  'perhaps',\n",
              "  'chrome',\n",
              "  'use',\n",
              "  'number',\n",
              "  'rather',\n",
              "  'than',\n",
              "  'date',\n",
              "  'object'],\n",
              " [],\n",
              " ['timestamp', 'of', 'type', 'domtimestamp', 'readonly'],\n",
              " ['used',\n",
              "  'to',\n",
              "  'specify',\n",
              "  'the',\n",
              "  'time',\n",
              "  'in',\n",
              "  'milliseconds',\n",
              "  'relative',\n",
              "  'to',\n",
              "  'the',\n",
              "  'epoch',\n",
              "  'at',\n",
              "  'which',\n",
              "  'the',\n",
              "  'event',\n",
              "  'was',\n",
              "  'created',\n",
              "  'due',\n",
              "  'to',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'that',\n",
              "  'some',\n",
              "  'systems',\n",
              "  'may',\n",
              "  'not',\n",
              "  'provide',\n",
              "  'this',\n",
              "  'information',\n",
              "  'the',\n",
              "  'value',\n",
              "  'of',\n",
              "  'timestamp',\n",
              "  'may',\n",
              "  'be',\n",
              "  'not',\n",
              "  'available',\n",
              "  'for',\n",
              "  'all',\n",
              "  'events',\n",
              "  'when',\n",
              "  'not',\n",
              "  'available',\n",
              "  'value',\n",
              "  'of',\n",
              "  'will',\n",
              "  'be',\n",
              "  'returned',\n",
              "  'examples',\n",
              "  'of',\n",
              "  'epoch',\n",
              "  'time',\n",
              "  'are',\n",
              "  'the',\n",
              "  'time',\n",
              "  'of',\n",
              "  'the',\n",
              "  'system',\n",
              "  'start',\n",
              "  'or',\n",
              "  'utc',\n",
              "  'st',\n",
              "  'january'],\n",
              " [],\n",
              " [],\n",
              " ['timestamp'],\n",
              " ['this', 'read', 'only', 'property', 'is', 'date', 'object'],\n",
              " [],\n",
              " ['probably',\n",
              "  'relying',\n",
              "  'on',\n",
              "  'it',\n",
              "  'as',\n",
              "  'sequence',\n",
              "  'for',\n",
              "  'primitive',\n",
              "  'ordering'],\n",
              " ['can', 'always', 'new', 'date'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'has',\n",
              "  'been',\n",
              "  'metioned',\n",
              "  'that',\n",
              "  'it',\n",
              "  'complicated',\n",
              "  'for',\n",
              "  'implementors',\n",
              "  'to',\n",
              "  'return',\n",
              "  'date'],\n",
              " ['new', 'date', 'event', 'timestamp'],\n",
              " ['bb',\n",
              "  'adad',\n",
              "  'dfb',\n",
              "  'ec',\n",
              "  'can',\n",
              "  'always',\n",
              "  'convert',\n",
              "  'timestamp',\n",
              "  'to',\n",
              "  'date'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'since',\n",
              "  'ie',\n",
              "  'is',\n",
              "  'not',\n",
              "  'supporting',\n",
              "  'this',\n",
              "  'there',\n",
              "  'likely',\n",
              "  'little',\n",
              "  'compat',\n",
              "  'impact',\n",
              "  'to',\n",
              "  'tightening',\n",
              "  'up',\n",
              "  'the',\n",
              "  'spec'],\n",
              " ['could', 'strip', 'out', 'the', 'clause', 'that', 'allows', 'zero'],\n",
              " ['action',\n",
              "  'db',\n",
              "  'fb',\n",
              "  'to',\n",
              "  'add',\n",
              "  'quot',\n",
              "  'should',\n",
              "  'quot',\n",
              "  'clause',\n",
              "  'to',\n",
              "  'make',\n",
              "  'the',\n",
              "  'timestamp',\n",
              "  'an',\n",
              "  'epoch',\n",
              "  'string',\n",
              "  'of',\n",
              "  'type',\n",
              "  'number',\n",
              "  'in',\n",
              "  'the',\n",
              "  'ecmascript',\n",
              "  'binding'],\n",
              " ['noticed', 'an', 'action', 'trying', 'to', 'create', 'it'],\n",
              " ['records', 'action'],\n",
              " ['created',\n",
              "  'action',\n",
              "  'add',\n",
              "  'quot',\n",
              "  'should',\n",
              "  'quot',\n",
              "  'clause',\n",
              "  'to',\n",
              "  'make',\n",
              "  'the',\n",
              "  'timestamp',\n",
              "  'an',\n",
              "  'epoch',\n",
              "  'string',\n",
              "  'of',\n",
              "  'type',\n",
              "  'number',\n",
              "  'in',\n",
              "  'the',\n",
              "  'ecmascript',\n",
              "  'binding',\n",
              "  'on',\n",
              "  'doug',\n",
              "  'schepers',\n",
              "  'due'],\n",
              " ['topic', 'defining', 'events', 'in', 'dom', 'events'],\n",
              " ['like', 'the', 'progress', 'events', 'spec', 'by', 'chaals'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'sync',\n",
              "  'and',\n",
              "  'async',\n",
              "  'svg',\n",
              "  'is',\n",
              "  'just',\n",
              "  'adding',\n",
              "  'this',\n",
              "  'concept'],\n",
              " ['have',\n",
              "  'to',\n",
              "  'do',\n",
              "  'this',\n",
              "  'anyway',\n",
              "  'may',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'provide',\n",
              "  'this',\n",
              "  'detail',\n",
              "  'before',\n",
              "  'november'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'regarding',\n",
              "  'the',\n",
              "  'algorithms',\n",
              "  'of',\n",
              "  'when',\n",
              "  'to',\n",
              "  'fire',\n",
              "  'that',\n",
              "  'seems',\n",
              "  'very',\n",
              "  'host',\n",
              "  'language',\n",
              "  'specific'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'sync',\n",
              "  'async',\n",
              "  'seems',\n",
              "  'like',\n",
              "  'good',\n",
              "  'idea',\n",
              "  'to',\n",
              "  'add',\n",
              "  'to',\n",
              "  'the',\n",
              "  'spec'],\n",
              " ['action',\n",
              "  'deb',\n",
              "  'to',\n",
              "  'provide',\n",
              "  'async',\n",
              "  'sync',\n",
              "  'data',\n",
              "  'based',\n",
              "  'on',\n",
              "  'browser',\n",
              "  'testing'],\n",
              " ['noticed', 'an', 'action', 'trying', 'to', 'create', 'it'],\n",
              " ['records', 'action'],\n",
              " ['created',\n",
              "  'action',\n",
              "  'provide',\n",
              "  'async',\n",
              "  'sync',\n",
              "  'data',\n",
              "  'based',\n",
              "  'on',\n",
              "  'browser',\n",
              "  'testing',\n",
              "  'on',\n",
              "  'deb',\n",
              "  'leithead',\n",
              "  'due'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'event',\n",
              "  'flow',\n",
              "  'diagrams',\n",
              "  'to',\n",
              "  'be',\n",
              "  'added',\n",
              "  'to',\n",
              "  'spec',\n",
              "  'soon'],\n",
              " ['some', 'combinations', 'will', 'be', 'tricky'],\n",
              " ['topic', 'webidl', 'for', 'dom', 'events'],\n",
              " ['deb',\n",
              "  'will',\n",
              "  'dom',\n",
              "  'events',\n",
              "  'support',\n",
              "  'webidl',\n",
              "  'syntax',\n",
              "  'on',\n",
              "  'its',\n",
              "  'idl',\n",
              "  'blocks'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'sam',\n",
              "  'wenieg',\n",
              "  'will',\n",
              "  'be',\n",
              "  'co',\n",
              "  'editing',\n",
              "  'webidl',\n",
              "  'as',\n",
              "  'cameron',\n",
              "  'finishes',\n",
              "  'his',\n",
              "  'disseration'],\n",
              " ['er', 'dissertation'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'common',\n",
              "  'myth',\n",
              "  'that',\n",
              "  'you',\n",
              "  'can',\n",
              "  'go',\n",
              "  'to',\n",
              "  'rec',\n",
              "  'with',\n",
              "  'normative',\n",
              "  'reference',\n",
              "  'to',\n",
              "  'non',\n",
              "  'rec',\n",
              "  'spec',\n",
              "  'actually',\n",
              "  'not',\n",
              "  'true',\n",
              "  'but',\n",
              "  'requires',\n",
              "  'greater',\n",
              "  'scrutiny'],\n",
              " ['we',\n",
              "  'could',\n",
              "  'quot',\n",
              "  'safely',\n",
              "  'quot',\n",
              "  'use',\n",
              "  'webidl',\n",
              "  'in',\n",
              "  'our',\n",
              "  'spec'],\n",
              " ['topic', 'last', 'call', 'timeline'],\n",
              " ['db',\n",
              "  'fb',\n",
              "  'what',\n",
              "  'do',\n",
              "  'people',\n",
              "  'think',\n",
              "  'about',\n",
              "  'targeting',\n",
              "  'last',\n",
              "  'call',\n",
              "  'for',\n",
              "  'november'],\n",
              " ['have',\n",
              "  'solicited',\n",
              "  'early',\n",
              "  'comments',\n",
              "  'from',\n",
              "  'other',\n",
              "  'working',\n",
              "  'groups',\n",
              "  'to',\n",
              "  'try',\n",
              "  'to',\n",
              "  'mitigate',\n",
              "  'issues',\n",
              "  'at',\n",
              "  'last',\n",
              "  'call'],\n",
              " ['topic', 'new', 'sections', 'to', 'the', 'spec'],\n",
              " ['db', 'fb', 'added', 'section', 'on', 'conformance', 'criteria'],\n",
              " ['also',\n",
              "  'section',\n",
              "  'on',\n",
              "  'feature',\n",
              "  'detection',\n",
              "  'based',\n",
              "  'on',\n",
              "  'hasfeature'],\n",
              " ['can',\n",
              "  'you',\n",
              "  'folks',\n",
              "  'look',\n",
              "  'this',\n",
              "  'over',\n",
              "  'and',\n",
              "  'give',\n",
              "  'me',\n",
              "  'feedback'],\n",
              " ['cameron',\n",
              "  'and',\n",
              "  'other',\n",
              "  'have',\n",
              "  'already',\n",
              "  'provided',\n",
              "  'feedback',\n",
              "  'seems',\n",
              "  'ok',\n",
              "  'so',\n",
              "  'far'],\n",
              " ['bb', 'adad', 'dfb', 'ec'],\n",
              " ['deb'],\n",
              " ['shepazu'],\n",
              " ['ia_webapps', 'dom', 'pm', 'has', 'ended'],\n",
              " ['attendees', 'were', 'shepazu', 'bb', 'adad', 'dfb', 'ec', 'deb'],\n",
              " ['wonders',\n",
              "  'if',\n",
              "  'shepazu',\n",
              "  'or',\n",
              "  'someone',\n",
              "  'is',\n",
              "  'going',\n",
              "  'to',\n",
              "  'bbbffbcaf',\n",
              "  'eaf',\n",
              "  'make',\n",
              "  'minutes'],\n",
              " ['de', 'make', 'minutes'],\n",
              " ['sorry',\n",
              "  'db',\n",
              "  'fb',\n",
              "  'don',\n",
              "  'understand',\n",
              "  'de',\n",
              "  'make',\n",
              "  'minutes',\n",
              "  'please',\n",
              "  'refer',\n",
              "  'to',\n",
              "  'http',\n",
              "  'www',\n",
              "  'org',\n",
              "  'tracker',\n",
              "  'irc',\n",
              "  'for',\n",
              "  'help'],\n",
              " ['de', 'end', 'telcon'],\n",
              " ['is', 'ending', 'teleconference'],\n",
              " ['dff', 'cbd', 'fbf', 'list', 'attendees'],\n",
              " ['sorry', 'de', 'don', 'know', 'what', 'conference', 'this', 'is'],\n",
              " ['bbbffbcaf', 'eaf', 'please', 'draft', 'minutes'],\n",
              " ['have',\n",
              "  'made',\n",
              "  'the',\n",
              "  'request',\n",
              "  'to',\n",
              "  'generate',\n",
              "  'http',\n",
              "  'www',\n",
              "  'org',\n",
              "  'webapps',\n",
              "  'minutes',\n",
              "  'html',\n",
              "  'de'],\n",
              " ['bbbffbcaf', 'eaf', 'bye'],\n",
              " ['see',\n",
              "  'open',\n",
              "  'action',\n",
              "  'items',\n",
              "  'saved',\n",
              "  'in',\n",
              "  'http',\n",
              "  'www',\n",
              "  'org',\n",
              "  'webapps',\n",
              "  'actions',\n",
              "  'rdf'],\n",
              " ['action',\n",
              "  'db',\n",
              "  'fb',\n",
              "  'to',\n",
              "  'add',\n",
              "  'quot',\n",
              "  'should',\n",
              "  'quot',\n",
              "  'clause',\n",
              "  'to',\n",
              "  'make',\n",
              "  'the',\n",
              "  'timestamp',\n",
              "  'an',\n",
              "  'epoch',\n",
              "  'string',\n",
              "  'of',\n",
              "  'type',\n",
              "  'number',\n",
              "  'in',\n",
              "  'the',\n",
              "  'ecmascript',\n",
              "  'binding'],\n",
              " ['recorded', 'in', 'http', 'www', 'org', 'webapps', 'irc'],\n",
              " ['action',\n",
              "  'deb',\n",
              "  'to',\n",
              "  'provide',\n",
              "  'async',\n",
              "  'sync',\n",
              "  'data',\n",
              "  'based',\n",
              "  'on',\n",
              "  'browser',\n",
              "  'testing'],\n",
              " ['recorded', 'in', 'http', 'www', 'org', 'webapps', 'irc'],\n",
              " ['heyyyyy'],\n",
              " ['hey'],\n",
              " ['how', 'are'],\n",
              " [],\n",
              " ['im', 'good', 'thankyou', 'yourself'],\n",
              " ['the',\n",
              "  'bart',\n",
              "  'logo',\n",
              "  'svg',\n",
              "  'image',\n",
              "  'would',\n",
              "  'render',\n",
              "  'correctly',\n",
              "  'under',\n",
              "  'hsivonen',\n",
              "  'proposal',\n",
              "  'when',\n",
              "  'parsed',\n",
              "  'as',\n",
              "  'text',\n",
              "  'html',\n",
              "  'although',\n",
              "  'the',\n",
              "  'rdf',\n",
              "  'and',\n",
              "  'inkscape',\n",
              "  'cruft',\n",
              "  'would',\n",
              "  'be',\n",
              "  'in',\n",
              "  'the',\n",
              "  'quot',\n",
              "  'wrong',\n",
              "  'quot',\n",
              "  'namespace'],\n",
              " ['annevk', 'yes', 'they', 'did', 'or', 'so', 'was', 'told'],\n",
              " ['napa',\n",
              "  'auto',\n",
              "  'parts',\n",
              "  'franchise',\n",
              "  'http',\n",
              "  'microformats',\n",
              "  'org',\n",
              "  'wiki',\n",
              "  'index',\n",
              "  'php',\n",
              "  'title',\n",
              "  'amp',\n",
              "  'rcid',\n",
              "  'rinixtursmo',\n",
              "  'new',\n",
              "  'page',\n",
              "  'http',\n",
              "  'www',\n",
              "  'lat',\n",
              "  'com',\n",
              "  'xenon',\n",
              "  'xenon',\n",
              "  'kits',\n",
              "  'http',\n",
              "  'wikicars',\n",
              "  'org',\n",
              "  'en',\n",
              "  'bi',\n",
              "  'hid',\n",
              "  'kits',\n",
              "  'http',\n",
              "  'carscritic',\n",
              "  'info',\n",
              "  'xenon',\n",
              "  'headlights',\n",
              "  'http',\n",
              "  'automobilesblog',\n",
              "  'info',\n",
              "  'hid',\n",
              "  'head',\n",
              "  'lights'],\n",
              " ['hi'],\n",
              " ['ice', 'ice', 'baby'],\n",
              " ['sorry', 'am', 'boy'],\n",
              " ['hi'],\n",
              " [],\n",
              " ['asl'],\n",
              " ['hi'],\n",
              " ['korea'],\n",
              " ['star'],\n",
              " ['am', 'star', 'thank', 'you'],\n",
              " ['kthxbai'],\n",
              " ['href',\n",
              "  'http',\n",
              "  'www',\n",
              "  'microsoft',\n",
              "  'com',\n",
              "  'windowsserver',\n",
              "  'http',\n",
              "  'www',\n",
              "  'microsoft',\n",
              "  'com',\n",
              "  'windowsserver',\n",
              "  'lt',\n",
              "  'users',\n",
              "  'can',\n",
              "  'be',\n",
              "  'wrong',\n",
              "  'quot'],\n",
              " ['bugmail',\n",
              "  'bug',\n",
              "  'new',\n",
              "  'documents',\n",
              "  'and',\n",
              "  'settings',\n",
              "  'bb',\n",
              "  'dokumentumok',\n",
              "  'vide',\n",
              "  'amator',\n",
              "  'tini',\n",
              "  'punci',\n",
              "  'jpg',\n",
              "  'form',\n",
              "  'tum',\n",
              "  'jpg',\n",
              "  'joint',\n",
              "  'photographic',\n",
              "  'experts',\n",
              "  'group',\n",
              "  'ret',\n",
              "  'kb',\n",
              "  'tum',\n",
              "  'rcius',\n",
              "  'sz',\n",
              "  'less',\n",
              "  'pixels',\n",
              "  'magass',\n",
              "  'pixels',\n",
              "  'sz',\n",
              "  'nm',\n",
              "  'lys',\n",
              "  'lt',\n",
              "  'http',\n",
              "  'lists',\n",
              "  'org',\n",
              "  'archives',\n",
              "  'public',\n",
              "  'public',\n",
              "  'html',\n",
              "  'bugzilla',\n",
              "  'mar',\n",
              "  'html',\n",
              "  'gt'],\n",
              " [],\n",
              " ['hi'],\n",
              " ['there'],\n",
              " [],\n",
              " [],\n",
              " ['hi'],\n",
              " [],\n",
              " [],\n",
              " ['nice'],\n",
              " ['ireland'],\n",
              " [],\n",
              " [],\n",
              " ['age'],\n",
              " ['from'],\n",
              " ['usa'],\n",
              " [],\n",
              " [],\n",
              " ['asl'],\n",
              " ['male', 'ireland'],\n",
              " ['on', 'msn'],\n",
              " ['so', 'what'],\n",
              " ['email'],\n",
              " ['add', 'me', 'plzz'],\n",
              " ['just', 'want', 'to', 'find', 'stimulating', 'chat'],\n",
              " ['have', 'bf'],\n",
              " ['sorry'],\n",
              " ['sorry', 'back', 'now'],\n",
              " ['and',\n",
              "  'for',\n",
              "  'the',\n",
              "  'record',\n",
              "  'want',\n",
              "  'to',\n",
              "  'be',\n",
              "  'you',\n",
              "  'when',\n",
              "  'grow',\n",
              "  'up',\n",
              "  'elika'],\n",
              " ['my', 'goodness', 'you', 're', 'powerhouse', 'woman'],\n",
              " ['so',\n",
              "  'the',\n",
              "  'actual',\n",
              "  'normal',\n",
              "  'flow',\n",
              "  'changes',\n",
              "  'within',\n",
              "  'the',\n",
              "  'block'],\n",
              " ['yeah'],\n",
              " ['it', 'changes', 'direction'],\n",
              " ['block', 'flow'],\n",
              " ['sure'],\n",
              " ['will', 'bring', 'that', 'up', 'tomorrow'],\n",
              " ['well', 'think', 'it', 'the', 'most', 'clear'],\n",
              " ['cool'],\n",
              " ['so'],\n",
              " ['if',\n",
              "  'the',\n",
              "  'whole',\n",
              "  'point',\n",
              "  'is',\n",
              "  'to',\n",
              "  'change',\n",
              "  'the',\n",
              "  'flow',\n",
              "  'of',\n",
              "  'the',\n",
              "  'block'],\n",
              " ['block', 'flow', 'tb', 'lr', 'rl'],\n",
              " [],\n",
              " ['think', 'that', 'makes', 'sense'],\n",
              " ['ok'],\n",
              " ['what', 'do', 'you', 'think'],\n",
              " ['makes', 'sense', 'to', 'me'],\n",
              " ['hehe'],\n",
              " ['isn', 'it', 'the', 'middle', 'of', 'the', 'night', 'there'],\n",
              " ['go', 'to', 'bed'],\n",
              " ['yep'],\n",
              " ['hehehe'],\n",
              " ['updates',\n",
              "  'few',\n",
              "  'more',\n",
              "  'issues',\n",
              "  'and',\n",
              "  'then',\n",
              "  'obeys',\n",
              "  'ed',\n",
              "  'ed',\n",
              "  'fb'],\n",
              " ['wish', 'could', 'be', 'there', 'with'],\n",
              " ['wish', 'you', 'could', 'too'],\n",
              " ['we', 'have', 'no', 'web', 'designers', 'present', 'this', 'meeting'],\n",
              " ['alas', 'not', 'this', 'trip', 'just', 'couldn', 'physically', 'do', 'it'],\n",
              " ['sure',\n",
              "  'you',\n",
              "  'be',\n",
              "  'bored',\n",
              "  'out',\n",
              "  'of',\n",
              "  'your',\n",
              "  'mind',\n",
              "  'half',\n",
              "  'the',\n",
              "  'time'],\n",
              " ['well',\n",
              "  'll',\n",
              "  'stick',\n",
              "  'around',\n",
              "  'online',\n",
              "  'as',\n",
              "  'close',\n",
              "  'to',\n",
              "  'things',\n",
              "  'as',\n",
              "  'possible'],\n",
              " ['but',\n",
              "  'it',\n",
              "  'really',\n",
              "  'helpful',\n",
              "  'to',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'ask',\n",
              "  'quot',\n",
              "  'what',\n",
              "  'makes',\n",
              "  'sense',\n",
              "  'here',\n",
              "  'from',\n",
              "  'your',\n",
              "  'perspective',\n",
              "  'quot'],\n",
              " ['yay'],\n",
              " ['absolutely',\n",
              "  'and',\n",
              "  'it',\n",
              "  'always',\n",
              "  'makes',\n",
              "  'me',\n",
              "  'feel',\n",
              "  'good',\n",
              "  'to',\n",
              "  'contribute',\n",
              "  'something',\n",
              "  'of',\n",
              "  'value'],\n",
              " [],\n",
              " ['am',\n",
              "  'planning',\n",
              "  'to',\n",
              "  'be',\n",
              "  'at',\n",
              "  'tpac',\n",
              "  'so',\n",
              "  'that',\n",
              "  'll',\n",
              "  'be',\n",
              "  'good'],\n",
              " ['awesome'],\n",
              " ['another', 'question'],\n",
              " ['relates', 'to', 'font', 'weights'],\n",
              " ['if', 'you', 'have', 'three', 'nested', 'spans'],\n",
              " ['okay',\n",
              "  'was',\n",
              "  'around',\n",
              "  'for',\n",
              "  'part',\n",
              "  'of',\n",
              "  'that',\n",
              "  'discussion',\n",
              "  'earlier'],\n",
              " ['with', 'the', 'font', 'weights', 'bolder', 'bolder', 'and', 'lighter'],\n",
              " ['but', 'you', 'only', 'have', 'two', 'weights', 'normal', 'and', 'bold'],\n",
              " ['is', 'the', 'innermost', 'span', 'bold', 'or', 'normal'],\n",
              " ['what', 'the', 'rule', 'look', 'like'],\n",
              " ['related',\n",
              "  'question',\n",
              "  'if',\n",
              "  'given',\n",
              "  'two',\n",
              "  'nested',\n",
              "  'spans',\n",
              "  'bolder',\n",
              "  'and',\n",
              "  'lighter',\n",
              "  'should',\n",
              "  'the',\n",
              "  'inner',\n",
              "  'span',\n",
              "  'always',\n",
              "  'be',\n",
              "  'the',\n",
              "  'same',\n",
              "  'weight',\n",
              "  'as',\n",
              "  'outside',\n",
              "  'the',\n",
              "  'spans'],\n",
              " ['the', 'rule', 'looks', 'like'],\n",
              " ['lt',\n",
              "  'span',\n",
              "  'style',\n",
              "  'quot',\n",
              "  'font',\n",
              "  'weight',\n",
              "  'bolder',\n",
              "  'quot',\n",
              "  'gt',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'style',\n",
              "  'quot',\n",
              "  'font',\n",
              "  'weight',\n",
              "  'bolder',\n",
              "  'quot',\n",
              "  'gt',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'style',\n",
              "  'quot',\n",
              "  'font',\n",
              "  'weight',\n",
              "  'lighter',\n",
              "  'quot',\n",
              "  'gt',\n",
              "  'am',\n",
              "  'normal',\n",
              "  'or',\n",
              "  'bold',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'gt',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'gt',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'gt'],\n",
              " ['well',\n",
              "  'might',\n",
              "  'argue',\n",
              "  'that',\n",
              "  'the',\n",
              "  'only',\n",
              "  'span',\n",
              "  'of',\n",
              "  'relevance',\n",
              "  'is',\n",
              "  'that',\n",
              "  'directly',\n",
              "  'spanning',\n",
              "  'the',\n",
              "  'content',\n",
              "  'and',\n",
              "  'therefore',\n",
              "  'in',\n",
              "  'this',\n",
              "  'case',\n",
              "  'quot',\n",
              "  'lighter',\n",
              "  'quot'],\n",
              " ['all', 'of', 'the', 'other', 'spans', 'are', 'empty'],\n",
              " ['oh', 'well', 'they', 'could', 'have', 'text', 'in', 'them'],\n",
              " ['or', 'they', 'could', 'be', 'lt', 'div', 'gt'],\n",
              " ['also',\n",
              "  'what',\n",
              "  'the',\n",
              "  'parent',\n",
              "  'it',\n",
              "  'seems',\n",
              "  'that',\n",
              "  'in',\n",
              "  'part',\n",
              "  'this',\n",
              "  'would',\n",
              "  'be',\n",
              "  'inheritance',\n",
              "  'no'],\n",
              " ['the', 'behavior', 'would', 'have', 'to', 'be', 'the', 'same'],\n",
              " ['no', 'there', 'no', 'inheritance'],\n",
              " ['it', 'just', 'quot', 'bolder', 'quot', 'is', 'relative', 'term'],\n",
              " ['if', 'you', 'have', 'font', 'with', 'three', 'weights'],\n",
              " ['then',\n",
              "  'the',\n",
              "  'innermost',\n",
              "  'span',\n",
              "  'would',\n",
              "  'be',\n",
              "  'the',\n",
              "  'same',\n",
              "  'weight',\n",
              "  'as',\n",
              "  'the',\n",
              "  'outermost',\n",
              "  'span',\n",
              "  'bolder',\n",
              "  'than',\n",
              "  'the',\n",
              "  'text',\n",
              "  'outside',\n",
              "  'the',\n",
              "  'spans'],\n",
              " ['but', 'lighter', 'than', 'the', 'text', 'inside', 'the', 'middle', 'span'],\n",
              " ['normal',\n",
              "  'weight',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'style',\n",
              "  'quot',\n",
              "  'font',\n",
              "  'weight',\n",
              "  'bolder',\n",
              "  'quot',\n",
              "  'gt',\n",
              "  'bold',\n",
              "  'weight',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'style',\n",
              "  'quot',\n",
              "  'font',\n",
              "  'weight',\n",
              "  'bolder',\n",
              "  'quot',\n",
              "  'gt',\n",
              "  'extra',\n",
              "  'bold',\n",
              "  'weight',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'style',\n",
              "  'quot',\n",
              "  'font',\n",
              "  'weight',\n",
              "  'lighter',\n",
              "  'quot',\n",
              "  'gt',\n",
              "  'bold',\n",
              "  'weight',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'gt',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'gt',\n",
              "  'lt',\n",
              "  'span',\n",
              "  'gt'],\n",
              " ['so', 'it', 'relative', 'to', 'the', 'immediate', 'parent', 'span'],\n",
              " ['like', 'that'],\n",
              " ['yep'],\n",
              " ['urgh'],\n",
              " ['the',\n",
              "  'question',\n",
              "  'is',\n",
              "  'what',\n",
              "  'happens',\n",
              "  'if',\n",
              "  'you',\n",
              "  'only',\n",
              "  'have',\n",
              "  'two',\n",
              "  'weights'],\n",
              " ['say', 'it', 'goes', 'to', 'normal'],\n",
              " ['in', 'some', 'ways', 'it', 'feels', 'like', 'as', 'the', 'designer'],\n",
              " ['need',\n",
              "  'quot',\n",
              "  'clue',\n",
              "  'quot',\n",
              "  'to',\n",
              "  'say',\n",
              "  'okay',\n",
              "  'there',\n",
              "  'breakdown',\n",
              "  'in',\n",
              "  'the',\n",
              "  'relation',\n",
              "  'here'],\n",
              " ['if', 'the', 'author', 'is', 'intending', 'to', 'have', 'lighter', 'color'],\n",
              " ['right', 'well'],\n",
              " ['that',\n",
              "  'should',\n",
              "  'be',\n",
              "  'explicit',\n",
              "  'hate',\n",
              "  'hate',\n",
              "  'hate',\n",
              "  'relative',\n",
              "  'values',\n",
              "  'btw'],\n",
              " ['for', 'this', 'very', 'reason'],\n",
              " ['it', 'defined', 'as', 'lighter', 'than', 'the', 'parent'],\n",
              " ['but',\n",
              "  'the',\n",
              "  'designer',\n",
              "  'might',\n",
              "  'be',\n",
              "  'trying',\n",
              "  'to',\n",
              "  'get',\n",
              "  'back',\n",
              "  'to',\n",
              "  'the',\n",
              "  'middle',\n",
              "  'value',\n",
              "  'here'],\n",
              " ['if',\n",
              "  'there',\n",
              "  'were',\n",
              "  'multiple',\n",
              "  'weights',\n",
              "  'like',\n",
              "  'or',\n",
              "  'that',\n",
              "  'what',\n",
              "  'happens'],\n",
              " ['don', 'know'],\n",
              " ['it', 'confusing'],\n",
              " ['don', 'know', 'either', 'definitely', 'question', 'to', 'ask', 'jason'],\n",
              " ['we', 've', 'got', 'both', 'kinds', 'of', 'implementations'],\n",
              " [],\n",
              " ['that', 'of', 'course', 'what', 'figured'],\n",
              " ['think', 'did', 'ask', 'him'],\n",
              " ['and', 'he', 'said', 'it', 'was', 'hard', 'question'],\n",
              " ['any', 'relative', 'value', 'questions', 'are', 'hard'],\n",
              " ['to', 'me', 'anyway'],\n",
              " ['relational',\n",
              "  'math',\n",
              "  'designers',\n",
              "  'don',\n",
              "  'think',\n",
              "  'that',\n",
              "  'way',\n",
              "  'they',\n",
              "  'just',\n",
              "  'don',\n",
              "  'for',\n",
              "  'the',\n",
              "  'vast',\n",
              "  'majority',\n",
              "  'anyway'],\n",
              " ['heh'],\n",
              " ['designers', 'say', 'quot', 'want', 'it', 'this', 'dark', 'quot'],\n",
              " ['and', 'give', 'it', 'specific', 'value'],\n",
              " ['seriously'],\n",
              " ['we', 'have', 'keywords', 'for', 'that'],\n",
              " ['seriously'],\n",
              " [],\n",
              " ['hey', 'alexmog'],\n",
              " ['is', 'pretty', 'tired'],\n",
              " ['seriously'],\n",
              " ['should', 'go', 'to', 'bed', 'now', 'think'],\n",
              " ['know',\n",
              "  'which',\n",
              "  'is',\n",
              "  'why',\n",
              "  'the',\n",
              "  'entire',\n",
              "  'bold',\n",
              "  'bolder',\n",
              "  'situation',\n",
              "  'is',\n",
              "  'strange',\n",
              "  'one',\n",
              "  'most',\n",
              "  'designers',\n",
              "  'are',\n",
              "  'not',\n",
              "  'gonna',\n",
              "  'use',\n",
              "  'relative',\n",
              "  'weights'],\n",
              " ['seriously'],\n",
              " ['lol'],\n",
              " [],\n",
              " ['go', 'to', 'bed', 'elika', 'sweetest', 'of', 'dreams'],\n",
              " ['hi', 'alex'],\n",
              " ['night', 'ed', 'ed', 'fb'],\n",
              " ['night', 'ffd', 'fe', 'acc', 'bb'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['asl'],\n",
              " ['go', 'talk', 'faggot'],\n",
              " ['hii'],\n",
              " ['knock', 'knock'],\n",
              " ['asl'],\n",
              " ['knock', 'knock'],\n",
              " ['hi'],\n",
              " ['wherea', 're', 'you', 'from'],\n",
              " ['hy'],\n",
              " ['hiii'],\n",
              " ['asl'],\n",
              " ['asl'],\n",
              " [],\n",
              " ['cool', 'cool'],\n",
              " ['arrummzen', 'you', 'should', 'try', 'soldering', 'it', 'with', 'something'],\n",
              " ['ops', 'it', 'is', 'in', 'laptop'],\n",
              " ['maybe',\n",
              "  'you',\n",
              "  'should',\n",
              "  'try',\n",
              "  'anyway',\n",
              "  'with',\n",
              "  'screw',\n",
              "  'that',\n",
              "  'has',\n",
              "  'the',\n",
              "  'same',\n",
              "  'dimension',\n",
              "  'like'],\n",
              " ['action',\n",
              "  'is',\n",
              "  'calling',\n",
              "  'microsoft',\n",
              "  'tomarrow',\n",
              "  'and',\n",
              "  'giving',\n",
              "  'them',\n",
              "  'plenty',\n",
              "  'of',\n",
              "  'info',\n",
              "  'about',\n",
              "  'all',\n",
              "  'the',\n",
              "  'bullshit',\n",
              "  'in',\n",
              "  'linux',\n",
              "  'so',\n",
              "  'they',\n",
              "  'can',\n",
              "  'make',\n",
              "  'wonderful',\n",
              "  'advetrtisements',\n",
              "  'about',\n",
              "  'it'],\n",
              " ['flyback', 'wasn', 'you', 'sleeping'],\n",
              " ['flyback', 'you', 'should', 'have', 'beer', 'or', 'two'],\n",
              " ['na',\n",
              "  'dude',\n",
              "  'it',\n",
              "  'the',\n",
              "  'same',\n",
              "  'bullshit',\n",
              "  'it',\n",
              "  'will',\n",
              "  'never',\n",
              "  'change'],\n",
              " ['ok'],\n",
              " ['just', 'so', 'turn', 'off', 'the', 'plug'],\n",
              " ['and', 'bye', 'bye'],\n",
              " ['hi',\n",
              "  'all',\n",
              "  'need',\n",
              "  'some',\n",
              "  'advice',\n",
              "  'have',\n",
              "  'rosewill',\n",
              "  'usb',\n",
              "  'jumbo',\n",
              "  'drive',\n",
              "  'that',\n",
              "  'has',\n",
              "  'data',\n",
              "  'on',\n",
              "  'it',\n",
              "  'need',\n",
              "  'to',\n",
              "  'recover',\n",
              "  'problem',\n",
              "  'is',\n",
              "  'the',\n",
              "  'drive',\n",
              "  'is',\n",
              "  'recognized',\n",
              "  'by',\n",
              "  'the',\n",
              "  'computer',\n",
              "  'but',\n",
              "  'displays',\n",
              "  'the',\n",
              "  'nothing',\n",
              "  'on',\n",
              "  'the',\n",
              "  'drive',\n",
              "  'and',\n",
              "  'says',\n",
              "  'there',\n",
              "  'are',\n",
              "  'bytes',\n",
              "  'available',\n",
              "  'occassinally',\n",
              "  'it',\n",
              "  'does',\n",
              "  'work',\n",
              "  'but',\n",
              "  'there',\n",
              "  'is',\n",
              "  'not',\n",
              "  'enough',\n",
              "  'time',\n",
              "  'to',\n",
              "  'get',\n",
              "  'the',\n",
              "  'data',\n",
              "  'off',\n",
              "  'before',\n",
              "  'it',\n",
              "  'craps',\n",
              "  'out',\n",
              "  'anyone',\n",
              "  'have',\n",
              "  'any',\n",
              "  'suggestions'],\n",
              " ['it',\n",
              "  'the',\n",
              "  'disk',\n",
              "  'which',\n",
              "  'is',\n",
              "  'dying',\n",
              "  'dead',\n",
              "  'not',\n",
              "  'the',\n",
              "  'enclosure',\n",
              "  'assume'],\n",
              " ['it', 'works', 'briefly'],\n",
              " ['it', 'does', 'very', 'briefly', 'sometimes'],\n",
              " ['actually',\n",
              "  'kinda',\n",
              "  'thought',\n",
              "  'it',\n",
              "  'was',\n",
              "  'the',\n",
              "  'other',\n",
              "  'way',\n",
              "  'around',\n",
              "  'the',\n",
              "  'enclosure',\n",
              "  'usb',\n",
              "  'interface',\n",
              "  'etc',\n",
              "  'but',\n",
              "  'maybe',\n",
              "  'not',\n",
              "  'the',\n",
              "  'drive',\n",
              "  'itself'],\n",
              " ['if', 'it', 'the', 'drive', 'the', 'freezer', 'trick', 'might', 'work'],\n",
              " ['when',\n",
              "  'can',\n",
              "  'access',\n",
              "  'it',\n",
              "  'the',\n",
              "  'data',\n",
              "  'appears',\n",
              "  'to',\n",
              "  'be',\n",
              "  'all',\n",
              "  'there'],\n",
              " ['freezer', 'trick'],\n",
              " ['satch', 'put', 'the', 'drive', 'in', 'the', 'freezer', 'for', 'bit'],\n",
              " ['how', 'long'],\n",
              " ['in', 'an', 'antistatic', 'bag', 'in', 'ziplock', 'bag'],\n",
              " ['start', 'googling', 'it', 'widely', 'documented'],\n",
              " ['long', 'enough', 'for', 'it', 'to', 'chill', 'right', 'down'],\n",
              " ['ok'],\n",
              " ['and', 'don', 'expect', 'it', 'to', 'keep', 'working'],\n",
              " ['now', 'if', 'its', 'the', 'enclosure', 'and', 'not', 'the', 'disk'],\n",
              " ['now',\n",
              "  'you',\n",
              "  'do',\n",
              "  'know',\n",
              "  'we',\n",
              "  'ew',\n",
              "  'talking',\n",
              "  'about',\n",
              "  'flash',\n",
              "  'drive',\n",
              "  'right'],\n",
              " ['what'],\n",
              " ['hmmm', 'shoulda', 'said', 'tha', 'earlier', 'guess'],\n",
              " ['its', 'usb', 'flash', 'drive'],\n",
              " ['nit', 'ysb', 'hard', 'drive'],\n",
              " ['my', 'bad', 'sorry'],\n",
              " ['ok', 'don', 'freeze', 'it'],\n",
              " [],\n",
              " ['ok'],\n",
              " ['then',\n",
              "  'dunno',\n",
              "  'flash',\n",
              "  'memory',\n",
              "  'shouldn',\n",
              "  'fail',\n",
              "  'unless',\n",
              "  'it',\n",
              "  'ancient'],\n",
              " ['well',\n",
              "  'it',\n",
              "  'seems',\n",
              "  'to',\n",
              "  'be',\n",
              "  'the',\n",
              "  'interface',\n",
              "  'or',\n",
              "  'what',\n",
              "  'not',\n",
              "  'not',\n",
              "  'the',\n",
              "  'memory',\n",
              "  'itself'],\n",
              " ['don',\n",
              "  'know',\n",
              "  'how',\n",
              "  'to',\n",
              "  'debug',\n",
              "  'that',\n",
              "  'you',\n",
              "  'have',\n",
              "  'to',\n",
              "  'get',\n",
              "  'into',\n",
              "  'the',\n",
              "  'internals'],\n",
              " ['guess', 'the', 'verbose', 'usb', 'logging', 'could', 'help'],\n",
              " ['linux', 'doesnt', 'pick', 'up', 'the', 'drive', 'at', 'all'],\n",
              " ['windows',\n",
              "  'recognizes',\n",
              "  'the',\n",
              "  'drive',\n",
              "  'but',\n",
              "  'says',\n",
              "  'its',\n",
              "  'got',\n",
              "  'files',\n",
              "  'available'],\n",
              " ['and', 'displays', 'no', 'files', 'like', 'its', 'empty'],\n",
              " ['bytes', 'meant'],\n",
              " ['did', 'you', 'fsck'],\n",
              " ['no',\n",
              "  'dont',\n",
              "  'even',\n",
              "  'know',\n",
              "  'what',\n",
              "  'device',\n",
              "  'file',\n",
              "  'it',\n",
              "  'is',\n",
              "  'on',\n",
              "  'linux'],\n",
              " ['might',\n",
              "  'it',\n",
              "  'be',\n",
              "  'that',\n",
              "  'the',\n",
              "  'filesystem',\n",
              "  'is',\n",
              "  'in',\n",
              "  'trouble',\n",
              "  'and',\n",
              "  'not',\n",
              "  'the',\n",
              "  'hardware'],\n",
              " ['hotplug', 'will', 'not', 'pick', 'it', 'up'],\n",
              " ['damn', 'had', 'too', 'much', 'caffiene'],\n",
              " ['try', 'dev', 'sda'],\n",
              " ['suppose',\n",
              "  'but',\n",
              "  'found',\n",
              "  'no',\n",
              "  'evidence',\n",
              "  'that',\n",
              "  'linux',\n",
              "  'even',\n",
              "  'knows',\n",
              "  'its',\n",
              "  'plugged',\n",
              "  'ib'],\n",
              " ['or', 'dev', 'sda'],\n",
              " ['sda', 'is', 'my', 'hard', 'drive'],\n",
              " ['ah'],\n",
              " ['then', 'sdb'],\n",
              " ['tried', 'to', 'fdisk', 'sdb', 'sdc', 'and', 'sdd', 'and', 'got', 'nothing'],\n",
              " ['satch', 'might', 'need', 'to', 'modprobe', 'drivers', 'first'],\n",
              " ['sbp'],\n",
              " ['that', 'was', 'loaded'],\n",
              " ['wait', 'flash', 'would', 'need', 'usb', 'storage', 'too'],\n",
              " ['sure'],\n",
              " ['im',\n",
              "  'not',\n",
              "  'sure',\n",
              "  'if',\n",
              "  'it',\n",
              "  'was',\n",
              "  'loaded',\n",
              "  'll',\n",
              "  'try',\n",
              "  'again',\n",
              "  'thanks'],\n",
              " ['hey', 'asl'],\n",
              " ['hey'],\n",
              " ['cali'],\n",
              " ['cali'],\n",
              " [],\n",
              " [],\n",
              " ['uk'],\n",
              " ['cool', 'never', 'been', 'hows', 'it', 'like'],\n",
              " [],\n",
              " ['what', 'part'],\n",
              " ['yeah', 'its', 'good', 'like'],\n",
              " ['live', 'in', 'america'],\n",
              " [],\n",
              " ['ehh'],\n",
              " ['no', 'duh'],\n",
              " ['so'],\n",
              " ['there'],\n",
              " [],\n",
              " ['im', 'like', 'talkin', 'to', 'myself', 'here'],\n",
              " ['okai', 'cheeky', 'american', 'typical'],\n",
              " ['god', 'give', 'me', 'chance', 'having', 'wank', 'here', 'like'],\n",
              " ['believe', 'in', 'god'],\n",
              " [],\n",
              " ['yush',\n",
              "  'like',\n",
              "  'most',\n",
              "  'of',\n",
              "  'the',\n",
              "  'american',\n",
              "  'population',\n",
              "  'are',\n",
              "  'bible',\n",
              "  'bashers',\n",
              "  'and',\n",
              "  'homophobics',\n",
              "  'so',\n",
              "  'im',\n",
              "  'gonna',\n",
              "  'say',\n",
              "  'yeah',\n",
              "  'that',\n",
              "  'do',\n",
              "  'believe',\n",
              "  'in',\n",
              "  'god'],\n",
              " ['me', 'too'],\n",
              " ['hahah'],\n",
              " ['lool'],\n",
              " ['mad'],\n",
              " [],\n",
              " ['good', 'stuff', 'then'],\n",
              " ['yush'],\n",
              " ['at', 'you'],\n",
              " ['yeah'],\n",
              " ['or', 'in', 'general', 'lol'],\n",
              " ['yeah'],\n",
              " ['nah', 'im', 'not', 'mad', 'at', 'you'],\n",
              " ['just',\n",
              "  'dnt',\n",
              "  'be',\n",
              "  'rude',\n",
              "  'ta',\n",
              "  'me',\n",
              "  'when',\n",
              "  'take',\n",
              "  'my',\n",
              "  'time',\n",
              "  'ta',\n",
              "  'type',\n",
              "  'bk',\n",
              "  'lyk'],\n",
              " ['lol'],\n",
              " ['whats', 'your', 'name', 'luv'],\n",
              " ['cool', 'then', 'we', 'cool'],\n",
              " ['jane'],\n",
              " ['urs'],\n",
              " [],\n",
              " ['matthew'],\n",
              " ['aewsome'],\n",
              " ['cool', 'name'],\n",
              " ['awk', 'yours', 'too'],\n",
              " ['your', 'sweet'],\n",
              " ['thx',\n",
              "  'most',\n",
              "  'people',\n",
              "  'say',\n",
              "  'that',\n",
              "  'im',\n",
              "  'weird',\n",
              "  'but',\n",
              "  'ur',\n",
              "  'like',\n",
              "  'the',\n",
              "  'frist',\n",
              "  'person',\n",
              "  'to',\n",
              "  'be',\n",
              "  'nice',\n",
              "  'to',\n",
              "  'me'],\n",
              " ['hahaha'],\n",
              " ['well',\n",
              "  'you',\n",
              "  'have',\n",
              "  'to',\n",
              "  'be',\n",
              "  'nice',\n",
              "  'ta',\n",
              "  'get',\n",
              "  'nice',\n",
              "  'in',\n",
              "  'return',\n",
              "  'lol'],\n",
              " ['im', 'nice', 'guy', 'when', 'you', 'get', 'ta', 'get', 'me'],\n",
              " ['know'],\n",
              " ['cool'],\n",
              " [],\n",
              " ['so'],\n",
              " ['have', 'ta', 'go', 'now', 'luv'],\n",
              " ['do', 'like', 'ur', 'life', 'is', 'it', 'good'],\n",
              " ['awww', 'why'],\n",
              " ['du', 'wnt', 'me', 'to', 'stay'],\n",
              " ['yes', 'my', 'luv'],\n",
              " ['ok'],\n",
              " ['just', 'for', 'bit', 'tho'],\n",
              " ['its', 'late', 'here', 'your', 'lucky', 'like', 'you', 'haha'],\n",
              " ['ok'],\n",
              " ['im', 'gonna', 'stay', 'for', 'bit'],\n",
              " ['for', 'you'],\n",
              " ['if', 'you', 'want'],\n",
              " ['me', 'too', 'like', 'too', 'hahaha'],\n",
              " ['awk', 'thats', 'kul', 'like'],\n",
              " ['cool', 'luv', 'for', 'that', 'so', 'nice'],\n",
              " ['hahah'],\n",
              " ['lt'],\n",
              " ['wait', 'what', 'time', 'is', 'it'],\n",
              " ['here'],\n",
              " ['lol'],\n",
              " ['yeah'],\n",
              " [],\n",
              " ['wbu'],\n",
              " ['omg', 'its', 'like'],\n",
              " ['lool'],\n",
              " ['in', 'the', 'afternoon', 'or', 'mornin'],\n",
              " ['afternoon'],\n",
              " ['lol'],\n",
              " ['already', 'got', 'ur', 'dai', 'over', 'nd', 'dun', 'wif'],\n",
              " ['thats', 'soo', 'cool'],\n",
              " ['stil', 'havta', 'do', 'mine', 'lol'],\n",
              " ['hahaha', 'yeah'],\n",
              " ['so'],\n",
              " ['do', 'like', 'ur', 'life'],\n",
              " ['meh', 'sumtymes'],\n",
              " ['wbu'],\n",
              " ['kinda'],\n",
              " ['kinda', 'chu', 'okai', 'luv'],\n",
              " ['aww', 'ok', 'luv', 'too', 'hahaha'],\n",
              " ['bye'],\n",
              " ['bye'],\n",
              " ['yeah', 'dont', 'have', 'to', 'go'],\n",
              " ['no'],\n",
              " ['im', 'gonna', 'rebel', 'ta', 'talk', 'ta', 'you'],\n",
              " ['awww', 'thx', 'so', 'sweet'],\n",
              " ['sweets', 'my', 'middle', 'name', 'aha'],\n",
              " ['nah', 'its', 'really', 'joseph', 'lol'],\n",
              " ['hahaha'],\n",
              " ['ur', 'so', 'funny', 'luva'],\n",
              " ['lol'],\n",
              " ['glad', 'ta', 'make', 'you', 'smile', 'love'],\n",
              " ['when', 'frist', 'started', 'to', 'talk', 'to', 'made', 'me', 'smile'],\n",
              " ['awwwk'],\n",
              " ['same', 'to', 'you', 'honey'],\n",
              " [],\n",
              " ['aww', 'thx', 'luv'],\n",
              " ['no', 'barr', 'love'],\n",
              " ['its', 'the', 'truth'],\n",
              " ['so',\n",
              "  'whats',\n",
              "  'your',\n",
              "  'plans',\n",
              "  'for',\n",
              "  'the',\n",
              "  'rest',\n",
              "  'of',\n",
              "  'your',\n",
              "  'apos',\n",
              "  'night',\n",
              "  'apos',\n",
              "  'lol'],\n",
              " ['doin', 'homework'],\n",
              " ['sucks'],\n",
              " ['awk'],\n",
              " ['so', 'whats', 'ur', 'plans', 'luv'],\n",
              " ['im',\n",
              "  'just',\n",
              "  'gonna',\n",
              "  'chill',\n",
              "  'for',\n",
              "  'the',\n",
              "  'rest',\n",
              "  'of',\n",
              "  'the',\n",
              "  'apos',\n",
              "  'night',\n",
              "  'apos',\n",
              "  'lol',\n",
              "  'prob',\n",
              "  'won',\n",
              "  'apos',\n",
              "  'sleep',\n",
              "  'haha'],\n",
              " ['oh', 'thats', 'cool'],\n",
              " [],\n",
              " ['well',\n",
              "  'really',\n",
              "  'gtg',\n",
              "  'because',\n",
              "  'my',\n",
              "  'mom',\n",
              "  'is',\n",
              "  'yelling',\n",
              "  'at',\n",
              "  'me',\n",
              "  'am',\n",
              "  'so',\n",
              "  'sorry'],\n",
              " ['its', 'okai', 'hun'],\n",
              " ['it', 'was', 'nice', 'talking', 'to', 'you'],\n",
              " [],\n",
              " ['ur', 'my', 'luv', 'always', 'and', 'forever', 'never', 'forget'],\n",
              " ['awk', 'lt', 'same', 'to', 'you', 'baby'],\n",
              " ['bye', 'luv'],\n",
              " ['apos', 'll', 'always', 'think', 'about', 'you', 'love'],\n",
              " ['bye', 'xoxo'],\n",
              " ['hey'],\n",
              " ['hi'],\n",
              " ['how', 'are', 'you'],\n",
              " ['whats', 'going', 'on'],\n",
              " ['hey'],\n",
              " ['hello'],\n",
              " [],\n",
              " ['how', 'are', 'going', 'stranger'],\n",
              " ['hi'],\n",
              " ['male',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'female',\n",
              "  'male',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'male',\n",
              "  'female',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'female',\n",
              "  'female',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'male'],\n",
              " ['male',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'female',\n",
              "  'male',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'male',\n",
              "  'female',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'female',\n",
              "  'female',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'male'],\n",
              " [],\n",
              " [],\n",
              " ['planet',\n",
              "  'html',\n",
              "  'video',\n",
              "  'secure',\n",
              "  'streaming',\n",
              "  'lt',\n",
              "  'http',\n",
              "  'stackoverflow',\n",
              "  'com',\n",
              "  'questions',\n",
              "  'html',\n",
              "  'video',\n",
              "  'secure',\n",
              "  'streaming',\n",
              "  'gt'],\n",
              " ['bugmail',\n",
              "  'bug',\n",
              "  'new',\n",
              "  'autofocus',\n",
              "  'attribute',\n",
              "  'should',\n",
              "  'be',\n",
              "  'ignored',\n",
              "  'after',\n",
              "  'the',\n",
              "  'load',\n",
              "  'event',\n",
              "  'lt',\n",
              "  'http',\n",
              "  'lists',\n",
              "  'org',\n",
              "  'archives',\n",
              "  'public',\n",
              "  'public',\n",
              "  'html',\n",
              "  'bugzilla',\n",
              "  'apr',\n",
              "  'html',\n",
              "  'gt',\n",
              "  'bug',\n",
              "  'new',\n",
              "  'if',\n",
              "  'more',\n",
              "  'than',\n",
              "  'one',\n",
              "  'element',\n",
              "  'has',\n",
              "  'the',\n",
              "  'autofocus',\n",
              "  'attribute',\n",
              "  'specified',\n",
              "  'only',\n",
              "  'the',\n",
              "  'first',\n",
              "  'one',\n",
              "  'should',\n",
              "  'be',\n",
              "  'focused',\n",
              "  'lt',\n",
              "  'http',\n",
              "  'lists',\n",
              "  'org',\n",
              "  'archives',\n",
              "  'public',\n",
              "  'public',\n",
              "  'html',\n",
              "  'bugzilla',\n",
              "  'apr',\n",
              "  'html',\n",
              "  'gt'],\n",
              " ['tell', 'me', 'secret', 'of', 'yours'],\n",
              " ['hi'],\n",
              " ['hate', 'you'],\n",
              " ['boobs', 'for', 'dick'],\n",
              " ['hi'],\n",
              " ['if', 'have', 'dick'],\n",
              " ['lol'],\n",
              " ['hi'],\n",
              " ['lol'],\n",
              " ['so'],\n",
              " ['or'],\n",
              " [],\n",
              " ['you'],\n",
              " [],\n",
              " ['cool'],\n",
              " ['are', 'you', 'hornyy'],\n",
              " ['yes'],\n",
              " ['want', 'to', 'do', 'virtual', 'sex'],\n",
              " ['how', 'horny'],\n",
              " ['sure'],\n",
              " ['whats', 'that', 'like', 'how', 'do', 'do', 'it'],\n",
              " ['age'],\n",
              " [],\n",
              " [],\n",
              " ['cool'],\n",
              " [],\n",
              " ['do', 'you', 'have', 'camera'],\n",
              " [],\n",
              " ['have', 'pics'],\n",
              " ['no', 'webcam'],\n",
              " ['ok', 'would', 'like', 'to', 'see', 'pictures'],\n",
              " ['mee', 'too'],\n",
              " ['email'],\n",
              " ['hey'],\n",
              " ['send', 'me', 'and', 'ill', 'send', 'one'],\n",
              " ['msn', 'address'],\n",
              " ['have', 'yahoo'],\n",
              " ['email'],\n",
              " ['whts', 'urs'],\n",
              " ['okey'],\n",
              " ['added'],\n",
              " ['send', 'anythign'],\n",
              " ['sent', 'the', 'request'],\n",
              " ['can', 'just', 'sand', 'pic'],\n",
              " ['send'],\n",
              " ['hi',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'some',\n",
              "  'vim',\n",
              "  'configuration',\n",
              "  'knob',\n",
              "  'but',\n",
              "  'can',\n",
              "  'seem',\n",
              "  'to',\n",
              "  'find',\n",
              "  'anything',\n",
              "  'even',\n",
              "  'though',\n",
              "  'googled',\n",
              "  'for',\n",
              "  'fifteen',\n",
              "  'minutes',\n",
              "  'now',\n",
              "  'what',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'is',\n",
              "  'vertical',\n",
              "  'indicator',\n",
              "  'line',\n",
              "  'in',\n",
              "  'vim',\n",
              "  'at',\n",
              "  'column',\n",
              "  'but',\n",
              "  'which',\n",
              "  'does',\n",
              "  'not',\n",
              "  'enforce',\n",
              "  'word',\n",
              "  'wrapping'],\n",
              " ['basically',\n",
              "  'want',\n",
              "  'to',\n",
              "  'know',\n",
              "  'when',\n",
              "  'going',\n",
              "  'past',\n",
              "  'columns',\n",
              "  'in',\n",
              "  'my',\n",
              "  'code',\n",
              "  'but',\n",
              "  'don',\n",
              "  'want',\n",
              "  'it',\n",
              "  'to',\n",
              "  'enforce',\n",
              "  'wordwrap',\n",
              "  'vertical',\n",
              "  'line',\n",
              "  'or',\n",
              "  'so',\n",
              "  'would',\n",
              "  'be',\n",
              "  'nice'],\n",
              " ['does',\n",
              "  'anybody',\n",
              "  'know',\n",
              "  'how',\n",
              "  'to',\n",
              "  'do',\n",
              "  'that',\n",
              "  'thought',\n",
              "  'ruler',\n",
              "  'is',\n",
              "  'what',\n",
              "  'mean',\n",
              "  'but',\n",
              "  'apparently',\n",
              "  'it',\n",
              "  'not'],\n",
              " ['frerich',\n",
              "  'ruler',\n",
              "  'will',\n",
              "  'display',\n",
              "  'column',\n",
              "  'numbers',\n",
              "  'across',\n",
              "  'the',\n",
              "  'bottom'],\n",
              " ['frerich', 'there', 'way'],\n",
              " ['ic', 'yes', 'noticed', 'but', 'somehow', 'always', 'overlook', 'that'],\n",
              " ['match', 'error', 'gt'],\n",
              " ['it',\n",
              "  'highlights',\n",
              "  'any',\n",
              "  'text',\n",
              "  'characters',\n",
              "  'that',\n",
              "  'are',\n",
              "  'past',\n",
              "  'columns'],\n",
              " ['cool', 'that', 'not', 'bad'],\n",
              " ['think',\n",
              "  'pixel',\n",
              "  'vertical',\n",
              "  'line',\n",
              "  'would',\n",
              "  'be',\n",
              "  'nice',\n",
              "  'but',\n",
              "  'vim',\n",
              "  'cannot',\n",
              "  'do',\n",
              "  'that',\n",
              "  'yet'],\n",
              " ['mgedmin',\n",
              "  'yes',\n",
              "  'using',\n",
              "  'gvim',\n",
              "  'so',\n",
              "  'suspect',\n",
              "  'it',\n",
              "  'should',\n",
              "  'be',\n",
              "  'feasible',\n",
              "  'after',\n",
              "  'all',\n",
              "  'it',\n",
              "  'can',\n",
              "  'do',\n",
              "  'those',\n",
              "  'nice',\n",
              "  'red',\n",
              "  'lines',\n",
              "  'when',\n",
              "  'spell',\n",
              "  'checking'],\n",
              " ['actually',\n",
              "  'maybe',\n",
              "  'prefer',\n",
              "  'slightly',\n",
              "  'different',\n",
              "  'shade',\n",
              "  'of',\n",
              "  'the',\n",
              "  'background',\n",
              "  'colour',\n",
              "  'but',\n",
              "  'vim',\n",
              "  'cannot',\n",
              "  'do',\n",
              "  'that',\n",
              "  'eithe'],\n",
              " ['it',\n",
              "  'simple',\n",
              "  'matter',\n",
              "  'of',\n",
              "  'writing',\n",
              "  'the',\n",
              "  'drawing',\n",
              "  'code',\n",
              "  'and',\n",
              "  'submitting',\n",
              "  'the',\n",
              "  'patch'],\n",
              " ['and', 'convincing', 'bram', 'to', 'accept', 'it'],\n",
              " ['think'],\n",
              " ['think',\n",
              "  'would',\n",
              "  'be',\n",
              "  'afraid',\n",
              "  'to',\n",
              "  'get',\n",
              "  'told',\n",
              "  'that',\n",
              "  'it',\n",
              "  'can',\n",
              "  'do',\n",
              "  'that',\n",
              "  'already',\n",
              "  'fool'],\n",
              " ['mgedmin',\n",
              "  'your',\n",
              "  'approach',\n",
              "  'is',\n",
              "  'already',\n",
              "  'step',\n",
              "  'in',\n",
              "  'the',\n",
              "  'right',\n",
              "  'direction',\n",
              "  'now',\n",
              "  'have',\n",
              "  'that',\n",
              "  'line',\n",
              "  'in',\n",
              "  'my',\n",
              "  'vimrc',\n",
              "  'plus',\n",
              "  'courtesy',\n",
              "  'of',\n",
              "  'mgedim',\n",
              "  'of',\n",
              "  'vim',\n",
              "  'fame',\n",
              "  'comment'],\n",
              " ['mgedmin', 'thanks'],\n",
              " ['frerich',\n",
              "  'didn',\n",
              "  'invent',\n",
              "  'it',\n",
              "  'got',\n",
              "  'it',\n",
              "  'from',\n",
              "  'somewhere',\n",
              "  'possibly',\n",
              "  'tip',\n",
              "  'on',\n",
              "  'vim',\n",
              "  'org'],\n",
              " ['mgedmin',\n",
              "  'well',\n",
              "  'then',\n",
              "  'let',\n",
              "  'me',\n",
              "  'tell',\n",
              "  'you',\n",
              "  'that',\n",
              "  'you',\n",
              "  'should',\n",
              "  'not',\n",
              "  'mention',\n",
              "  'that',\n",
              "  'when',\n",
              "  'all',\n",
              "  'the',\n",
              "  'groupies',\n",
              "  'show',\n",
              "  'up',\n",
              "  'at',\n",
              "  'your',\n",
              "  'place',\n",
              "  'and',\n",
              "  'start',\n",
              "  'throwing',\n",
              "  'their',\n",
              "  'underwear',\n",
              "  'up',\n",
              "  'your',\n",
              "  'balcony'],\n",
              " ['mgedmin',\n",
              "  'just',\n",
              "  'pretend',\n",
              "  'you',\n",
              "  'always',\n",
              "  'knew',\n",
              "  'people',\n",
              "  'would',\n",
              "  'somewhen',\n",
              "  'appreciate',\n",
              "  'your',\n",
              "  'genius'],\n",
              " [],\n",
              " ['aren',\n",
              "  'geniuses',\n",
              "  'supposed',\n",
              "  'to',\n",
              "  'act',\n",
              "  'all',\n",
              "  'modest',\n",
              "  'and',\n",
              "  'humble'],\n",
              " ['think', 'you', 'are', 'talking', 'of', 'thinktank', 'drones'],\n",
              " ['mmm',\n",
              "  'seems',\n",
              "  'putty',\n",
              "  'indeed',\n",
              "  'does',\n",
              "  'not',\n",
              "  'allow',\n",
              "  'cursor',\n",
              "  'highlight',\n",
              "  'overrides'],\n",
              " ['if',\n",
              "  've',\n",
              "  'opened',\n",
              "  'vim',\n",
              "  'with',\n",
              "  'vim',\n",
              "  'some',\n",
              "  'long',\n",
              "  'lengthy',\n",
              "  'tedious',\n",
              "  'path',\n",
              "  'to',\n",
              "  'file'],\n",
              " ['is',\n",
              "  'there',\n",
              "  'an',\n",
              "  'easy',\n",
              "  'way',\n",
              "  'to',\n",
              "  'view',\n",
              "  'the',\n",
              "  'directory',\n",
              "  'of',\n",
              "  'that',\n",
              "  'file',\n",
              "  'to',\n",
              "  'open',\n",
              "  'different',\n",
              "  'file'],\n",
              " ['explore'],\n",
              " [],\n",
              " ['ah', 'very', 'neat', 'thanks'],\n",
              " ['hi'],\n",
              " ['gurl'],\n",
              " ['asl'],\n",
              " ['hi'],\n",
              " ['asl'],\n",
              " ['hey', 'asl'],\n",
              " [],\n",
              " ['you', 'rst'],\n",
              " [],\n",
              " ['brazil'],\n",
              " [],\n",
              " [],\n",
              " ['turkey'],\n",
              " ['what', 'your', 'name'],\n",
              " ['wow'],\n",
              " ['sofia'],\n",
              " ['hmm', 'good', 'name'],\n",
              " ['sofia'],\n",
              " [],\n",
              " ['do', 'ukan'],\n",
              " [],\n",
              " ['wassup'],\n",
              " ['do', 'ukan'],\n",
              " ['looool'],\n",
              " ['cute'],\n",
              " ['thanks'],\n",
              " ['wassup'],\n",
              " ['what', 'are', 'do', 'ng'],\n",
              " ['mm'],\n",
              " ['wassup'],\n",
              " ['noth', 'ng', 'much'],\n",
              " ['apos', 're', 'shopping'],\n",
              " ['bored'],\n",
              " [],\n",
              " ['mm'],\n",
              " ['yeah'],\n",
              " ['wana', 'see', 'your', 'pic'],\n",
              " ['poss', 'ble'],\n",
              " [],\n",
              " ['yeah'],\n",
              " ['msn'],\n",
              " ['hm', 'have', 'msn'],\n",
              " ['give', 'me', 'your', 'msn'],\n",
              " ['email'],\n",
              " ['add', 'me', 'wa'],\n",
              " ['apos', 'll', 'send', 'pictures'],\n",
              " ['add', 'me', 'your', 'msn', 'st'],\n",
              " ['wait', 'okay', 'apos', 'll', 'give', 'the', 'best'],\n",
              " ['what', 'your', 'msn'],\n",
              " ['are', 'you', 'here'],\n",
              " ['heeeyyy'],\n",
              " ['sorrrry'],\n",
              " ['what', 'your', 'msn'],\n",
              " ['apos', 'gonna', 'walk', 'to', 'the', 'bathroom'],\n",
              " ['what', 'your', 'msn'],\n",
              " ['wait', 'ok', 'apos', 'faxing', 'photo'],\n",
              " ['ok'],\n",
              " ['msn'],\n",
              " ['again'],\n",
              " ['pls'],\n",
              " [],\n",
              " ['email'],\n",
              " ['why', 'did', 'you', 'go', 'bathroom'],\n",
              " ['send'],\n",
              " [],\n",
              " ['hmm', 'good', 'pic'],\n",
              " [],\n",
              " ['check', 'it', 'out', 'photos'],\n",
              " ['email'],\n",
              " ['your', 'msn'],\n",
              " ['yeah'],\n",
              " ['add'],\n",
              " ['send', 'me', 'your', 'pictures'],\n",
              " ['but', 'arent', 'onl', 'ne'],\n",
              " ['xd'],\n",
              " ['yeaaaaahhhh'],\n",
              " ['send', 'me', 'your', 'pictures'],\n",
              " ['send'],\n",
              " ['want', 'to', 'see', 'your', 'face'],\n",
              " ['send'],\n",
              " ['wait'],\n",
              " ['wow'],\n",
              " ['atmosphere', 'apos', 'got', 'lot'],\n",
              " ['thanks'],\n",
              " [],\n",
              " ['have', 'you', 'facebook'],\n",
              " [],\n",
              " ['oh', 'think', 'you', 'should', 'get'],\n",
              " ['apos', 'll', 'call', 'the', 'next'],\n",
              " [],\n",
              " ['sorry'],\n",
              " ['hmm', 'ok'],\n",
              " ['bye'],\n",
              " ['by'],\n",
              " ['hello', 'everyone'],\n",
              " ['cc', 'ae', 'df', 'hej'],\n",
              " ['apart',\n",
              "  'from',\n",
              "  'being',\n",
              "  'publication',\n",
              "  'time',\n",
              "  'soon',\n",
              "  'what',\n",
              "  'has',\n",
              "  'happened'],\n",
              " ['want',\n",
              "  'to',\n",
              "  'tell',\n",
              "  'you',\n",
              "  'that',\n",
              "  'we',\n",
              "  'usurped',\n",
              "  'hixie',\n",
              "  'and',\n",
              "  'instated',\n",
              "  'gsnedders',\n",
              "  'as',\n",
              "  'editor',\n",
              "  'but',\n",
              "  'would',\n",
              "  'be',\n",
              "  'lying'],\n",
              " ['think', 'how', 'awesome', 'that', 'would', 'be', 'though'],\n",
              " ['heh'],\n",
              " ['we',\n",
              "  'could',\n",
              "  'get',\n",
              "  'an',\n",
              "  'emo',\n",
              "  'version',\n",
              "  'of',\n",
              "  'the',\n",
              "  'spec',\n",
              "  'done',\n",
              "  'out',\n",
              "  'in',\n",
              "  'black'],\n",
              " ['take', 'that', 'as', 'nothing', 'in', 'particular', 'has', 'happened'],\n",
              " ['can',\n",
              "  'remember',\n",
              "  'anything',\n",
              "  'but',\n",
              "  'there',\n",
              "  'were',\n",
              "  'some',\n",
              "  'decisions',\n",
              "  'and',\n",
              "  'stuff'],\n",
              " ['asusme', 'you', 'were', 'here', 'when', 'lt', 'device', 'gt', 'died'],\n",
              " ['oh',\n",
              "  'and',\n",
              "  'hixie',\n",
              "  'had',\n",
              "  'some',\n",
              "  'proposal',\n",
              "  'for',\n",
              "  'multi',\n",
              "  'track',\n",
              "  'media',\n",
              "  'stuff'],\n",
              " ['myabe', 'you', 'missed', 'that'],\n",
              " ['yes', 'and', 'yes'],\n",
              " ['thanks'],\n",
              " ['hi'],\n",
              " ['asl'],\n",
              " ['hi'],\n",
              " ['apos', 'for', 'north', 'korea'],\n",
              " ['are',\n",
              "  'we',\n",
              "  'meeting',\n",
              "  'sunday',\n",
              "  'monday',\n",
              "  'tuesday',\n",
              "  'at',\n",
              "  'tpac',\n",
              "  'or',\n",
              "  'just',\n",
              "  'monday',\n",
              "  'tuesday'],\n",
              " ['guess',\n",
              "  'nobody',\n",
              "  'ever',\n",
              "  'responded',\n",
              "  'when',\n",
              "  'jdaggett',\n",
              "  'asked',\n",
              "  'that',\n",
              "  'months',\n",
              "  'ago'],\n",
              " ['so', 'guess', 'it', 'probably', 'just', 'monday', 'tuesday'],\n",
              " ['fajita', 'has', 'factoid', 'about', 'the', 'error'],\n",
              " ['sorry', 'don', 'know'],\n",
              " ['and', 'think', 'it', 'has', 'link'],\n",
              " ['pacman', 'is', 'mod_ssl', 'loaded'],\n",
              " ['loaded', 'it', 'using', 'apache'],\n",
              " ['is', 'there', 'way', 'to', 'check', 'if', 'it', 'actually', 'loaded'],\n",
              " ['mod_info'],\n",
              " ['mod_info',\n",
              "  'is',\n",
              "  'href',\n",
              "  'http',\n",
              "  'httpd',\n",
              "  'apache',\n",
              "  'org',\n",
              "  'docs',\n",
              "  'mod',\n",
              "  'mod_info',\n",
              "  'html',\n",
              "  'http',\n",
              "  'httpd',\n",
              "  'apache',\n",
              "  'org',\n",
              "  'docs',\n",
              "  'mod',\n",
              "  'mod_info',\n",
              "  'html',\n",
              "  'or',\n",
              "  'href',\n",
              "  'http',\n",
              "  'httpd',\n",
              "  'apache',\n",
              "  'org',\n",
              "  'docs',\n",
              "  'mod',\n",
              "  'mod_info',\n",
              "  'html',\n",
              "  'http',\n",
              "  'httpd',\n",
              "  'apache',\n",
              "  'org',\n",
              "  'docs',\n",
              "  'mod',\n",
              "  'mod_info',\n",
              "  'html'],\n",
              " ['lt', 'location', 'server', 'info', 'gt'],\n",
              " ['sethandler', 'server', 'info'],\n",
              " ['lt', 'location', 'gt'],\n",
              " ['adding', 'that', 'handler', 'does', 'nothing'],\n",
              " ['how', 'about', 'browsing', 'to', 'server', 'info', 'on', 'your', 'server'],\n",
              " ['yea', 'that', 'what', 'meant', 'it', 'gives', 'me', 'error'],\n",
              " ['restarted', 'apache', 'btw'],\n",
              " ['ok',\n",
              "  'your',\n",
              "  'config',\n",
              "  'is',\n",
              "  'possibly',\n",
              "  'bloated',\n",
              "  'at',\n",
              "  'the',\n",
              "  'vhost',\n",
              "  'level'],\n",
              " ['what',\n",
              "  'ports',\n",
              "  'do',\n",
              "  'you',\n",
              "  'want',\n",
              "  'your',\n",
              "  'server',\n",
              "  'to',\n",
              "  'listen',\n",
              "  'on',\n",
              "  'see',\n",
              "  'some',\n",
              "  'port',\n",
              "  'there'],\n",
              " ['port', 'for', 'http', 'for', 'https'],\n",
              " ['try',\n",
              "  'adding',\n",
              "  'quot',\n",
              "  'gryzor',\n",
              "  'rulez',\n",
              "  'quot',\n",
              "  'line',\n",
              "  'in',\n",
              "  'your',\n",
              "  'default',\n",
              "  'vhost',\n",
              "  'conf',\n",
              "  'file'],\n",
              " ['haha'],\n",
              " ['just', 'to', 'see', 'if', 'it', 'really', 'parsed'],\n",
              " ['ok'],\n",
              " ['put', 'it', 'inside', 'the', 'lt', 'ifmodule', 'gt', 'block'],\n",
              " ['for',\n",
              "  'some',\n",
              "  'stupid',\n",
              "  'reason',\n",
              "  'apache',\n",
              "  'cant',\n",
              "  'parse',\n",
              "  'that',\n",
              "  'string',\n",
              "  'yet'],\n",
              " ['restarted', 'it'],\n",
              " ['it', 'starts', 'fine'],\n",
              " ['no', 'errors'],\n",
              " ['then', 'you', 'know', 'where', 'to', 'look'],\n",
              " ['hmm'],\n",
              " ['it', 'either', 'means', 'apache', 'has', 'acknowledged', 'rule'],\n",
              " ['doubtful'],\n",
              " ['which',\n",
              "  'would',\n",
              "  'be',\n",
              "  'good',\n",
              "  'but',\n",
              "  'seems',\n",
              "  'not',\n",
              "  'probable',\n",
              "  'to',\n",
              "  'me'],\n",
              " ['hehe'],\n",
              " ['or',\n",
              "  'either',\n",
              "  'that',\n",
              "  'your',\n",
              "  'config',\n",
              "  'is',\n",
              "  'not',\n",
              "  'read',\n",
              "  'for',\n",
              "  'some',\n",
              "  'reason'],\n",
              " ['gryzor', 'lemme', 'ship', 'mod_pony', 'first'],\n",
              " ['hehe'],\n",
              " ['you', 'think', 'kidding'],\n",
              " ['servertokens', 'www', 'iamcode', 'net'],\n",
              " ['apache',\n",
              "  'win',\n",
              "  'php',\n",
              "  'dev',\n",
              "  'mod_ssl',\n",
              "  'openssl',\n",
              "  'mod_i_win',\n",
              "  'mod_pony',\n",
              "  'not_yours'],\n",
              " ['quot',\n",
              "  'not',\n",
              "  'yours',\n",
              "  'quot',\n",
              "  'will',\n",
              "  'be',\n",
              "  'the',\n",
              "  'first',\n",
              "  'release',\n",
              "  'going',\n",
              "  'to',\n",
              "  'use',\n",
              "  'named',\n",
              "  'releases'],\n",
              " ['yeah', 'goog', 'idea'],\n",
              " ['good'],\n",
              " ['it', 'works'],\n",
              " ['damn', 'right', 'it', 'does'],\n",
              " ['thanks'],\n",
              " ['jgraham',\n",
              "  'getting',\n",
              "  'an',\n",
              "  'error',\n",
              "  'from',\n",
              "  'pms',\n",
              "  'that',\n",
              "  've',\n",
              "  'never',\n",
              "  'had',\n",
              "  'before'],\n",
              " ['quot',\n",
              "  'typeerror',\n",
              "  'expected',\n",
              "  'string',\n",
              "  'or',\n",
              "  'buffer',\n",
              "  'quot',\n",
              "  'deep',\n",
              "  'inside',\n",
              "  'build',\n",
              "  'bdist',\n",
              "  'linux',\n",
              "  'egg',\n",
              "  'html',\n",
              "  'lib',\n",
              "  'treewalkers',\n",
              "  'lxmletree',\n",
              "  'py',\n",
              "  'according',\n",
              "  'to',\n",
              "  'the',\n",
              "  'traceback',\n",
              "  'get'],\n",
              " ['hi'],\n",
              " ['hi'],\n",
              " ['asl'],\n",
              " ['what', 'is', 'your', 'name'],\n",
              " ['gun'],\n",
              " ['gun', 'lee'],\n",
              " ['korea'],\n",
              " ['hello'],\n",
              " [],\n",
              " ['and', 'am', 'jicak', 'in', 'india'],\n",
              " ['oh', 'india'],\n",
              " ['like', 'india', 'food'],\n",
              " ['know', 'korea', 'food'],\n",
              " ['thank', 'apos'],\n",
              " ['you', 'are', 'come', 'india'],\n",
              " ['don', 'apos', 'have', 'money'],\n",
              " ['so', 'expensive'],\n",
              " ['and'],\n",
              " ['apos', 'yong'],\n",
              " ['you', 'have', 'mobile'],\n",
              " ['ok'],\n",
              " ['have'],\n",
              " ['hey'],\n",
              " ['know', 'baseball'],\n",
              " ['what', 'is', 'your', 'mo', 'no'],\n",
              " ['mo', 'no'],\n",
              " ['what'],\n",
              " ['know', 'baseball', 'but', 'no', 'play'],\n",
              " ['sorry'],\n",
              " ['oh'],\n",
              " ['play', 'cricket'],\n",
              " ['ah'],\n",
              " ['play', 'baseball'],\n",
              " ['but'],\n",
              " ['know', 'cricket'],\n",
              " ['cricket'],\n",
              " ['famouse'],\n",
              " ['don', 'apos', 'know', 'cricket'],\n",
              " ...]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create bag f words list from chat sentences\n",
        "def create_bow_from_text(text):\n",
        "    bow_words = []\n",
        "    text_word_list = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "    bow_words.append(text_word_list)\n",
        "    return bow_words[0]\n",
        "\n",
        "\n",
        "def create_bow_from_text_list(text_list):\n",
        "    bow_words = []\n",
        "    for text in text_list:\n",
        "        # text_word_list = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "        # bow_words.append(text_word_list)\n",
        "        bow_words.append(create_bow_from_text(text))\n",
        "    \n",
        "    return bow_words\n",
        "\n",
        "\n",
        "bow_words = create_bow_from_text_list(pan12_df['text'])\n",
        "bow_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c60d18f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Likely\n",
            "[(67, 1)]\n",
            "don\n"
          ]
        }
      ],
      "source": [
        "# create word indexes and frequencies from chat sentences bow\n",
        "id2word = corpora.Dictionary(bow_words)\n",
        "corpus = []\n",
        "for word in bow_words:\n",
        "    corpus.append(id2word.doc2bow(word))\n",
        "\n",
        "\n",
        "print(pan12_df['text'][7])\n",
        "print(corpus[7])\n",
        "print(id2word[31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "dbe34994",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create LDA topic model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "id2word=id2word,\n",
        "num_topics=10, \n",
        "update_every=1,\n",
        "chunksize=100,\n",
        "passes=10,\n",
        "alpha='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf30d0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bc6d6a",
      "metadata": {},
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600af7b9",
      "metadata": {},
      "source": [
        "### LDA topic model - Bad performance due to sexual term sparsity and diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a734e28c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bag f words list from chat sentences\n",
        "def create_bow_from_text(text):\n",
        "    bow_words = []\n",
        "    text_word_list = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "    bow_words.append(text_word_list)\n",
        "    return bow_words[0]\n",
        "\n",
        "\n",
        "def create_bow_from_text_list(text_list):\n",
        "    bow_words = []\n",
        "    for text in text_list:\n",
        "        # text_word_list = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "        # bow_words.append(text_word_list)\n",
        "        bow_words.append(create_bow_from_text(text))\n",
        "    \n",
        "    return bow_words\n",
        "\n",
        "\n",
        "bow_words = create_bow_from_text_list(pan12_df['preprocessed_bow'])\n",
        "bow_words\n",
        "\n",
        "# bow_words.append(sex_word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a1f56f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you ever have enough to shave it\n",
            "[(1, 1), (5, 1), (31, 1), (34, 1), (36, 1), (37, 1), (38, 1)]\n",
            "shave\n"
          ]
        }
      ],
      "source": [
        "# create word indexes and frequencies from chat sentences bow\n",
        "id2word = corpora.Dictionary(bow_words)\n",
        "corpus = []\n",
        "for word in bow_words:\n",
        "    corpus.append(id2word.doc2bow(word))\n",
        "\n",
        "\n",
        "print(pan12_df['preprocessed_bow'][7])\n",
        "print(corpus[7])\n",
        "print(id2word[31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964c65d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create LDA topic model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "id2word=id2word,\n",
        "num_topics=10, \n",
        "update_every=1,\n",
        "chunksize=100,\n",
        "passes=10,\n",
        "alpha='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6679c62",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1459624586421842243367462537\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1459624586421842243367462537_data = {\"mdsDat\": {\"x\": [0.4262598554189084, 0.1191764216244423, -0.3211307359761326, -0.2622939438481509, 0.17659389981545948, -0.13333995636651533, -0.0996045627180655, -0.4172506245560066, 0.14055055393912583, 0.3710390926669349], \"y\": [0.23614378581757728, 0.43853535124459353, -0.24271332658654204, 0.3563394300266705, -0.3984303947040551, 0.15112493208586955, -0.37768058683233074, 0.008782973756211187, -0.028214069800748566, -0.14388809500724548], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [40.08961212948988, 19.408800461341354, 6.08248099503866, 5.800684476941676, 5.399003416738145, 5.369835079325616, 5.263129420445102, 5.048328958368984, 4.132478694038597, 3.4056463682719826]}, \"tinfo\": {\"Term\": [\"you\", \"your\", \"to\", \"it\", \"and\", \"am\", \"the\", \"me\", \"on\", \"out\", \"we\", \"be\", \"that\", \"like\", \"see\", \"when\", \"have\", \"or\", \"want\", \"of\", \"could\", \"get\", \"sex\", \"are\", \"my\", \"cock\", \"about\", \"in\", \"pussy\", \"just\", \"you\", \"to\", \"and\", \"me\", \"want\", \"are\", \"my\", \"do\", \"so\", \"with\", \"for\", \"what\", \"can\", \"will\", \"how\", \"make\", \"going\", \"cum\", \"in\", \"love\", \"feel\", \"let\", \"as\", \"would\", \"anything\", \"meet\", \"hot\", \"even\", \"now\", \"say\", \"if\", \"it\", \"the\", \"be\", \"that\", \"like\", \"have\", \"of\", \"get\", \"just\", \"all\", \"but\", \"is\", \"know\", \"not\", \"dont\", \"over\", \"don\", \"might\", \"hard\", \"here\", \"dick\", \"something\", \"him\", \"time\", \"stuff\", \"though\", \"try\", \"really\", \"much\", \"cant\", \"if\", \"we\", \"when\", \"was\", \"go\", \"baby\", \"her\", \"them\", \"age\", \"older\", \"girls\", \"wait\", \"young\", \"little\", \"thats\", \"maybe\", \"find\", \"tits\", \"lol\", \"sucking\", \"big\", \"taste\", \"doing\", \"married\", \"lot\", \"ask\", \"ill\", \"slut\", \"until\", \"tomorrow\", \"lots\", \"see\", \"cock\", \"about\", \"think\", \"naked\", \"well\", \"one\", \"some\", \"more\", \"other\", \"never\", \"yes\", \"been\", \"sexual\", \"hurt\", \"than\", \"who\", \"wearing\", \"back\", \"before\", \"cause\", \"inside\", \"since\", \"wanted\", \"work\", \"nervous\", \"always\", \"thing\", \"once\", \"havent\", \"on\", \"or\", \"at\", \"there\", \"suck\", \"porn\", \"fuck\", \"re\", \"down\", \"link\", \"any\", \"oh\", \"edit\", \"an\", \"cool\", \"mouth\", \"seeing\", \"mine\", \"each\", \"bed\", \"house\", \"around\", \"wouldn\", \"sitting\", \"dirty\", \"mom\", \"hair\", \"top\", \"alone\", \"pretty\", \"your\", \"pussy\", \"keep\", \"stupid\", \"simple\", \"he\", \"take\", \"lick\", \"horny\", \"ass\", \"things\", \"body\", \"does\", \"man\", \"those\", \"slowly\", \"sweet\", \"front\", \"breasts\", \"caress\", \"lap\", \"cute\", \"slide\", \"coming\", \"person\", \"skirt\", \"between\", \"virgin\", \"yours\", \"tight\", \"out\", \"sex\", \"laughing\", \"loud\", \"ever\", \"girl\", \"still\", \"had\", \"way\", \"guys\", \"hope\", \"thinking\", \"yourself\", \"touch\", \"wiht\", \"penis\", \"night\", \"bet\", \"done\", \"check\", \"anal\", \"nipples\", \"feeling\", \"guess\", \"oral\", \"yea\", \"blow\", \"strip\", \"everything\", \"teach\", \"sexy\", \"too\", \"off\", \"were\", \"she\", \"they\", \"nude\", \"its\", \"bad\", \"tell\", \"us\", \"only\", \"panties\", \"very\", \"why\", \"pics\", \"babe\", \"pic\", \"bra\", \"playing\", \"erotic\", \"fingers\", \"has\", \"beautiful\", \"jack\", \"bikini\", \"saw\", \"send\", \"address\", \"sometime\", \"did\", \"then\", \"no\", \"gonna\", \"good\", \"ok\", \"put\", \"need\", \"where\", \"play\", \"someone\", \"ready\", \"because\", \"rub\", \"long\", \"both\", \"mind\", \"talking\", \"face\", \"open\", \"orgasm\", \"tongue\", \"sit\", \"caught\", \"stop\", \"hand\", \"dressed\", \"softly\", \"sis\", \"cunt\", \"am\", \"could\", \"up\", \"guy\", \"from\", \"wish\", \"lips\", \"excited\", \"hold\", \"shower\", \"better\", \"same\", \"touching\", \"probably\", \"tonight\", \"gets\", \"sleep\", \"sometimes\", \"ummm\", \"huh\", \"real\", \"myself\", \"pants\", \"home\", \"boobs\", \"shit\", \"arms\", \"warm\", \"muah\", \"jerking\"], \"Freq\": [4883.0, 768.0, 2199.0, 1008.0, 1414.0, 362.0, 695.0, 1201.0, 356.0, 347.0, 363.0, 625.0, 552.0, 536.0, 302.0, 300.0, 500.0, 254.0, 803.0, 437.0, 206.0, 402.0, 219.0, 733.0, 714.0, 216.0, 214.0, 719.0, 206.0, 376.0, 4882.504970445245, 2198.1975298655516, 1413.7429208836343, 1200.5937982164955, 802.3186242128481, 732.8287865790994, 713.8014634809784, 607.2756645515702, 595.7538407164652, 517.791868208884, 472.05614480972827, 399.20756775556936, 380.55116345701924, 365.7055909710428, 216.11360223956984, 215.06616749148276, 213.41533959804775, 196.28423744774975, 716.6606378956178, 191.84371740499037, 140.17795310115304, 140.12007895082178, 139.94498988315, 517.7832973426237, 114.12828212083683, 114.02371547215242, 103.81015532002056, 97.19693108178276, 96.30789093492574, 95.86792673458227, 198.832343505127, 1007.2247994340684, 694.7732982051299, 624.9230648944714, 551.3230493834695, 535.52935946372, 499.97823838622094, 436.8296243324228, 401.95990095082306, 375.38949998451267, 370.81320985410736, 321.61798016570305, 293.7193492104014, 207.950478897468, 205.4219068812705, 188.45734415593876, 128.50994767698805, 122.14389080087695, 98.08773010012457, 92.05854490032361, 90.64092023903717, 86.59862126738467, 84.6403481829936, 81.07736217317066, 80.2233892508431, 75.37540014854656, 74.27857807843654, 73.18511485501539, 71.18758304680655, 70.40182442320085, 67.77893780485057, 250.92657481470061, 362.354938954183, 299.708713321691, 172.5618680262262, 158.3702825861449, 156.46439058621726, 153.09062339243965, 123.50706628108256, 98.392665570205, 97.54149086999884, 96.56172274792421, 80.72432962259802, 73.03785052169744, 63.950321123314275, 61.32285785670761, 54.709743395776876, 52.68417763944148, 50.83892421447178, 49.18120786051377, 45.50347450177024, 45.32165030920295, 45.26983986739914, 39.5366254695047, 37.764796735834544, 36.9173401410469, 34.06526606298214, 33.11609182530619, 30.62415949814854, 29.27259345936837, 25.00204796085203, 24.52458230878682, 301.2907147277233, 215.27490667701807, 213.3828563418124, 210.4346608702759, 166.78176157652635, 151.8870545270968, 130.13242486857348, 104.3334449087559, 98.40363629968635, 88.90509668351423, 65.11043018905971, 63.8415040394613, 60.68761925174674, 56.27938301617718, 56.104895963169746, 52.79287475852173, 48.894421977570914, 47.109767508433634, 41.57307851875543, 41.13409464276261, 38.95357839122527, 38.728696944044316, 33.34080416074406, 33.09980987208593, 32.18094746956623, 30.837907667277552, 30.28110409371337, 27.412834697227016, 25.5603214763129, 25.143131926625983, 355.8395561310159, 253.2048695582323, 131.24535811534264, 126.2052098696266, 121.3922057781295, 119.13612832077112, 116.84492975337662, 89.53623272407577, 80.56013317395794, 80.06026250589284, 77.95083305298009, 75.60996736379872, 71.80559134750605, 62.80047261601025, 59.93156316666715, 49.740927667763636, 47.06806999236018, 40.478387673229506, 38.83625445105019, 38.4221077559338, 36.095309001459775, 31.01140785406586, 29.634742147958047, 27.76094575827901, 27.588337941644898, 27.548720231289064, 23.045639764195705, 21.922967538888738, 21.710928319637897, 18.478129038004102, 767.5950954049699, 206.01004290352685, 154.5871698980586, 129.38766628526008, 128.85458955268984, 125.63333571594652, 123.69928156844995, 85.14544063618695, 80.43190186798445, 72.8153926422026, 58.89282987759633, 55.995203896845275, 46.55239665674071, 44.48424295378937, 35.53125227797585, 32.74674362613257, 30.93399628481093, 30.913510052463845, 30.103058316285626, 29.55535811427069, 29.16987030750239, 28.50874626331007, 25.286154914117525, 23.909160769296104, 23.802329748899623, 22.976885334111028, 21.9308572516048, 18.46555894283939, 14.996556003238096, 14.325410368371633, 346.8423715719636, 218.92596662187142, 188.0188327680589, 186.09746187555655, 131.5511670045367, 118.66192963396921, 115.46994657203093, 107.95122477253324, 95.37829400552909, 70.57398002305445, 68.20556413926535, 62.01814530619523, 61.1967247830022, 51.1090796948041, 49.52678742752455, 37.87143694751862, 37.86138918214502, 37.625005874557, 37.383624629857344, 25.528282925330334, 24.676506079188783, 23.762290523623097, 22.219525657336263, 22.063741596234564, 21.848187123281445, 21.219102514377848, 20.16776732515358, 19.305617711170505, 17.950907739270104, 17.9514034363115, 191.019187997564, 170.2277373651709, 130.2984535774343, 118.223396298394, 115.68279671905727, 113.53923131524546, 110.39808218015038, 109.50601795658777, 103.38974263379843, 94.44722108018624, 83.86606644833677, 79.11753666119874, 76.09878282003923, 71.32141068483753, 53.285072303310784, 48.39626549036774, 44.7288181631398, 37.993645254997375, 36.915701135957434, 36.684388648067, 29.772745627757384, 27.183548918636, 25.81100156427436, 24.449776337826542, 23.04778411586857, 22.803658919133504, 22.686523882683115, 20.717237253694694, 18.126403263234547, 17.243442323915474, 181.7100661223973, 176.81192524391744, 142.89839104971236, 126.39406711211439, 92.659581935133, 86.35298044890347, 75.78045078010632, 73.44140533578488, 70.5336343710317, 56.81275323970512, 49.367281118267464, 46.069190329606265, 37.06180845825054, 36.30977084875645, 34.82356279000958, 32.469071215681154, 29.73001324890503, 29.168737382179295, 28.712110493722385, 27.807076210952218, 26.995314852072564, 26.80858887449554, 26.668540102508107, 24.310638928469682, 23.990331696342547, 22.62871984636914, 21.554417165417586, 18.87168835913337, 18.380798210194516, 18.193898536028495, 361.6778568927107, 205.51688913117212, 124.24189520575892, 91.31043525488757, 74.20211610382887, 70.85511457159332, 38.43183822897417, 35.24757816843035, 31.42684357795084, 30.19525850563935, 28.326732352391748, 26.632307480800627, 24.356081141016126, 23.642604489147587, 22.64380715918169, 21.150334993112093, 20.622732966706067, 20.403146496985975, 20.363940307803013, 20.025240186646315, 16.871885595403004, 16.27160764724984, 15.844209818854335, 15.822142309982228, 15.536597192577855, 16.36892064284867, 13.648452039591026, 12.299304067747737, 12.861138390190211, 11.404430973125248], \"Total\": [4883.0, 768.0, 2199.0, 1008.0, 1414.0, 362.0, 695.0, 1201.0, 356.0, 347.0, 363.0, 625.0, 552.0, 536.0, 302.0, 300.0, 500.0, 254.0, 803.0, 437.0, 206.0, 402.0, 219.0, 733.0, 714.0, 216.0, 214.0, 719.0, 206.0, 376.0, 4883.314858562205, 2199.007418422115, 1414.552832846357, 1201.4036905641813, 803.1285012138292, 733.6386630801898, 714.6113711133163, 608.0855331618894, 596.5637322873857, 518.6017688378606, 472.86604870828796, 400.0174487186952, 381.36107004198595, 366.51548753628947, 216.92345223391186, 215.8760520826401, 214.22523659853283, 197.09417818584652, 719.621620495302, 192.6535762325871, 140.98785220486263, 140.93003495406592, 140.7548620548299, 521.0418677383052, 114.93826423812253, 114.83357015625106, 104.62003709630342, 98.00685795369603, 97.1177571572642, 96.67793551627203, 450.4727317340063, 1008.0369157617763, 695.5854299167654, 625.735200198698, 552.1351826681677, 536.3414807758102, 500.79034720125327, 437.6417410021213, 402.77202197354524, 376.20162253480237, 371.62534347163887, 322.4301002217198, 294.816515474341, 208.76259303511992, 206.23403294430793, 189.2694976193789, 129.32209687839045, 122.95603816356909, 98.89993209731726, 92.87066306991686, 91.4530940621643, 87.41083810875422, 85.45254072643795, 81.88960565267864, 81.03545886122372, 76.18757865783464, 75.09072550113119, 73.9972808296669, 71.99966872703277, 71.21390855808416, 68.59109073575893, 450.4727317340063, 363.1710714915859, 300.5248769715699, 173.3780446575083, 159.1864858481991, 157.28051670536448, 153.90678704557484, 124.3232411730541, 99.20881786706609, 98.3576563081972, 97.37783171276942, 81.54047726380081, 73.8539728117187, 64.76641433168984, 62.139002122616745, 55.52589215259921, 53.500605270696056, 51.65514709916508, 49.99765731654878, 46.31967017871128, 46.13775746462525, 46.08608279142227, 40.35277792311, 38.5810216906153, 37.73346878986045, 34.88147499640067, 33.93218866392836, 31.44042519585097, 30.088788588542375, 25.818342339764513, 25.340677044783472, 302.10500753997536, 216.08921087599018, 214.1971059762571, 211.24891907671238, 167.59604011330032, 152.701391872931, 130.94666054953356, 105.14771192623509, 99.21794420098547, 89.71938239915997, 65.92467562200974, 64.65574934796655, 61.50183936706758, 57.093754639800224, 56.91941030748378, 53.60713188300429, 49.70872215249848, 47.92402872004999, 42.387454006199526, 41.948331831254265, 39.7678066771629, 39.54304285843238, 34.155549668668286, 33.91408304827019, 32.995354912703924, 31.652203017994886, 31.09540104791132, 28.227118058850266, 26.37463113132092, 25.958659458739017, 356.66289135047776, 254.02826349742696, 132.0686973512533, 127.02858699858783, 122.2156133645506, 119.95944218777655, 117.66830512920764, 90.35970929586225, 81.3834993451313, 80.88353103736834, 78.77420353053107, 76.43336035977833, 72.62883925572896, 63.62386483103803, 60.754961220735744, 50.56429024492062, 47.8916476890344, 41.301804995746316, 39.65970290500356, 39.245486912005596, 36.918723569450904, 31.834794152993613, 30.45828673192769, 28.58446460879687, 28.411796954521517, 28.372052931860704, 23.868993231589435, 22.746268493719594, 22.534463816350616, 19.30145820632666, 768.4190326757019, 206.83394474128153, 155.41105450109322, 130.2115278352461, 129.67845152451775, 126.45727803017584, 124.52339194089082, 85.96949296591677, 81.25588421946202, 73.63934203369523, 59.71692100184218, 56.819122830067414, 47.376333847048905, 45.30831196434371, 36.355417946204575, 33.57077872984545, 31.757986097717584, 31.737569042270884, 30.92698458212149, 30.379603143320498, 29.994601441510266, 29.332747249721677, 26.11007291477369, 24.73386897003282, 24.626630333516697, 23.800793288389418, 22.754923143201893, 19.28941843622767, 15.820906516808435, 15.149351944320053, 347.6647134141057, 219.74831009813494, 188.84113221565445, 186.91976055872883, 132.37353519767004, 119.48433362325905, 116.29232602555432, 108.77360145251464, 96.20073801108671, 71.39639808294967, 69.02798856390365, 62.84052776770016, 62.01915442709822, 51.931464186552674, 50.34919182555113, 38.69398227289466, 38.6837530328638, 38.447412934016505, 38.20600629460069, 26.35097060615006, 25.49930620963201, 24.584700193013564, 23.041962122738678, 22.886333567393873, 22.670532042864647, 22.04151451363416, 20.9903527245145, 20.12892317988056, 18.77327807334115, 18.773967475022253, 191.82849502349865, 171.03706845131398, 131.1078490821468, 119.03275646227443, 116.49212183448243, 114.34857727483421, 111.20744793329372, 110.3153686524824, 104.19913145640704, 95.25652636531751, 84.67550145887512, 79.92690234891204, 76.90809052769457, 72.13070701017327, 54.09444881984285, 49.20558274142637, 45.53833399558651, 38.80295015702768, 37.724965665666346, 37.493810304650445, 30.582306212672727, 27.993091641035647, 26.62036650450919, 25.259076653551716, 23.857524580057582, 23.6132783789105, 23.495993358910738, 21.526707310445005, 18.93594222973506, 18.05286938466616, 182.5285589902639, 177.63043299924172, 143.7169080097552, 127.21257260048876, 93.47807862945454, 87.17143466613669, 76.59899944599877, 74.25998798114259, 71.35210657693915, 57.63120703828456, 50.18600750820083, 46.887695069719136, 37.8802994465736, 37.12819558709838, 35.64200473757067, 33.287651166774346, 30.548533854891417, 29.98727829345422, 29.530629434799515, 28.6261046048488, 27.814121484993215, 27.62705456873418, 27.48735809036788, 25.12915519828244, 24.809203177033517, 23.447233013698423, 22.379519443379994, 19.69039670318683, 19.200008388048225, 19.012722841189415, 362.50168967832195, 206.34073842630434, 125.06575609178311, 92.13432774776427, 75.02598989387762, 71.67895206298834, 39.255672314823485, 36.07140091269355, 32.250658502812996, 31.019248490703674, 29.15057148345687, 27.45615680807308, 25.179921401613967, 24.466561367783495, 23.46765202338724, 21.974274425667986, 21.446570057682223, 21.226948053205962, 21.18774443773295, 20.849074754674415, 17.69568018794109, 17.09557985857734, 16.66811297943718, 16.64626389380745, 16.360653566626972, 17.345921144568166, 14.47233319154675, 13.124460471780848, 13.748635984518097, 12.22926765711715], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.5206, -2.3186, -2.76, -2.9234, -3.3265, -3.4171, -3.4434, -3.605, -3.6242, -3.7644, -3.8569, -4.0245, -4.0724, -4.1122, -4.6382, -4.6431, -4.6508, -4.7344, -3.4394, -4.7573, -5.0711, -5.0715, -5.0728, -3.7644, -5.2767, -5.2776, -5.3714, -5.4373, -5.4465, -5.451, -4.7215, -2.3737, -2.745, -2.851, -2.9763, -3.0054, -3.0741, -3.2091, -3.2923, -3.3607, -3.3729, -3.5153, -3.606, -3.9513, -3.9635, -4.0497, -4.4326, -4.4834, -4.7028, -4.7662, -4.7817, -4.8273, -4.8502, -4.8932, -4.9038, -4.9661, -4.9808, -4.9956, -5.0233, -5.0344, -5.0724, -3.7635, -2.2357, -2.4255, -2.9775, -3.0634, -3.0755, -3.0973, -3.312, -3.5393, -3.548, -3.5581, -3.7373, -3.8373, -3.9702, -4.0121, -4.1263, -4.164, -4.1996, -4.2328, -4.3105, -4.3145, -4.3157, -4.4511, -4.4969, -4.5196, -4.6, -4.6283, -4.7065, -4.7516, -4.9093, -4.9286, -2.3728, -2.7089, -2.7178, -2.7317, -2.9642, -3.0577, -3.2123, -3.4333, -3.4918, -3.5933, -3.9048, -3.9245, -3.9751, -4.0505, -4.0536, -4.1145, -4.1912, -4.2284, -4.3534, -4.364, -4.4185, -4.4243, -4.5741, -4.5813, -4.6095, -4.6521, -4.6703, -4.7699, -4.8398, -4.8563, -2.1346, -2.4749, -3.132, -3.1712, -3.2101, -3.2288, -3.2483, -3.5145, -3.6201, -3.6263, -3.653, -3.6835, -3.7351, -3.8691, -3.9159, -4.1023, -4.1575, -4.3083, -4.3497, -4.3605, -4.4229, -4.5747, -4.6202, -4.6855, -4.6917, -4.6931, -4.8716, -4.9216, -4.9313, -5.0925, -1.3604, -2.6758, -2.9629, -3.1409, -3.145, -3.1703, -3.1858, -3.5593, -3.6163, -3.7158, -3.928, -3.9784, -4.1631, -4.2086, -4.4333, -4.5149, -4.5718, -4.5725, -4.5991, -4.6174, -4.6305, -4.6535, -4.7734, -4.8294, -4.8339, -4.8692, -4.9158, -5.0878, -5.2959, -5.3417, -2.1347, -2.5949, -2.7471, -2.7573, -3.1042, -3.2073, -3.2346, -3.3019, -3.4258, -3.727, -3.7611, -3.8562, -3.8695, -4.0497, -4.0811, -4.3494, -4.3497, -4.3559, -4.3624, -4.7438, -4.7778, -4.8155, -4.8826, -4.8897, -4.8995, -4.9287, -4.9795, -5.0232, -5.096, -5.0959, -2.6896, -2.8048, -3.0721, -3.1694, -3.1911, -3.2098, -3.2379, -3.246, -3.3034, -3.3939, -3.5127, -3.571, -3.6099, -3.6747, -3.9663, -4.0625, -4.1413, -4.3045, -4.3333, -4.3396, -4.5484, -4.6393, -4.6911, -4.7453, -4.8044, -4.815, -4.8202, -4.911, -5.0446, -5.0945, -2.5394, -2.5667, -2.7796, -2.9024, -3.2128, -3.2833, -3.4139, -3.4453, -3.4857, -3.702, -3.8425, -3.9116, -4.1292, -4.1497, -4.1915, -4.2615, -4.3496, -4.3687, -4.3844, -4.4165, -4.4461, -4.453, -4.4583, -4.5509, -4.5641, -4.6225, -4.6712, -4.8041, -4.8305, -4.8407, -1.6576, -2.2228, -2.7261, -3.0341, -3.2415, -3.2877, -3.8994, -3.9859, -4.1007, -4.1406, -4.2045, -4.2662, -4.3555, -4.3853, -4.4284, -4.4967, -4.5219, -4.5326, -4.5346, -4.5513, -4.7227, -4.7589, -4.7855, -4.7869, -4.8051, -4.7529, -4.9347, -5.0388, -4.9941, -5.1143], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9139, 0.9137, 0.9135, 0.9134, 0.913, 0.9129, 0.9129, 0.9127, 0.9127, 0.9125, 0.9123, 0.912, 0.9119, 0.9118, 0.9103, 0.9103, 0.9103, 0.9099, 0.9099, 0.9098, 0.9083, 0.9083, 0.9083, 0.9078, 0.907, 0.907, 0.9063, 0.9058, 0.9057, 0.9056, 0.0962, 1.6386, 1.6383, 1.6381, 1.638, 1.6379, 1.6378, 1.6376, 1.6374, 1.6373, 1.6373, 1.6369, 1.6357, 1.6355, 1.6355, 1.6351, 1.6331, 1.6328, 1.6312, 1.6307, 1.6305, 1.6301, 1.6299, 1.6295, 1.6294, 1.6287, 1.6286, 1.6284, 1.6281, 1.628, 1.6275, 1.0543, 2.7975, 2.797, 2.795, 2.7946, 2.7946, 2.7944, 2.7932, 2.7915, 2.7914, 2.7913, 2.7897, 2.7886, 2.7871, 2.7865, 2.7849, 2.7844, 2.7838, 2.7833, 2.782, 2.7819, 2.7819, 2.7793, 2.7784, 2.7779, 2.7761, 2.7754, 2.7735, 2.7723, 2.7676, 2.767, 2.8445, 2.8434, 2.8434, 2.8433, 2.8423, 2.8418, 2.841, 2.8394, 2.839, 2.8381, 2.8348, 2.8345, 2.8339, 2.8328, 2.8328, 2.8319, 2.8307, 2.8301, 2.8278, 2.8276, 2.8265, 2.8264, 2.8231, 2.8229, 2.8222, 2.8211, 2.8207, 2.8179, 2.8158, 2.8153, 2.9166, 2.9157, 2.9127, 2.9125, 2.9122, 2.9121, 2.9119, 2.9098, 2.9088, 2.9087, 2.9084, 2.9081, 2.9076, 2.9059, 2.9053, 2.9025, 2.9016, 2.8988, 2.898, 2.8978, 2.8964, 2.8928, 2.8915, 2.8897, 2.8895, 2.8895, 2.8839, 2.8821, 2.8817, 2.8754, 2.9233, 2.9204, 2.9191, 2.918, 2.918, 2.9178, 2.9177, 2.9147, 2.9142, 2.9131, 2.9105, 2.9098, 2.9068, 2.906, 2.9014, 2.8995, 2.8981, 2.8981, 2.8974, 2.8969, 2.8965, 2.8959, 2.8923, 2.8905, 2.8903, 2.8891, 2.8875, 2.8807, 2.8709, 2.8685, 2.9421, 2.9407, 2.9401, 2.94, 2.9382, 2.9375, 2.9373, 2.9369, 2.9359, 2.9329, 2.9325, 2.9313, 2.9311, 2.9285, 2.928, 2.923, 2.923, 2.9228, 2.9227, 2.9127, 2.9116, 2.9104, 2.9081, 2.9078, 2.9075, 2.9064, 2.9045, 2.9027, 2.8997, 2.8996, 2.9819, 2.9814, 2.9799, 2.9793, 2.9791, 2.979, 2.9788, 2.9787, 2.9783, 2.9776, 2.9765, 2.9759, 2.9755, 2.9748, 2.971, 2.9695, 2.9682, 2.965, 2.9644, 2.9643, 2.9593, 2.9568, 2.9552, 2.9535, 2.9516, 2.9512, 2.9511, 2.9478, 2.9424, 2.9402, 3.1818, 3.1817, 3.1806, 3.1798, 3.1775, 3.1769, 3.1755, 3.1752, 3.1748, 3.172, 3.1698, 3.1687, 3.1644, 3.164, 3.1631, 3.1614, 3.1591, 3.1586, 3.1582, 3.1573, 3.1564, 3.1562, 3.1561, 3.1532, 3.1527, 3.1508, 3.1487, 3.1438, 3.1427, 3.1423, 3.3775, 3.3757, 3.3731, 3.3708, 3.3687, 3.3682, 3.3585, 3.3566, 3.3539, 3.3528, 3.3511, 3.3493, 3.3465, 3.3455, 3.344, 3.3415, 3.3406, 3.3402, 3.3401, 3.3394, 3.3321, 3.3303, 3.329, 3.329, 3.3281, 3.3218, 3.3211, 3.3148, 3.313, 3.3099]}, \"token.table\": {\"Topic\": [4, 8, 3, 2, 5, 4, 10, 5, 7, 1, 5, 1, 1, 10, 5, 1, 3, 6, 5, 8, 3, 4, 8, 2, 8, 9, 5, 4, 4, 7, 10, 6, 3, 8, 7, 6, 10, 9, 8, 6, 2, 1, 2, 6, 9, 4, 7, 4, 6, 5, 10, 1, 9, 6, 2, 9, 5, 1, 6, 3, 2, 7, 2, 5, 9, 5, 5, 8, 1, 7, 7, 10, 9, 1, 7, 3, 8, 1, 10, 6, 5, 2, 10, 7, 3, 3, 1, 9, 9, 7, 10, 7, 7, 5, 9, 2, 8, 2, 4, 6, 3, 2, 2, 10, 10, 7, 6, 1, 5, 1, 10, 4, 1, 2, 3, 1, 2, 4, 2, 2, 8, 8, 10, 2, 6, 2, 6, 7, 1, 6, 2, 5, 10, 3, 3, 9, 3, 3, 7, 1, 1, 6, 3, 3, 1, 1, 2, 9, 5, 5, 4, 5, 10, 2, 1, 10, 4, 9, 4, 4, 7, 7, 9, 2, 1, 8, 2, 8, 5, 9, 3, 5, 4, 4, 8, 9, 5, 7, 9, 4, 7, 2, 8, 10, 7, 6, 8, 8, 9, 8, 5, 5, 10, 6, 9, 5, 9, 10, 2, 9, 10, 8, 1, 4, 5, 8, 7, 4, 8, 8, 10, 10, 6, 4, 9, 9, 5, 6, 10, 6, 6, 3, 1, 9, 4, 9, 2, 8, 10, 7, 9, 7, 2, 6, 5, 3, 6, 6, 9, 3, 7, 8, 4, 2, 3, 2, 3, 9, 5, 8, 4, 6, 4, 7, 6, 2, 6, 2, 3, 1, 3, 9, 10, 8, 5, 7, 10, 2, 10, 3, 10, 8, 8, 6, 3, 1, 4, 10, 3, 7, 3, 4, 4, 8, 1, 3, 9, 4, 8, 7, 1, 10, 1, 4, 1, 2, 5, 7, 4, 1, 3, 6, 6, 7], \"Freq\": [0.9944111944426094, 0.9505732422300407, 0.9878154191023036, 0.9983172744199923, 0.976282381479926, 0.9647728920999108, 0.9986160349245071, 0.9901944838985374, 0.9804188315741938, 0.9996091819029166, 0.9901718646989429, 0.9918367982643388, 0.9991294582574093, 0.967363023964744, 0.9737773032556232, 0.9946370445481602, 0.974729423096597, 0.9913179284871573, 0.9919080192908168, 0.9881784433387772, 0.9918583895056545, 0.9908592290977689, 0.9884919246480599, 0.998825061785777, 0.9501534964709537, 0.9767610219709277, 0.9682642002939557, 0.9918402543365833, 0.977392859504661, 0.9883629898639902, 0.9605300539610407, 0.9668237445386657, 0.9753399920770403, 0.9740282408452766, 0.9528186716291873, 0.985583676951205, 0.9779560415995455, 0.9613174519186383, 0.9807828674493363, 0.9700266742895662, 0.9986660667802912, 0.9990532068678478, 0.9913823977805507, 0.9875046707644711, 0.9550659308133201, 0.9806927577526214, 0.9866809230143444, 0.9949594388744598, 0.9703293904030152, 0.9875736696136994, 0.9983486613990865, 0.9944484499952364, 0.946734465670775, 0.9886561171073114, 0.9952999179776418, 0.9971042395053802, 0.9855061277827419, 0.998214834751544, 0.9920565012847159, 0.9912576545837266, 0.9922245529552826, 0.9684341177850059, 0.9932926454851596, 0.9952877506101704, 0.9830416625191539, 0.9833659140971444, 0.9913417416253233, 0.9809593753779291, 0.9897266581674139, 0.9971781731362523, 0.9588096404730063, 0.9702977737048044, 0.9820312182653916, 0.9929933523391276, 0.9547798005574174, 0.9906429979966928, 0.9645236884239019, 0.9981685115464438, 0.9863248736160781, 0.9767603800628673, 0.9943204320953396, 0.9980832283986301, 0.9556629535612815, 0.9959464675529247, 0.9961199412009513, 0.9925465667397761, 0.9942806150293628, 0.990468138677638, 0.9948856605049656, 0.9612723652399863, 0.9876883266476995, 0.9944479260355805, 0.9928879669130717, 0.9635932180650433, 0.9809259790510403, 0.9906249935002468, 0.9766957940115473, 0.9984218002490058, 0.9630697625097785, 0.9963839326822556, 0.9941082062527474, 0.995045612542575, 0.9891365253796464, 0.9612206832085642, 0.9611766401199571, 0.9851076558177272, 0.9845440827882689, 0.9940734383822415, 0.97511496929945, 0.9957429580600806, 0.9592751829678173, 0.9838471568395203, 0.44175814867636626, 0.5571924387827534, 0.9725278945852579, 0.9963569458995721, 0.002779238342816101, 0.9862670442338866, 0.9972304283122427, 0.9989713514003674, 0.9971412083707404, 0.9640564310358342, 0.8994814986814239, 0.9968059081544998, 0.9973550497909379, 0.9963470800777435, 0.9668406515269173, 0.9955458209459691, 0.993400732822006, 0.9887228255923164, 0.999363314626875, 0.9890765026447702, 0.9680129713547327, 0.9881664850586789, 0.9800459187471057, 0.9819874122598404, 0.9805618509672361, 0.9865561190736377, 0.9950793829610121, 0.9966075053192988, 0.9959418746350581, 0.9711242395131977, 0.9849402202130735, 0.9905288842337926, 0.999663984248299, 0.992741058602316, 0.9909005792194909, 0.9820438565890918, 0.9684806754600582, 0.9868866404290786, 0.9877245571776988, 0.9888401430695983, 0.9455483449150074, 0.9829540523380478, 0.999144470493992, 0.9359144370860484, 0.9964435907143309, 0.9830327473058231, 0.9793947038181167, 0.9859737554521841, 0.9823245424950646, 0.976216907734359, 0.9950116655048928, 0.9940163467363257, 0.9884907025246249, 0.9891423824956581, 0.9985336384946925, 0.9915500933780657, 0.994330219713768, 0.9865617140451658, 0.9963637166477766, 0.9981414064469455, 0.9857957774099054, 0.9927706400028776, 0.9884031243339602, 0.9781281940560385, 0.9959521689308506, 0.9704227478386115, 0.970729922732506, 0.9919818618906732, 0.9980880618927986, 0.9975093438309051, 0.9881925227701815, 0.9599166996131233, 0.9820648526688142, 0.9745547675410607, 0.9793069817171555, 0.9754990658730399, 0.989047478428393, 0.9868295513142553, 0.9920019452385024, 0.9325720268171206, 0.980930652216709, 0.9959680470131502, 0.9921800617458318, 0.9960191406251158, 0.9810676325974398, 0.960686439823027, 0.9861156482424559, 0.9696135088371918, 0.9833859920286093, 0.9788903005148903, 0.9929876914246072, 0.9963423064418118, 0.98138197927906, 0.9755323792510785, 0.996594694640424, 0.9808428321678854, 0.9956810638409213, 0.9957754925678007, 0.9224070527387566, 0.9671414189480078, 0.9947682015281507, 0.966168025990567, 0.9374995904275117, 0.9822697369181267, 0.979553067836121, 0.9663543446352243, 0.979177553497779, 0.9574848787899943, 0.9829977512753373, 0.9859917544655505, 0.9990550342622669, 0.9649373898558837, 0.9890847655625616, 0.9763677652977869, 0.9947041864104816, 0.9416785574507922, 0.9421985652327135, 0.9888872630746904, 0.9673829436899201, 0.9439153714387986, 0.9844124373191048, 0.9906956944950448, 0.9900535346418905, 0.9930986084858134, 0.9761324255453321, 0.9957968383872866, 0.967076762225876, 0.9764336058601966, 0.9587744318801034, 0.9868090259716314, 0.9886744195841454, 0.9979440131623528, 0.9816700931185024, 0.9991583637442845, 0.9973999939994794, 0.9964508728116177, 0.9919027124296104, 0.9969516256070557, 0.9565269803211272, 0.9879946757164512, 0.9940879267824378, 0.9866244317551358, 0.9902237969941512, 0.98547456435063, 0.9241319398648614, 0.9872221509475626, 0.9873169057498306, 0.999541875841948, 0.968303838836929, 0.9773028801469186, 0.9800724834797622, 0.9939365866083633, 0.967191607980639, 0.9820635870537641, 0.953140385833836, 0.9865227368021464, 0.9439419122113956, 0.9638141434196198, 0.9914784340247303, 0.9920224687514464, 0.9843241934393656, 0.933154105164415, 0.9933716691153003, 0.9985948684275012, 0.9730470953034711, 0.9143232992930588, 0.9978195355804417, 0.9875184116472336, 0.996775427385292, 0.9807188847697313, 0.9954067748543206, 0.9913237625258073, 0.997456489155775, 0.9982534658133492, 0.9950652252073389, 0.9857424990663765, 0.9797678164077829, 0.9930645991943425, 0.9985935450101862, 0.9905278740348811, 0.9988396321146202, 0.9698334836725551, 0.9941619514157104, 0.005757694699318787, 0.98495362736712, 0.9527475975850066, 0.9898578339191861, 0.999935523599988, 0.9884370091518868, 0.999454682070741, 0.9481125486749897, 0.9835671021878215], \"Term\": [\"about\", \"address\", \"age\", \"all\", \"alone\", \"always\", \"am\", \"an\", \"anal\", \"and\", \"any\", \"anything\", \"are\", \"arms\", \"around\", \"as\", \"ask\", \"ass\", \"at\", \"babe\", \"baby\", \"back\", \"bad\", \"be\", \"beautiful\", \"because\", \"bed\", \"been\", \"before\", \"bet\", \"better\", \"between\", \"big\", \"bikini\", \"blow\", \"body\", \"boobs\", \"both\", \"bra\", \"breasts\", \"but\", \"can\", \"cant\", \"caress\", \"caught\", \"cause\", \"check\", \"cock\", \"coming\", \"cool\", \"could\", \"cum\", \"cunt\", \"cute\", \"dick\", \"did\", \"dirty\", \"do\", \"does\", \"doing\", \"don\", \"done\", \"dont\", \"down\", \"dressed\", \"each\", \"edit\", \"erotic\", \"even\", \"ever\", \"everything\", \"excited\", \"face\", \"feel\", \"feeling\", \"find\", \"fingers\", \"for\", \"from\", \"front\", \"fuck\", \"get\", \"gets\", \"girl\", \"girls\", \"go\", \"going\", \"gonna\", \"good\", \"guess\", \"guy\", \"guys\", \"had\", \"hair\", \"hand\", \"hard\", \"has\", \"have\", \"havent\", \"he\", \"her\", \"here\", \"him\", \"hold\", \"home\", \"hope\", \"horny\", \"hot\", \"house\", \"how\", \"huh\", \"hurt\", \"if\", \"if\", \"ill\", \"in\", \"in\", \"inside\", \"is\", \"it\", \"its\", \"jack\", \"jerking\", \"just\", \"keep\", \"know\", \"lap\", \"laughing\", \"let\", \"lick\", \"like\", \"link\", \"lips\", \"little\", \"lol\", \"long\", \"lot\", \"lots\", \"loud\", \"love\", \"make\", \"man\", \"married\", \"maybe\", \"me\", \"meet\", \"might\", \"mind\", \"mine\", \"mom\", \"more\", \"mouth\", \"muah\", \"much\", \"my\", \"myself\", \"naked\", \"need\", \"nervous\", \"never\", \"night\", \"nipples\", \"no\", \"not\", \"now\", \"nude\", \"of\", \"off\", \"oh\", \"ok\", \"older\", \"on\", \"once\", \"one\", \"only\", \"open\", \"or\", \"oral\", \"orgasm\", \"other\", \"out\", \"over\", \"panties\", \"pants\", \"penis\", \"person\", \"pic\", \"pics\", \"play\", \"playing\", \"porn\", \"pretty\", \"probably\", \"pussy\", \"put\", \"re\", \"ready\", \"real\", \"really\", \"rub\", \"same\", \"saw\", \"say\", \"see\", \"seeing\", \"send\", \"sex\", \"sexual\", \"sexy\", \"she\", \"shit\", \"shower\", \"simple\", \"since\", \"sis\", \"sit\", \"sitting\", \"skirt\", \"sleep\", \"slide\", \"slowly\", \"slut\", \"so\", \"softly\", \"some\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"still\", \"stop\", \"strip\", \"stuff\", \"stupid\", \"suck\", \"sucking\", \"sweet\", \"take\", \"talking\", \"taste\", \"teach\", \"tell\", \"than\", \"that\", \"thats\", \"the\", \"them\", \"then\", \"there\", \"they\", \"thing\", \"things\", \"think\", \"thinking\", \"those\", \"though\", \"tight\", \"time\", \"tits\", \"to\", \"tomorrow\", \"tongue\", \"tonight\", \"too\", \"top\", \"touch\", \"touching\", \"try\", \"ummm\", \"until\", \"up\", \"us\", \"very\", \"virgin\", \"wait\", \"want\", \"wanted\", \"warm\", \"was\", \"way\", \"we\", \"wearing\", \"well\", \"were\", \"what\", \"when\", \"where\", \"who\", \"why\", \"wiht\", \"will\", \"wish\", \"with\", \"work\", \"would\", \"would\", \"wouldn\", \"yea\", \"yes\", \"you\", \"young\", \"your\", \"yours\", \"yourself\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 3, 8, 1, 2, 9, 7, 6, 4, 10]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1459624586421842243367462537\", ldavis_el1459624586421842243367462537_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1459624586421842243367462537\", ldavis_el1459624586421842243367462537_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1459624586421842243367462537\", ldavis_el1459624586421842243367462537_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "4      0.426260  0.236144       1        1  40.089612\n",
              "2      0.119176  0.438535       2        1  19.408800\n",
              "7     -0.321131 -0.242713       3        1   6.082481\n",
              "0     -0.262294  0.356339       4        1   5.800684\n",
              "1      0.176594 -0.398430       5        1   5.399003\n",
              "8     -0.133340  0.151125       6        1   5.369835\n",
              "6     -0.099605 -0.377681       7        1   5.263129\n",
              "5     -0.417251  0.008783       8        1   5.048329\n",
              "3      0.140551 -0.028214       9        1   4.132479\n",
              "9      0.371039 -0.143888      10        1   3.405646, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
              "5         you  4883.000000  4883.000000  Default  30.0000  30.0000\n",
              "62       your   768.000000   768.000000  Default  29.0000  29.0000\n",
              "1          to  2199.000000  2199.000000  Default  28.0000  28.0000\n",
              "38         it  1008.000000  1008.000000  Default  27.0000  27.0000\n",
              "134       and  1414.000000  1414.000000  Default  26.0000  26.0000\n",
              "...       ...          ...          ...      ...      ...      ...\n",
              "348      shit    16.368921    17.345921  Topic10  -4.7529   3.3218\n",
              "413      arms    13.648452    14.472333  Topic10  -4.9347   3.3211\n",
              "1201     warm    12.299304    13.124460  Topic10  -5.0388   3.3148\n",
              "2711     muah    12.861138    13.748636  Topic10  -4.9941   3.3130\n",
              "2022  jerking    11.404431    12.229268  Topic10  -5.1143   3.3099\n",
              "\n",
              "[332 rows x 6 columns], token_table=      Topic      Freq      Term\n",
              "term                           \n",
              "11        4  0.994411     about\n",
              "1219      8  0.950573   address\n",
              "293       3  0.987815       age\n",
              "127       2  0.998317       all\n",
              "1089      5  0.976282     alone\n",
              "...     ...       ...       ...\n",
              "5         1  0.999936       you\n",
              "171       3  0.988437     young\n",
              "62        6  0.999455      your\n",
              "966       6  0.948113     yours\n",
              "631       7  0.983567  yourself\n",
              "\n",
              "[304 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 3, 8, 1, 2, 9, 7, 6, 4, 10])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# visualize\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a3c8a5",
      "metadata": {},
      "source": [
        "### TF/IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f21fe2b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized data shape: (6476, 873)\n",
            "you have hair down there\n",
            "  (0, 863)\t0.14431819925711445\n",
            "  (0, 714)\t0.4574372173172504\n",
            "  (0, 310)\t0.3281164897432353\n",
            "  (0, 297)\t0.6278215402777263\n",
            "  (0, 195)\t0.5177867633477271\n"
          ]
        }
      ],
      "source": [
        "# Train TF/IDF model\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.95)\n",
        "tfidf_vectorizer.fit(pan12_df['preprocessed_bow'])\n",
        "\n",
        "# Transform chat messages to vocabulary vectors\n",
        "vectorized_data = tfidf_vectorizer.transform(pan12_df['preprocessed_bow'])\n",
        "print(f'Vectorized data shape: {vectorized_data.shape}')\n",
        "\n",
        "print(pan12_df['preprocessed_bow'][6])\n",
        "print(vectorized_data[6])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc2379a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>babe</th>\n",
              "      <th>young</th>\n",
              "      <th>age</th>\n",
              "      <th>dick</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.847736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4753</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.847736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4455</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.847736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5624</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3287</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.679020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2178</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2177</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2174</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2173</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6475</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6476 rows  5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sex  babe  young  age      dick\n",
              "952   0.0   0.0    0.0  0.0  0.847736\n",
              "4753  0.0   0.0    0.0  0.0  0.847736\n",
              "4455  0.0   0.0    0.0  0.0  0.847736\n",
              "5624  0.0   0.0    0.0  0.0  0.722405\n",
              "3287  0.0   0.0    0.0  0.0  0.679020\n",
              "...   ...   ...    ...  ...       ...\n",
              "2178  0.0   0.0    0.0  0.0  0.000000\n",
              "2177  0.0   0.0    0.0  0.0  0.000000\n",
              "2174  0.0   0.0    0.0  0.0  0.000000\n",
              "2173  0.0   0.0    0.0  0.0  0.000000\n",
              "6475  0.0   0.0    0.0  0.0  0.000000\n",
              "\n",
              "[6476 rows x 5 columns]"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe of vectors\n",
        "tfidf_df = pd.DataFrame(vectorized_data.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
        "# tfidf_df[['sex', 'babe', 'young', 'age', 'dick']].sort_values('dick', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62539499",
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html\n",
        "#Visualize TF/IDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bac1a26",
      "metadata": {},
      "source": [
        "## Backup - not useful currently"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae40be1",
      "metadata": {},
      "source": [
        "### Pan12 dataloader and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0a4589",
      "metadata": {
        "id": "6f0a4589"
      },
      "source": [
        "#### Pan12 convesation level dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e001f9d6",
      "metadata": {
        "id": "e001f9d6"
      },
      "outputs": [],
      "source": [
        "# class Pan12Dataset(Dataset):\n",
        "#     '''\n",
        "#     Wrapper around Torch Dataset.\n",
        "#     Prepares an indexed list of Pan12 conversation in a folder, returns conversations per index (like an array)\n",
        "#     Load is lazy - loads conversation from disk on request.\n",
        "#     Uses load_one_chat_as_df_pj() for conversation loading\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, chat_data_file: Path, conversation_labels: Path=None, line_labels: Path=None, preprocess_fn=None, preprocess_args=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             chat_data_file: path to chat xml file\n",
        "#             conversation_labels:\n",
        "#             line_labels:  \n",
        "#         \"\"\"\n",
        "       \n",
        "#         self.chat_data_file = chat_data_file\n",
        "#         self.conversations = self._get_conversation_roots(chat_data_file)\n",
        "#         self.preprocess_fn = preprocess_fn\n",
        "#         self.preprocess_args = preprocess_args\n",
        "\n",
        "#         self.conversation_labels = conversation_labels\n",
        "#         self.line_labels = line_labels\n",
        "\n",
        "#         self.TEXT_COLUMN_NAME = 'text'\n",
        "\n",
        "                \n",
        "#     def __len__(self) -> int:\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#             int: length of the dataset\n",
        "#         \"\"\"\n",
        "#         return len(self.conversations)\n",
        "\n",
        "#     def __getitem__(self, idx) -> Dict[str, pd.DataFrame]:\n",
        "#         \"\"\"Gets element of the dataset\n",
        "\n",
        "#         Args:\n",
        "#             index (int): index of the element in the dataset\n",
        "#         Returns:\n",
        "#             Single element by index\n",
        "#         \"\"\"        \n",
        "\n",
        "#         conversation = self.conversations[idx]\n",
        "#         conversation_id = conversation.attrib['id']\n",
        "#         conversation_df = pd.DataFrame(columns = ['author', 'line', 'time', 'text'], dtype=str)\n",
        "\n",
        "#         for message in conversation.findall('message'):\n",
        "#             message_dict = {}\n",
        "#             message_dict['line'] = message.attrib['line']\n",
        "#             for field in message:\n",
        "#                 message_dict[field.tag] = field.text\n",
        "\n",
        "#             conversation_df = conversation_df.append(message_dict, ignore_index=True)\n",
        "                \n",
        "#         if self.preprocess_fn is not None:\n",
        "#             conversation_df = self.preprocess_fn(conversation_df, self.TEXT_COLUMN_NAME, **self.preprocess_args)\n",
        "\n",
        "#         return {'conversation_id': conversation_id, 'conversation': conversation_df}\n",
        "    \n",
        "#     def _get_conversation_roots(self, file_path):\n",
        "#         doc_tree = ET.parse(file_path)\n",
        "#         conversation_roots = doc_tree.getroot().findall('conversation')\n",
        "#         return conversation_roots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa6bbd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eaa6bbd4",
        "outputId": "f0674403-59f1-4453-e668-802ddf728610"
      },
      "outputs": [],
      "source": [
        "# preprocess_args = {'stemmer': PorterStemmer(),\n",
        "#                     'speller': SpellChecker(),\n",
        "#                     'words_to_remove': set(stopwords.words('english')),\n",
        "#                     'emoticons': emoticons,\n",
        "#                     'chat_slang': chat_slang\n",
        "#                     }\n",
        "\n",
        "# pan12_ds = Pan12Dataset(PAN12_DATA_FILE, preprocess_fn=preprocess_df_for_bow, preprocess_args=preprocess_args)\n",
        "# pan12_ds[34]['conversation']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab1a7b2",
      "metadata": {
        "id": "bab1a7b2"
      },
      "source": [
        "#### pan12 line level dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64c6027",
      "metadata": {
        "id": "e64c6027"
      },
      "outputs": [],
      "source": [
        "# class Pan12LineLevelDataloader():  \n",
        "#     \"\"\"\n",
        "#     Wrapper around Torch Dataset to perform text classification\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, chat_data_file: Path, user_labels_file: Path=None, line_labels_file: Path=None, preprocess_fn=None, preprocess_args:Dict=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             chat_data_file: path to chat xml file\n",
        "#             conversation_labels:\n",
        "#             line_labels:  \n",
        "#         \"\"\"\n",
        "       \n",
        "#         self.chat_data_file = chat_data_file\n",
        "#         self.conversations = self._get_conversation_roots(chat_data_file)\n",
        "#         self.preprocess_fn = preprocess_fn\n",
        "#         self.preprocess_args = preprocess_args\n",
        "\n",
        "#         self.user_labels_file = user_labels_file\n",
        "#         self.line_labels_file = line_labels_file\n",
        "#         self.TEXT_COLUMN_NAME = 'text'\n",
        "\n",
        "#         self.length = self._get_ds_length()\n",
        "#         self.num_conversations = len(self.conversations)\n",
        "\n",
        "#         # Initiate queue\n",
        "#         self.message_list = None\n",
        "#         self.current_conversation_id = None\n",
        "#         self.next_conversation_idx = 0\n",
        "#         self.next_message_idx = 0\n",
        "\n",
        "#         # Create sets of problematic lines and authors for labels\n",
        "#         user_labels = pd.read_csv(user_labels_file, delimiter='\\t', header=None)\n",
        "#         self.perverted_authors = set(user_labels[0])\n",
        "\n",
        "#         line_labels = pd.read_csv(line_labels_file, delimiter='\\t', header=None)\n",
        "#         line_labels['concat'] = line_labels[0] + '_' + line_labels[1].astype(str)\n",
        "#         self.pervert_lines = set(line_labels['concat'])\n",
        "\n",
        "#         self.load_next_conversation_to_list()\n",
        "                       \n",
        "#     def __iter__(self):\n",
        "#         return self\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#             int: length of the dataset\n",
        "#         \"\"\"\n",
        "#         return self.length\n",
        "\n",
        "#     def __next__(self) -> Dict[str, pd.DataFrame]:\n",
        "#         \"\"\"Gets element of the dataset\n",
        "\n",
        "#         Args:\n",
        "#             index (int): index of the element in the dataset\n",
        "#         Returns:\n",
        "#             Single element by index\n",
        "#         \"\"\"        \n",
        "#         message_dict = {}\n",
        "#         try:\n",
        "#             message = self.message_list[self.next_message_idx]\n",
        "#         except(IndexError):\n",
        "#             self.load_next_conversation_to_list()\n",
        "#             message = self.message_list[self.next_message_idx]\n",
        "\n",
        "#         message_dict['conversation_id'] = self.current_conversation_id\n",
        "#         self.next_message_idx += 1\n",
        "        \n",
        "#         message_dict['line'] = message.attrib['line']  \n",
        "\n",
        "#         for field in message:\n",
        "#             message_dict[field.tag] = field.text\n",
        "        \n",
        "#         if self.preprocess_fn is not None:\n",
        "#             message_dict['text'] = self.preprocess_fn(message_dict['text'], **self.preprocess_args)\n",
        "        \n",
        "#         message_dict['author_label'] = 1 if message_dict['author'] in self.perverted_authors else 0\n",
        "#         message_dict['line_label'] = 1 if message_dict['conversation_id'] + '_' + message_dict['line'] in self.pervert_lines else 0\n",
        "\n",
        "#         return message_dict\n",
        "    \n",
        "#     def _get_conversation_roots(self, file_path):\n",
        "#         doc_tree = ET.parse(file_path)\n",
        "#         conversation_roots = doc_tree.getroot().findall('conversation')\n",
        "#         return conversation_roots\n",
        "\n",
        "#     def _get_ds_length(self):\n",
        "#         number_messages = 0\n",
        "#         for conversation in self.conversations:\n",
        "#             number_messages += len(conversation.findall('message'))\n",
        "        \n",
        "#         return number_messages\n",
        "\n",
        "#     def load_next_conversation_to_list(self):\n",
        "#         try:\n",
        "#             conversation = self.conversations[self.next_conversation_idx] \n",
        "#             self.current_conversation_id = conversation.attrib['id']  \n",
        "#         except(IndexError):\n",
        "#             raise StopIteration()\n",
        "\n",
        "#         self.next_conversation_idx += 1\n",
        "#         self.message_list = [m for m in conversation.findall('message')]\n",
        "#         self.next_message_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfc77a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dfc77a4",
        "outputId": "1029d1bd-7ff7-4823-c0d8-89f3481c5706"
      },
      "outputs": [],
      "source": [
        "# # Test dataset\n",
        "# preprocess_args = {'stemmer': PorterStemmer(),\n",
        "#                     'speller': SpellChecker(),\n",
        "#                     'words_to_remove': set(stopwords.words('english')),\n",
        "#                     'emoticons': emoticons,\n",
        "#                     'chat_slang': chat_slang\n",
        "#                     }\n",
        "\n",
        "# pan12_ds = Pan12LineLevelDataloader(PAN12_DATA_FILE, user_labels_file=PAN12_USER_LABELS_FILE, line_labels_file=PAN12_LINE_LABELS_FILE, preprocess_fn=preprocess_string_for_bow, preprocess_args=preprocess_args)\n",
        "# print(len(pan12_ds))\n",
        "\n",
        "# for i, m in enumerate(pan12_ds):\n",
        "#     print(i, m) \n",
        "#     if i==50:\n",
        "#         break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d684c5",
      "metadata": {
        "id": "72d684c5"
      },
      "source": [
        "## some thoughts\n",
        "Bag of words - sexual words, fear, trust, family, approach (Location, transport) , other categories - DrouinBoydHancockJames2017\n",
        "Good article: file:///D:/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/ref%20docs/Early%20Text%20Classification%20using%20Multi-Resolution%20Concept%20Representations.pdf\n",
        "Ensamble and preprocessing: file:///D:/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/ref%20docs/PredatoryConversationDetection.pdf\n",
        "file:///D:/docs/DSML_IDC/Semester%204/Cyber/Tasks/Task2/ref%20docs/Analyzing_Chat_Conversations_of_Pedophil.pdf\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "cyber_hw2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1a8e22f9a50968eda7c2b2da6b1cd647c6294c71990fbcb0be47dbd614eb6ed8"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14c4346449c64fa4a695d62f33d10cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3304f532f9c34d59bf10cf3a4b143a8f",
            "placeholder": "",
            "style": "IPY_MODEL_cbd617ce94ed4af587dd6dfa9789ac2d",
            "value": " 355158/2058781 [3:52:56&lt;35:12:01, 13.44it/s]"
          }
        },
        "295caedfec664f0bacdab9553c5e674d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3304f532f9c34d59bf10cf3a4b143a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45501c874a53430ea0979ac01e4dcd87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a30eca9cd274aedba3c3cfee08e986a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e80d4d0d52a54f9ab8ac1f41f281366e",
              "IPY_MODEL_7f1f823c15724c948715c043b9ea89b9",
              "IPY_MODEL_14c4346449c64fa4a695d62f33d10cc1"
            ],
            "layout": "IPY_MODEL_e032f1f7e816478a96c3c1f9246034c6"
          }
        },
        "6aebf996aa9c4960bf0a46ebca04d783": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f1f823c15724c948715c043b9ea89b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45501c874a53430ea0979ac01e4dcd87",
            "max": 2058781,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_295caedfec664f0bacdab9553c5e674d",
            "value": 355158
          }
        },
        "bc6c2d7cdac74c079d40c0e1c69aaeec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd617ce94ed4af587dd6dfa9789ac2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e032f1f7e816478a96c3c1f9246034c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80d4d0d52a54f9ab8ac1f41f281366e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc6c2d7cdac74c079d40c0e1c69aaeec",
            "placeholder": "",
            "style": "IPY_MODEL_6aebf996aa9c4960bf0a46ebca04d783",
            "value": " 17%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
